#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pydantic",
#   "textual>=0.50.0",
# ]
# ///
"""
git-smart-commit: Analyze unstaged changes, group into logical commits, execute them.

Uses an LLM for classification. Supports Ollama (default) and any OpenAI-compatible
API (OpenRouter, Anthropic, Together, etc.) via --api-base.

Usage:
    git-smart-commit [--repo PATH] [--dry-run] [--model MODEL] [--yes]
                     [--api-base URL] [--api-key KEY]
    git-smart-commit --setup

Options:
    --repo PATH     Path to git repository (default: current directory)
    --dry-run       Print proposed commits without executing
    --model MODEL   Model name (default: qwen3-coder:30b-a3b-q8_0)
    --yes           Skip confirmation prompt and commit immediately
    --json          Output proposed commits as JSON and exit (implies --dry-run)
    --quiet, -q     Suppress progress output (for scripting/CI)
    --api-base URL  API base URL (default: http://localhost:11434 for Ollama)
                    Use https://openrouter.ai/api/v1 for OpenRouter, etc.
    --api-key KEY   API key for authenticated endpoints (or set LLM_API_KEY env var)
    --setup         Run interactive configuration wizard and exit
    --help          Show this message
"""

from pathlib import Path
from pydantic import BaseModel, Field, ValidationError
from typing import Any, Callable

import argparse
import ast
import getpass
import httpx
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import textwrap
from dataclasses import dataclass, field


def wrap_markdown(text: str, width: int = 80) -> str:
    """Wrap text at width, preserving markdown structure.

    - Code blocks (```) are not wrapped
    - Inline code (`...`) within a line is preserved
    - List items wrap with a hanging indent; continuation lines (indented to
      at least the marker end) are wrapped with the same indent
    - Blockquotes are preserved with the correct continuation prefix
    - Regular paragraphs preserve their leading indent on continuation lines
    """
    if not text:
        return text

    lines = text.split('\n')
    result = []
    in_code_block = False
    # Number of leading spaces that mark a list continuation line; None when
    # we are not inside a list item.
    list_continuation_indent: int | None = None

    for line in lines:
        stripped = line.lstrip()
        indent = len(line) - len(stripped)

        # Code block fence toggles verbatim mode
        if stripped.startswith('```'):
            in_code_block = not in_code_block
            list_continuation_indent = None
            result.append(line)
            continue

        if in_code_block:
            result.append(line)
            continue

        # Blank line resets list state
        if not stripped:
            list_continuation_indent = None
            result.append(line)
            continue

        # New list item
        list_match = re.match(r'^(\s*)([-*+]|\d+\.)\s+', line)
        if list_match:
            marker_end = list_match.end()
            list_continuation_indent = marker_end
            content = line[marker_end:]
            wrapped_content = _wrap_line_preserving_inline(content, width - marker_end)
            wrapped_lines = wrapped_content.split('\n')
            result.append(line[:marker_end] + wrapped_lines[0])
            for wrapped_line in wrapped_lines[1:]:
                result.append(' ' * marker_end + wrapped_line)
            continue

        # List continuation line: indented to at least the marker position
        if list_continuation_indent is not None and indent >= list_continuation_indent:
            cont_prefix = ' ' * indent
            wrapped_content = _wrap_line_preserving_inline(stripped, width - indent)
            wrapped_lines = wrapped_content.split('\n')
            result.append(cont_prefix + wrapped_lines[0])
            for wrapped_line in wrapped_lines[1:]:
                result.append(cont_prefix + wrapped_line)
            continue

        # Not a list continuation — reset list state
        list_continuation_indent = None

        # Blockquote
        if stripped.startswith('>'):
            quote_match = re.match(r'^(\s*)(>+\s?)', line)
            if quote_match:
                full_prefix = quote_match.group(1) + quote_match.group(2)
                content = line[len(full_prefix):]
                wrapped_content = _wrap_line_preserving_inline(content, width - len(full_prefix))
                wrapped_lines = wrapped_content.split('\n')
                result.append(full_prefix + wrapped_lines[0])
                for wrapped_line in wrapped_lines[1:]:
                    result.append(full_prefix + wrapped_line)
                continue

        # Regular paragraph — preserve leading indent on all continuation lines
        cont_prefix = ' ' * indent
        wrapped_content = _wrap_line_preserving_inline(stripped, width - indent)
        wrapped_lines = wrapped_content.split('\n')
        result.append(cont_prefix + wrapped_lines[0])
        for wrapped_line in wrapped_lines[1:]:
            result.append(cont_prefix + wrapped_line)

    return '\n'.join(result)


def _wrap_line_preserving_inline(text: str, width: int) -> str:
    """Wrap a line while preserving inline code spans."""
    if len(text) <= width:
        return text

    # Split on inline code to preserve it
    parts = re.split(r'(`[^`]+`)', text)
    lines = []
    current_line = ''

    for part in parts:
        if not part:
            continue
        if part.startswith('`') and part.endswith('`'):
            # Inline code - add to current line if it fits, else start new line
            if current_line and len(current_line) + 1 + len(part) > width:
                lines.append(current_line.rstrip())
                current_line = part
            else:
                if current_line:
                    current_line += ' '
                current_line += part
        else:
            # Regular text - wrap it
            words = part.split(' ')
            for word in words:
                if not word:
                    continue
                if current_line and len(current_line) + 1 + len(word) > width:
                    lines.append(current_line.rstrip())
                    current_line = word
                else:
                    if current_line:
                        current_line += ' '
                    current_line += word

    if current_line:
        lines.append(current_line.rstrip())

    return '\n'.join(lines)


# ── Configuration ──────────────────────────────────────────────────────────────

SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    Given a list of changed files and their diffs, your job is to group them into
    one or more commits. Each commit should represent a single logical change
    (e.g. "add feature X", "fix bug in Y", "update dependencies",
    "refactor Z").

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Changes are represented as individual diff hunks, each identified by an ID
      like "src/main.py#1". Untracked (new) files use "filepath#0".
    - You CAN split a file across multiple commits by assigning different hunks
      from that file to different commits. Prefer splitting when hunks are unrelated.
    - Keep related hunks together (same feature, same module, same concern).
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).
    - BREAKING CHANGE detection: if a commit removes or renames a public function,
      class, or module; deletes an exported symbol; or changes a function/method
      signature incompatibly (added required param, removed param, changed type),
      set breaking_change to a concise description of what broke and how callers
      must update. Also add '!' after the type in the subject (e.g. 'feat!(api):').
      Leave breaking_change empty for purely additive or internal changes.

    You have access to a read_file tool. Use it when the diff alone is not
    enough context — for example, to see which class a changed function belongs
    to, what imports already exist, or how unchanged surrounding code looks.
    Call read_file as many times as needed, then call propose_commits when ready.
""")


AGENTIC_SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    You have investigation tools to gather information before proposing commits:
    - read_file(path): Read the full current content of any file in the repository
    - get_diff(path, staged): Get the raw diff for a specific file on demand
      (staged=true for staged only, staged=false for unstaged only, omit for both)
    - get_git_log(n): Get the last n commit subjects to learn this project's conventions
    - search_diff(pattern, context_lines): Search all diffs for a regex pattern
    - query_tool_result(result_id, question): When a previous tool result was summarized
      to save context, use this to ask a targeted question about the original full output.
      The result_id is shown in the summary (e.g. "r5"). Use this instead of re-fetching
      a diff or file you already retrieved but whose details were compressed away.

    Investigation strategy:
    - Start with the file list and stats you receive
    - Use get_git_log early to learn this project's commit style conventions
    - Use get_diff on files whose grouping is unclear from stats alone
    - Use read_file when you need full file context (imports, class structure, etc.)
    - Use search_diff to find patterns across all changes (debug prints, TODOs, etc.)
    - If a prior result was summarized, use query_tool_result before re-fetching it
    - Investigate as many files as needed, then call propose_commits when confident

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Changes are represented as individual diff hunks, each identified by an ID
      like "src/main.py#1". Untracked (new) files use "filepath#0".
    - You CAN split a file across multiple commits by assigning different hunks
      from that file to different commits. Prefer splitting when hunks are unrelated.
    - Keep related hunks together (same feature, same module, same concern).
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).
    - BREAKING CHANGE detection: if a commit removes or renames a public function,
      class, or module; deletes an exported symbol; or changes a function/method
      signature incompatibly (added required param, removed param, changed type),
      set breaking_change to a concise description of what broke and how callers
      must update. Also add '!' after the type in the subject (e.g. 'feat!(api):').
      Leave breaking_change empty for purely additive or internal changes.

    When you have investigated enough to be confident in your groupings, call the
    propose_commits tool with your answer.
""")


OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "gpt-oss:20b"

DIFF_CHARS_PER_FILE = 6000    # files larger than this get hunk-summarized
DIFF_TOTAL_CHARS   = 120000   # hard cap on total context sent to classifier
HUNK_SUMMARIZE_CTX = 8192     # smaller context for summarization calls (faster)
PRIOR_HUNK_WINDOW  = 4        # how many previous hunk summaries to include as context

MAX_AGENTIC_TURNS     = 20    # base safeguard for the agentic investigation loop
MAX_AGENTIC_TURNS_CAP = 50    # absolute ceiling when scaling with hunk count
READ_FILE_LIMIT    = 50_000   # max chars returned by the read_file tool

CONTEXT_TRIM_THRESHOLD       = 300_000  # total message chars before summarizing old results
TOOL_RESULT_SUMMARIZE_SKIP   = 500      # results shorter than this are never summarized
TOOL_RESULT_SUMMARIZE_INPUT  = 20_000   # max chars fed to the summarizer model
TURNS_WARN_AT                = 4        # remaining turns at which to inject a commit warning

# Preset backends for the setup wizard
SETUP_BACKENDS = [
    {
        "key": "ollama",
        "label": "Ollama  (local, no API key needed)",
        "api_base": OLLAMA_BASE_URL,
        "default_model": DEFAULT_MODEL,
        "needs_key": False,
    },
    {
        "key": "openrouter",
        "label": "OpenRouter  (cloud, API key required)",
        "api_base": "https://openrouter.ai/api/v1",
        "default_model": "anthropic/claude-3.5-sonnet",
        "needs_key": True,
    },
    {
        "key": "anthropic",
        "label": "Anthropic  (cloud, API key required)",
        "api_base": "https://api.anthropic.com/v1",
        "default_model": "claude-sonnet-4-6",
        "needs_key": True,
    },
    {
        "key": "custom",
        "label": "Custom endpoint",
        "api_base": None,
        "default_model": None,
        "needs_key": None,
    },
]


@dataclass
class ApiConfig:
    """API connection and model configuration."""
    base_url: str = OLLAMA_BASE_URL
    model: str = DEFAULT_MODEL
    api_key: str | None = None
    num_ctx: int = 128000

    @property
    def is_ollama(self) -> bool:
        """True when targeting a native Ollama endpoint (not OpenAI-compatible)."""
        # OpenAI-compatible APIs use /v1 in the path; Ollama doesn't.
        return "/v1" not in self.base_url

    @property
    def chat_url(self) -> str:
        if self.is_ollama:
            return f"{self.base_url}/api/chat"
        return f"{self.base_url}/chat/completions"

    @property
    def auth_headers(self) -> dict[str, str]:
        if self.api_key:
            return {"Authorization": f"Bearer {self.api_key}"}
        return {}


@dataclass
class DiffHunk:
    """A single diff hunk that can be independently staged via git apply --cached."""
    hunk_id: str        # e.g. "src/main.py#1" or "src/main.py#0" for untracked new files
    file_path: str      # repository-relative path
    patch: str          # complete patch text for git apply --cached; empty for untracked
    is_untracked: bool = False


@dataclass
class ToolCall:
    """Represents a tool call extracted from an LLM response."""
    name: str
    arguments: dict
    call_id: str | None = None


# ── Tool decorator ─────────────────────────────────────────────────────────────
#
# Defining a new tool:
#   1. Define argument types as pydantic BaseModel subclasses
#   2. Define a top-level args model (also a BaseModel) for the tool
#   3. Decorate a function with @tool(ArgsModel)
#
# Schema generation, validation, and parsing are all handled automatically.

def tool(args_model: type[BaseModel]):
    """Decorator factory. @tool(MyArgsModel) attaches LLM tool metadata to a function."""
    def decorator(fn: Callable) -> Callable:
        spec = {
            "type": "function",
            "function": {
                "name": fn.__name__,
                "description": (fn.__doc__ or "").strip(),
                "parameters": args_model.model_json_schema(),
            },
        }
        fn._tool_spec = spec
        fn._tool_model = args_model
        return fn
    return decorator


def _summarize_args(arguments: dict, max_len: int = 80) -> str:
    """Return a compact one-line summary of tool call arguments for logging."""
    raw = json.dumps(arguments, separators=(",", ":"))
    if len(raw) <= max_len:
        return raw
    return raw[:max_len - 3] + "..."


def _total_message_chars(messages: list[dict]) -> int:
    """Count total characters across all message content fields."""
    return sum(len(str(m.get("content", "") or "")) for m in messages)


# ── LLM client ────────────────────────────────────────────────────────────────

@dataclass
class TokenUsage:
    """Accumulated token usage across all API calls in a session."""
    prompt_tokens: int = 0
    completion_tokens: int = 0

    def __add__(self, other: "TokenUsage") -> "TokenUsage":
        return TokenUsage(
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            completion_tokens=self.completion_tokens + other.completion_tokens,
        )

    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.completion_tokens

    def __str__(self) -> str:
        return (f"{self.total_tokens:,} tokens "
                f"({self.prompt_tokens:,} in / {self.completion_tokens:,} out)")


class LLMClient:
    """Encapsulates all communication with the LLM API (Ollama or OpenAI-compatible)."""

    def __init__(self, config: ApiConfig):
        self.config = config
        self.usage = TokenUsage()

    def _build_payload(self, prompt: str, tools: list[dict] | None = None,
                       stream: bool = False) -> dict:
        """Build an API request payload, adapting to Ollama or OpenAI format."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
        else:
            payload = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
                # Force the model to call the specific tool
                payload["tool_choice"] = {
                    "type": "function",
                    "function": {"name": tools[0]["function"]["name"]},
                }
        return payload

    def _extract_tool_args(self, data: dict) -> dict | None:
        """Extract tool-call arguments from a response, handling both API formats."""
        if self.config.is_ollama:
            tool_calls = data.get("message", {}).get("tool_calls", [])
            if tool_calls:
                raw = tool_calls[0]["function"]["arguments"]
                return json.loads(raw) if isinstance(raw, str) else raw
            # Fallback: content as JSON
            content = data.get("message", {}).get("content", "").strip()
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            msg = choices[0].get("message", {})
            tool_calls = msg.get("tool_calls", [])
            if tool_calls:
                raw = tool_calls[0]["function"]["arguments"]
                return json.loads(raw) if isinstance(raw, str) else raw
            content = msg.get("content", "").strip()

        # Fallback: parse content as JSON (both backends)
        if content:
            content = content.removeprefix("```json").removeprefix("```").strip().removesuffix("```").strip()
            return json.loads(content)
        return None

    def _build_payload_messages(self, messages: list[dict],
                                tools: list[dict] | None = None,
                                tool_choice: str | dict | None = None,
                                stream: bool = False) -> dict:
        """Build a payload from a full conversation history (messages list)."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
            # Ollama does not support tool_choice — omit it
        else:
            payload = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
            if tool_choice is not None:
                payload["tool_choice"] = tool_choice
        return payload

    def _extract_tool_call(self, data: dict) -> ToolCall | None:
        """Extract a tool call from an API response, handling both Ollama and OpenAI formats."""
        if self.config.is_ollama:
            tool_calls = data.get("message", {}).get("tool_calls", [])
            if tool_calls:
                tc = tool_calls[0]
                raw = tc["function"]["arguments"]
                args = json.loads(raw) if isinstance(raw, str) else raw
                return ToolCall(
                    name=tc["function"]["name"],
                    arguments=args,
                    call_id=tc.get("id"),
                )
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            msg = choices[0].get("message", {})
            tool_calls = msg.get("tool_calls", [])
            if tool_calls:
                tc = tool_calls[0]
                raw = tc["function"]["arguments"]
                args = json.loads(raw) if isinstance(raw, str) else raw
                return ToolCall(
                    name=tc["function"]["name"],
                    arguments=args,
                    call_id=tc.get("id"),
                )
        return None

    def _extract_text_content(self, data: dict) -> str | None:
        """Extract plain text content from a response when no tool call is present."""
        if self.config.is_ollama:
            content = data.get("message", {}).get("content", "").strip()
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            content = choices[0].get("message", {}).get("content", "").strip()
        return content or None

    def _extract_reasoning(self, data: dict) -> str | None:
        """Extract reasoning/thinking tokens from an API response if present.

        Handles the two common OpenRouter formats:
        - message.reasoning        (most thinking models)
        - message.reasoning_content (DeepSeek-R1 style)
        """
        if self.config.is_ollama:
            return None
        choices = data.get("choices", [])
        if not choices:
            return None
        msg = choices[0].get("message", {})
        reasoning = msg.get("reasoning") or msg.get("reasoning_content")
        return reasoning.strip() if reasoning else None

    def _format_assistant_tool_call(self, data: dict) -> dict:
        """Extract the assistant message from a raw API response for conversation history.

        Using the raw response message guarantees the format Ollama/OpenAI expects to see back.
        """
        if self.config.is_ollama:
            return data.get("message", {"role": "assistant", "content": ""})
        return data.get("choices", [{}])[0].get("message", {"role": "assistant", "content": ""})

    def _format_tool_result(self, call_id: str | None, content: str) -> dict:
        """Build a tool-result message for appending to conversation history."""
        if call_id is not None and not self.config.is_ollama:
            return {"role": "tool", "tool_call_id": call_id, "content": content}
        return {"role": "tool", "content": content}

    def _extract_usage(self, data: dict) -> TokenUsage:
        """Extract token usage from an API response (Ollama or OpenAI format)."""
        if self.config.is_ollama:
            return TokenUsage(
                prompt_tokens=data.get("prompt_eval_count", 0),
                completion_tokens=data.get("eval_count", 0),
            )
        usage = data.get("usage", {})
        return TokenUsage(
            prompt_tokens=usage.get("prompt_tokens", 0),
            completion_tokens=usage.get("completion_tokens", 0),
        )

    def call(self, prompt: str, num_ctx: int | None = None) -> str:
        """Call the LLM with streaming and return the assistant message text."""
        payload = self._build_payload(prompt, stream=True)
        if num_ctx is not None and self.config.is_ollama:
            payload["options"]["num_ctx"] = num_ctx
        headers = {**self.config.auth_headers, "Content-Type": "application/json"}
        with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=60.0, write=30.0, pool=5.0)) as client:
            with client.stream("POST", self.config.chat_url, json=payload, headers=headers) as response:
                response.raise_for_status()
                chunks = []
                for line in response.iter_lines():
                    if not line:
                        continue
                    # OpenAI SSE format: lines prefixed with "data: "
                    if line.startswith("data: "):
                        line = line[6:]
                    if line.strip() == "[DONE]":
                        break
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    if self.config.is_ollama:
                        content = data.get("message", {}).get("content", "")
                        done = data.get("done", False)
                    else:
                        delta = (data.get("choices", [{}])[0]
                                 .get("delta", {}))
                        content = delta.get("content", "")
                        done = data.get("choices", [{}])[0].get("finish_reason") is not None
                    if content:
                        chunks.append(content)
                    if done:
                        self.usage = self.usage + self._extract_usage(data)
                        break
                return "".join(chunks)

    def call_with_tool(self, prompt: str, tool_fn: Callable, **tool_kwargs) -> Any:
        """Call the LLM forcing tool_fn to be called.

        Parses and validates the response into the tool's args model, then calls
        tool_fn(validated_args, **tool_kwargs) and returns its result.
        """
        spec = tool_fn._tool_spec
        args_model: type[BaseModel] = tool_fn._tool_model
        actual_prompt = prompt
        retries = 0
        failures = []
        while True:
            retries += 1
            payload = self._build_payload(actual_prompt, tools=[spec], stream=False)
            headers = {**self.config.auth_headers, "Content-Type": "application/json"}
            with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
                response = client.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()
            self.usage = self.usage + self._extract_usage(data)

            raw_args = self._extract_tool_args(data)
            if raw_args is None:
                raw_args = {}

            # Fix stringified nested values: some models return e.g.
            # {"commits": "[{'subject': ...}]"} instead of a proper nested structure.
            # Detect strings that look like Python lists/dicts and try to parse them.
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass  # leave as-is, let pydantic report the error

            try:
                validated = args_model.model_validate(raw_args).post_process()
                return tool_fn(validated, **tool_kwargs)
            except ValidationError as e:
                failures.append(str(e))
                if retries > 5:
                    raise

                failure_summary = "\n".join(set(failures))
                actual_prompt = textwrap.dedent(f"""
                Your previous ({len(failures)}) attempts failed with a validation error:
                {failure_summary}

                Try again following this prompt exactly:
                {prompt}
                """)

    def agentic_loop(self, system_prompt: str, initial_user_message: str,
                     tool_registry: dict[str, Callable],
                     terminal_tool: Callable, max_turns: int = MAX_AGENTIC_TURNS,
                     **kwargs) -> Any:
        """Run a multi-turn agentic loop until the terminal tool is called.

        Each turn: send messages → extract tool call → dispatch.
        Investigation tool results are appended to history.
        Validation errors for the terminal tool become tool-result messages.
        On the last turn, only the terminal tool is offered.

        Context management: when total message size exceeds CONTEXT_TRIM_THRESHOLD,
        old tool-result messages are replaced with LLM-generated summaries. The
        original full content is stored and accessible via the query_tool_result
        tool so the model can ask targeted questions without re-fetching.
        A countdown warning is injected when TURNS_WARN_AT turns remain.
        """
        # ── result storage for compressed tool results ─────────────────────────
        result_store: dict[str, str] = {}  # result_id → original full content

        def _summarize_and_store(msg_idx: int, content: str) -> str:
            """Summarize a tool result via LLM, store the original, return summary."""
            result_id = f"r{msg_idx}"
            result_store[result_id] = content
            prompt = textwrap.dedent(f"""\
                Summarize this tool output in 2-4 sentences. Focus on filenames,
                key changes, and any suspicious code. Preserve suspicious code verbatim.

                {content[:TOOL_RESULT_SUMMARIZE_INPUT]}
            """)
            summary = self.call(prompt, num_ctx=HUNK_SUMMARIZE_CTX)
            return (
                f"[Summarized — id: '{result_id}'. "
                f"Use query_tool_result to ask specific questions about the full output.]\n"
                f"{summary.strip()}"
            )

        def _trim_with_summaries(messages: list[dict]) -> None:
            """Summarize oldest large tool results until total context is under threshold."""
            for i, msg in enumerate(messages):
                if _total_message_chars(messages) <= CONTEXT_TRIM_THRESHOLD:
                    break
                if msg.get("role") != "tool":
                    continue
                content = msg.get("content", "")
                if not isinstance(content, str):
                    continue
                if content.startswith("[Summarized"):  # already summarized
                    continue
                if len(content) <= TOOL_RESULT_SUMMARIZE_SKIP:
                    continue
                _log(f"  {_ansi('⋯', _ANSI_DIM)} compressing old result #{i} ({len(content):,} chars)")
                messages[i] = {**msg, "content": _summarize_and_store(i, content)}

        # ── query_tool_result: closure over result_store and self ───────────────
        @tool(QueryResultArgs)
        def query_tool_result(args: QueryResultArgs, **_: Any) -> str:
            """Access the full (unsummarized) content of a previously summarized tool result.

            When a tool result was compressed to save context, use this to ask a
            targeted question. The answer is generated from the original full content
            so no information is lost.
            """
            full = result_store.get(args.result_id)
            if full is None:
                available = list(result_store.keys()) or ["none"]
                return (
                    f"No stored result with id '{args.result_id}'. "
                    f"Available ids: {available}"
                )
            prompt = textwrap.dedent(f"""\
                The following is the full content of stored tool result '{args.result_id}':

                {full[:50_000]}

                Question: {args.question}

                Answer concisely and directly based only on the content above.
            """)
            return self.call(prompt, num_ctx=HUNK_SUMMARIZE_CTX)

        # ── build extended registry that includes query_tool_result ─────────────
        extended_registry = dict(tool_registry)
        extended_registry["query_tool_result"] = query_tool_result

        messages: list[dict] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": initial_user_message},
        ]

        terminal_spec = terminal_tool._tool_spec
        terminal_name: str = terminal_spec["function"]["name"]
        terminal_model: type[BaseModel] = terminal_tool._tool_model
        all_tool_specs = [fn._tool_spec for fn in extended_registry.values()] + [terminal_spec]

        headers = {**self.config.auth_headers, "Content-Type": "application/json"}

        for turn in range(1, max_turns + 1):
            is_last_turn = (turn == max_turns)
            turns_remaining = max_turns - turn

            if is_last_turn:
                tools = [terminal_spec]
                tool_choice: str | dict | None = (
                    None if self.config.is_ollama
                    else {"type": "function", "function": {"name": terminal_name}}
                )
            else:
                tools = all_tool_specs
                tool_choice = None if self.config.is_ollama else "auto"

            payload = self._build_payload_messages(messages, tools=tools,
                                                   tool_choice=tool_choice, stream=False)

            with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
                response = client.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()
            self.usage = self.usage + self._extract_usage(data)

            reasoning = self._extract_reasoning(data)
            if reasoning:
                _log_reasoning(reasoning)

            tool_call = self._extract_tool_call(data)

            if tool_call is None:
                text = self._extract_text_content(data) or ""
                _log(
                    f"  {_ansi('⟳', _ANSI_DIM)} {_ansi(f'[{turn}/{max_turns}]', _ANSI_DIM)}"
                    f" {_ansi('(no tool call — nudging)', _ANSI_DIM)}"
                )
                messages.append({"role": "assistant", "content": text})
                messages.append({
                    "role": "user",
                    "content": (
                        f"Please call a tool. Use the investigation tools to gather more "
                        f"information, or call {terminal_name} if you are ready to propose commits."
                    ),
                })
                continue

            _log(
                f"  {_ansi('⟳', _ANSI_DIM)} {_ansi(f'[{turn}/{max_turns}]', _ANSI_DIM)}"
                f" {_ansi(tool_call.name, _ANSI_CYAN)}"
                f"({_ansi(_summarize_args(tool_call.arguments), _ANSI_DIM)})"
            )
            messages.append(self._format_assistant_tool_call(data))

            # Coerce stringified nested values (some models return JSON-as-string)
            raw_args = dict(tool_call.arguments)
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass

            if tool_call.name == terminal_name:
                try:
                    validated = terminal_model.model_validate(raw_args).post_process()
                    return terminal_tool(validated, **kwargs)
                except ValidationError as e:
                    messages.append(self._format_tool_result(
                        tool_call.call_id,
                        f"Validation error: {e}\nPlease fix the arguments and call "
                        f"{terminal_name} again.",
                    ))
                    continue

            tool_fn = extended_registry.get(tool_call.name)
            if tool_fn is None:
                result_str = (f"Unknown tool: {tool_call.name!r}. "
                              f"Available: {list(extended_registry.keys())}")
            else:
                try:
                    validated_args = tool_fn._tool_model.model_validate(raw_args).post_process()
                    result = tool_fn(validated_args, **kwargs)
                    result_str = result if isinstance(result, str) else json.dumps(result)
                except Exception as e:
                    result_str = f"Error calling {tool_call.name}: {e}"

            messages.append(self._format_tool_result(tool_call.call_id, result_str))

            # Context management: when messages are large, summarize old tool results
            # with the LLM and store originals for query_tool_result access.
            total_chars = _total_message_chars(messages)
            if total_chars > CONTEXT_TRIM_THRESHOLD:
                _log(f"  {_ansi('⋯', _ANSI_DIM)} context {total_chars:,} chars — compressing old results")
                _trim_with_summaries(messages)

            # Inject a countdown warning when few turns remain so the model
            # knows it must call the terminal tool soon.
            if 0 < turns_remaining <= TURNS_WARN_AT:
                messages.append({
                    "role": "user",
                    "content": (
                        f"WARNING: Only {turns_remaining} turn(s) remaining. "
                        f"Stop gathering context and call {terminal_name} NOW with "
                        f"your best grouping based on what you already know."
                    ),
                })

        raise RuntimeError(
            f"agentic_loop exceeded {max_turns} turns without calling {terminal_name}"
        )



# ── Git helpers ────────────────────────────────────────────────────────────────

def _git(args: list[str], cwd: Path) -> str:
    """Run a git command and return stdout. Raises on non-zero exit."""
    result = subprocess.run(
        ["git"] + args,
        cwd=cwd,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed:\n{result.stderr.strip()}")
    return result.stdout


def read_git_config(repo: Path) -> dict[str, str]:
    """Read smart-commit.* keys from git config (local → global → system).

    Returns a dict of whichever keys are set; missing keys are absent.
    Keys: smart-commit.model, smart-commit.api-base, smart-commit.api-key
    """
    keys = ["smart-commit.model", "smart-commit.api-base", "smart-commit.api-key"]
    config: dict[str, str] = {}
    for key in keys:
        result = subprocess.run(
            ["git", "config", "--get", key],
            cwd=repo,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0 and result.stdout.strip():
            config[key] = result.stdout.strip()
    return config


def _prompt(message: str, default: str | None = None) -> str | None:
    """Prompt the user for input, returning the default on empty entry.

    Returns None if the user presses Ctrl-C or Ctrl-D.
    """
    try:
        raw = input(message).strip()
    except (KeyboardInterrupt, EOFError):
        return None
    return raw if raw else default


def run_setup_wizard(repo: Path | None) -> None:
    """Interactive wizard to configure git-smart-commit via git config.

    Prompts for API backend, model, API key, and config scope (global / local).
    Writes settings with ``git config``.

    Args:
        repo: Path to a git repository (required for local scope).
              May be None when only global scope is needed.
    """
    print()
    print("git-smart-commit setup wizard")
    print("=" * 40)
    print()

    # ── Step 1: choose backend ────────────────────────────────────────────────
    print("Select API backend:")
    for i, b in enumerate(SETUP_BACKENDS, 1):
        marker = "  ← default" if i == 1 else ""
        print(f"  [{i}] {b['label']}{marker}")
    print()

    backend_choice = _prompt("Choice [1]: ", default="1")
    if backend_choice is None:
        print("\nSetup cancelled.")
        return
    try:
        backend_idx = int(backend_choice) - 1
        if not 0 <= backend_idx < len(SETUP_BACKENDS):
            raise ValueError
    except ValueError:
        print("Invalid choice. Setup cancelled.")
        return

    backend = SETUP_BACKENDS[backend_idx]
    print()

    # ── Step 2: API base URL (custom only) ───────────────────────────────────
    api_base: str
    if backend["api_base"] is None:
        val = _prompt("API base URL: ")
        if not val:
            print("URL required. Setup cancelled.")
            return
        api_base = val.rstrip("/")
    else:
        api_base = backend["api_base"]

    # ── Step 3: model ─────────────────────────────────────────────────────────
    default_model: str = backend["default_model"] or ""
    model_prompt = f"Model [{default_model}]: " if default_model else "Model: "
    model_val = _prompt(model_prompt, default=default_model)
    if model_val is None:
        print("\nSetup cancelled.")
        return
    if not model_val:
        print("Model name required. Setup cancelled.")
        return
    model = model_val

    # ── Step 4: API key ───────────────────────────────────────────────────────
    api_key: str | None = None
    needs_key: bool | None = backend["needs_key"]
    if needs_key is True:
        try:
            api_key = getpass.getpass("API key (input hidden): ").strip() or None
        except (KeyboardInterrupt, EOFError):
            print("\nSetup cancelled.")
            return
        if not api_key:
            print("API key required for this backend. Setup cancelled.")
            return
    elif needs_key is None:
        # custom endpoint — key is optional
        try:
            raw_key = getpass.getpass("API key (leave empty if not needed, input hidden): ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nSetup cancelled.")
            return
        api_key = raw_key or None

    print()

    # ── Step 5: config scope ──────────────────────────────────────────────────
    print("Save configuration to:")
    print("  [1] Global  (~/.gitconfig)  ← affects all repos  [default]")
    if repo is not None:
        print("  [2] Local   (.git/config)   ← this repo only")
    print()

    scope_choice = _prompt("Choice [1]: ", default="1")
    if scope_choice is None:
        print("\nSetup cancelled.")
        return

    scope_flag: str
    if scope_choice == "2" and repo is not None:
        scope_flag = "--local"
        scope_label = f"local ({repo})"
    else:
        scope_flag = "--global"
        scope_label = "global (~/.gitconfig)"

    print()

    # ── Step 6: write config ──────────────────────────────────────────────────
    cwd = repo if repo is not None else Path.cwd()

    settings: list[tuple[str, str]] = [
        ("smart-commit.api-base", api_base),
        ("smart-commit.model", model),
    ]
    if api_key:
        settings.append(("smart-commit.api-key", api_key))

    print(f"Writing {scope_label} config...")
    for key, value in settings:
        cmd = ["git", "config", scope_flag, key, value]
        result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"  Error setting {key}: {result.stderr.strip()}", file=sys.stderr)
            sys.exit(1)
        # Redact the API key in output
        display_value = "***" if "api-key" in key else value
        print(f"  git config {scope_flag} {key} {display_value}")

    print()
    print("Done! Run git-smart-commit to use your new settings.")
    if api_key:
        print()
        print("Tip: your API key is stored in plaintext in git config.")
        print("     Consider using the LLM_API_KEY environment variable instead.")
    print()


def split_hunks(diff_text: str) -> list[str]:
    """Split a unified diff into individual hunks (each starting with @@)."""
    lines = diff_text.splitlines(keepends=True)
    hunks: list[str] = []
    header_lines: list[str] = []
    current: list[str] = []

    for line in lines:
        if line.startswith("@@"):
            if current:
                hunks.append("".join(current))
            current = header_lines + [line]
        elif line.startswith(("diff --git", "index ", "--- ", "+++ ")):
            header_lines.append(line)
            if current:
                # flush any open hunk first
                hunks.append("".join(current))
                current = []
        else:
            if current:
                current.append(line)
            # lines before the first @@ go into header_lines
            elif not line.startswith("@@"):
                header_lines.append(line)

    if current:
        hunks.append("".join(current))

    return hunks or [diff_text]   # fallback: treat whole diff as one hunk


# ── Tool definitions ───────────────────────────────────────────────────────────

class Issue(BaseModel):
    message: str = Field(description="REQUIRED. A brief description of a code issue, including a line number")
    path: str = Field(description="REQUIRED. The path affected by the issue")

class Commit(BaseModel):
    subject: str = Field(description="REQUIRED. Conventional commit subject line (type(scope): desc), under 72 chars. Add '!' after the type when a breaking change is present (e.g. 'feat!(api): rename endpoint').")
    hunks: list[str] = Field(description="REQUIRED. Hunk IDs to include in this commit, e.g. ['src/main.py#1', 'src/main.py#3']. New untracked files use 'filepath#0'.")
    body: str = Field(default="", description="4-10 line plain-text commit body, no markdown")
    breaking_change: str = Field(default="", description="If this commit removes or renames a public API, changes a function signature incompatibly, or deletes an exported symbol, describe what broke and how callers must migrate. Leave empty if there is no breaking change.")
    issues: list[Issue] = Field(default_factory=list, description="Issues noticed in the code being committed")

    def post_process(self):
        self.body = wrap_markdown(self.body, 80)
        self.breaking_change = wrap_markdown(self.breaking_change, 80)
        return self

class ProposeCommitsArgs(BaseModel):
    commits: list[Commit] = Field(description="REQUIRED. Logical commit groups")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commits = [commit.post_process() for commit in self.commits]
        return self

sample_issue = Issue(
    message="Unused import 'os' on line 1",
    path="main.py"
)

sample_commit = Commit(
    subject="refactor(main): cleanup imports",
    hunks=["main.py#1"],
    body="Removed unnecessary imports to improve load time and code cleanliness.",
    issues=[sample_issue]
)

SAMPLE_OUTPUT = ProposeCommitsArgs(
    commits=[sample_commit],
    gitignore=["*.pyc", "__pycache__/"]
)


@tool(ProposeCommitsArgs)
def propose_commits(result: ProposeCommitsArgs, analyzer: "GitAnalyzer") -> tuple[list[dict], list[str]]:
    """Group working-tree changes into logical commits and suggest .gitignore patterns for junk files."""
    commits = [c.model_dump() for c in result.commits]
    if not commits:
        raise ValueError("Model returned no valid commit groups.")

    commits = analyzer.merge_overlapping_commits(commits)
    return commits, result.gitignore


class MergeCommitsArgs(BaseModel):
    commit: Commit = Field(description="REQUIRED. A single merged commit covering all the provided changes")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commit = self.commit.post_process()
        return self


@tool(MergeCommitsArgs)
def merge_commits(result: MergeCommitsArgs, all_hunks: list[str], all_issues: list[dict]) -> dict:
    """Merge multiple overlapping commits into a single coherent commit."""
    commit = result.commit.model_dump()
    # Ensure all hunks and issues are preserved regardless of what the model returns
    commit["hunks"] = all_hunks
    commit["issues"] = all_issues
    return commit


class ReadFileArgs(BaseModel):
    path: str = Field(description="Repository-relative path to the file to read")

    def post_process(self): return self


@tool(ReadFileArgs)
def read_file(args: ReadFileArgs, analyzer: "GitAnalyzer") -> str:
    """Read the full current content of a file in the repository.

    Use this when the diff alone lacks context — e.g. to see the surrounding
    class structure, existing imports, or unchanged code near a changed line.
    The result is truncated at 50,000 characters for very large files.
    """
    return analyzer._read_file(args.path)


class GetDiffArgs(BaseModel):
    path: str = Field(description="Repository-relative path of the file")
    staged: bool | None = Field(
        default=None,
        description="true=staged only, false=unstaged only, null/omit=both",
    )

    def post_process(self): return self


@tool(GetDiffArgs)
def get_diff(result: GetDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Get the raw unified diff for a specific file (staged, unstaged, or both)."""
    parts = []

    def _fetch(extra_args: list[str], label: str) -> None:
        try:
            diff = analyzer.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", result.path])
            if diff.strip():
                parts.append(f"=== {label} ===\n{diff}" if result.staged is None else diff)
        except RuntimeError as e:
            parts.append(f"Error ({label}): {e}")

    if result.staged is None:
        _fetch(["--cached"], "staged")
        _fetch([], "unstaged")
    elif result.staged:
        _fetch(["--cached"], "staged")
    else:
        _fetch([], "unstaged")

    result_text = "\n".join(parts) if parts else f"No diff found for {result.path}"

    if len(result_text) > DIFF_CHARS_PER_FILE:
        result_text = result_text[:DIFF_CHARS_PER_FILE] + f"\n\n... (truncated at {DIFF_CHARS_PER_FILE} chars)"

    return result_text


class GetGitLogArgs(BaseModel):
    n: int = Field(default=20, description="Number of recent commits to return (max 50)")

    def post_process(self): return self


@tool(GetGitLogArgs)
def get_git_log(result: GetGitLogArgs, analyzer: "GitAnalyzer") -> str:
    """Get recent commit history to understand this project's commit style conventions."""
    n = min(result.n, 50)
    try:
        log = analyzer.git(["log", f"--max-count={n}", "--oneline", "--no-color"])
        return log.strip() or "No commits found."
    except RuntimeError as e:
        return f"Error: {e}"


class SearchDiffArgs(BaseModel):
    pattern: str = Field(description="Python regex pattern to search for across all diffs")
    context_lines: int = Field(default=2, description="Lines of context around each match")

    def post_process(self): return self


@tool(SearchDiffArgs)
def search_diff(result: SearchDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Search for a regex pattern across all staged and unstaged diffs."""
    try:
        compiled = re.compile(result.pattern)
    except re.error as e:
        return f"Invalid regex: {e}"

    matches: list[dict] = []
    MAX_MATCHES = 50

    for extra_args in (["--cached"], []):
        if len(matches) >= MAX_MATCHES:
            break
        try:
            changed = analyzer.git(
                ["diff"] + extra_args + ["--name-only", "--no-ext-diff"]
            ).splitlines()
            for fname in changed:
                if len(matches) >= MAX_MATCHES:
                    break
                diff = analyzer.git(
                    ["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname]
                )
                lines = diff.splitlines()
                for i, line in enumerate(lines):
                    if compiled.search(line):
                        start = max(0, i - result.context_lines)
                        end = min(len(lines), i + result.context_lines + 1)
                        matches.append({
                            "file": fname,
                            "line_number": i + 1,
                            "match_context": "\n".join(lines[start:end]),
                        })
                        if len(matches) >= MAX_MATCHES:
                            break
        except RuntimeError:
            pass

    if not matches:
        return f"No matches found for pattern: {result.pattern!r}"

    result_lines = [
        f"File: {m['file']} (line {m['line_number']})\n{m['match_context']}"
        for m in matches
    ]
    text = "\n---\n".join(result_lines)
    if len(matches) >= MAX_MATCHES:
        text += f"\n... (limited to {MAX_MATCHES} matches)"
    return text


class QueryResultArgs(BaseModel):
    result_id: str = Field(
        description="The result_id shown in the summarized tool output (e.g. 'r5')")
    question: str = Field(
        description="Specific question to answer from the full stored result")

    def post_process(self): return self


MERGE_PROMPT = textwrap.dedent("""\
    You are merging multiple proposed commits that share the same diff hunks
    and therefore cannot be committed separately.

    Combine them into a SINGLE commit that:
    - Has one conventional commit subject line (type(scope): desc), under 72 chars
    - Has a body (4-10 lines) summarizing ALL the changes coherently
    - Lists ALL affected hunks (deduped)
    - Collects ALL issues (deduped)
    - If any commit is a feat type, it must take priority over the others (although the others should be mentioned in the commit body)
    - chore should only be used if no other category applies.
    - If any input commit has a non-empty breaking_change, set breaking_change on the merged commit to a combined description and add '!' to the type.

    Do NOT just concatenate the subjects. Write a new, coherent subject and body
    that covers the full set of changes as a single logical unit.

    Call the merge_commits tool with your answer.

    Here are the commits to merge:
""")


# ── GitAnalyzer ────────────────────────────────────────────────────────────────

class GitAnalyzer:
    """Encapsulates git operations and commit analysis for a repository."""

    def __init__(self, repo: Path, client: LLMClient):
        self.repo = repo
        self.client = client

    @property
    def config(self) -> ApiConfig:
        return self.client.config

    def git(self, args: list[str]) -> str:
        return _git(args, self.repo)

    def get_changed_files(self) -> list[str]:
        """Return list of files with unstaged or staged changes."""
        staged = self.git(["diff", "--cached", "--name-only"]).splitlines()
        unstaged = self.git(["diff", "--name-only"]).splitlines()
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        seen = set()
        files = []
        for f in staged + unstaged + untracked:
            if f and f not in seen:
                seen.add(f)
                files.append(f)
        return files

    def collect_hunks(self) -> dict[str, DiffHunk]:
        """Collect all diff hunks from the working tree, indexed by hunk ID.

        Any currently staged changes are first unstaged (git reset HEAD) so that
        everything is treated uniformly as unstaged working-tree changes.  This is
        safe: working-tree content is never modified.

        Returns a dict mapping hunk_id → DiffHunk.  Hunk IDs for unstaged changes
        look like "path/file.py#1"; untracked new files use "path/file.py#0".
        """
        hunk_map: dict[str, DiffHunk] = {}

        # Unstage any staged files so we work uniformly with unstaged diffs
        staged_files = self.git(["diff", "--cached", "--name-only"]).splitlines()
        if staged_files:
            self.git(["reset", "HEAD", "--"] + staged_files)

        # Split each unstaged file's diff into individual hunks
        changed = self.git(["diff", "--name-only", "--no-ext-diff"]).splitlines()
        for fpath in changed:
            diff = self.git(["diff", "--no-color", "--no-ext-diff", "--", fpath])
            if not diff.strip():
                continue
            file_hunks = split_hunks(diff)
            for i, patch in enumerate(file_hunks, 1):
                hunk_id = f"{fpath}#{i}"
                hunk_map[hunk_id] = DiffHunk(
                    hunk_id=hunk_id,
                    file_path=fpath,
                    patch=patch,
                )

        # Untracked (new) files are staged whole via git add — use #0 suffix
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        for fpath in untracked:
            hunk_id = f"{fpath}#0"
            hunk_map[hunk_id] = DiffHunk(
                hunk_id=hunk_id,
                file_path=fpath,
                patch="",
                is_untracked=True,
            )

        return hunk_map

    def build_hunk_context(self, hunk_map: dict[str, DiffHunk]) -> str:
        """Build the initial LLM message: hunk list with inline diff content.

        Small hunks are included verbatim; very large ones are summarized.
        Total output is capped at DIFF_TOTAL_CHARS.
        """
        parts: list[str] = []
        total_chars = 0

        # Header: compact hunk list
        hunk_list_lines = []
        for hunk_id, dh in hunk_map.items():
            if dh.is_untracked:
                hunk_list_lines.append(f"  {hunk_id}  (new untracked file)")
            else:
                adds = dh.patch.count("\n+") - dh.patch.count("\n+++")
                dels = dh.patch.count("\n-") - dh.patch.count("\n---")
                hunk_list_lines.append(f"  {hunk_id}  (+{adds}/-{dels})")
        parts.append("Available hunks:\n" + "\n".join(hunk_list_lines))

        # Inline diff content for each hunk
        diff_parts: list[str] = []
        for hunk_id, dh in hunk_map.items():
            if dh.is_untracked:
                diff_parts.append(f"--- {hunk_id} ---\n(new untracked file — no diff)")
                total_chars += 50
            elif len(dh.patch) <= DIFF_CHARS_PER_FILE:
                diff_parts.append(f"--- {hunk_id} ---\n{dh.patch}")
                total_chars += len(dh.patch)
            else:
                _log(f"  {_ansi('⋯', _ANSI_DIM)} summarizing large hunk: {hunk_id} ({len(dh.patch):,} chars)")
                summary = self._summarize_file_diff(dh.file_path, dh.patch)
                diff_parts.append(f"--- {hunk_id} ---\n{summary}")
                total_chars += len(summary)

            if total_chars >= DIFF_TOTAL_CHARS:
                diff_parts.append("\n... (remaining hunks omitted — total size limit reached)")
                break

        parts.append("\nDiff content:\n" + "\n\n".join(diff_parts))
        return "\n\n".join(parts)

    def build_file_metadata(self) -> str:
        """Return a compact stats-only summary: diff --stat for staged/unstaged + untracked list.

        No full diffs — the LLM requests specific diffs lazily via get_diff.
        """
        parts: list[str] = []
        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if stat:
                parts.append(f"{label}:\n{stat}")
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))
        return "\n\n".join(parts)

    def _summarize_file_diff(self, fname: str, diff_text: str) -> str:
        """Ask the model to summarize each hunk, passing the last N summaries as context."""
        hunks = split_hunks(diff_text)
        hunk_summaries: list[str] = []

        for i, hunk in enumerate(hunks, 1):
            prior_context = ""
            if hunk_summaries:
                window = hunk_summaries[-PRIOR_HUNK_WINDOW:]
                start_idx = i - len(window)
                prior_context = "Previous hunks in this file:\n" + "\n".join(
                    f"  Hunk {start_idx + j}: {s}" for j, s in enumerate(window)
                ) + "\n\n"

            prompt = textwrap.dedent(f"""\
                Summarize hunk {i} of {len(hunks)} from '{fname}' in 1-3 plain English sentences.
                Focus on WHAT changed. Preserve verbatim any code that looks buggy or suspicious
                (wrong arguments, misused APIs, incorrect syntax). Output only the summary.

                {prior_context}Hunk {i}:
                {hunk}
            """)

            summary = self.client.call(prompt, num_ctx=HUNK_SUMMARIZE_CTX)
            hunk_summaries.append(summary.strip())

        print()
        return f"[summarized — {len(hunks)} hunk(s)]\n" + "\n".join(
            f"  Hunk {i}: {s}" for i, s in enumerate(hunk_summaries, 1)
        )

    def build_diff_summary(self) -> str:
        """Return diff context for the classifier.

        Small files: include raw diff.
        Large files: summarize each hunk via a fast model call, then include summaries.
        """
        parts: list[str] = []
        total_chars = 0

        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if not stat:
                continue
            parts.append(f"{label}:\n{stat}\n")

            changed = self.git(["diff"] + extra_args + ["--name-only", "--no-ext-diff"]).splitlines()
            for fname in changed:
                file_diff = self.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname])

                if len(file_diff) <= DIFF_CHARS_PER_FILE:
                    content = file_diff
                else:
                    _log(f"  {_ansi('⋯', _ANSI_DIM)} summarizing large diff: {fname} ({len(file_diff):,} chars)")
                    content = self._summarize_file_diff(fname, file_diff)

                parts.append(content)
                total_chars += len(content)

                if total_chars >= DIFF_TOTAL_CHARS:
                    parts.append("\n... (remaining diffs omitted — total size limit reached)")
                    break
            else:
                continue
            break   # hit total limit inside inner loop

        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))

        return "\n".join(parts)

    def _read_file(self, path: str) -> str:
        """Read a repo file, rejecting path traversal and truncating large files."""
        target = (self.repo / path).resolve()
        if not str(target).startswith(str(self.repo) + os.sep) and target != self.repo:
            return f"[error: path traversal rejected for '{path}']"
        if not target.exists():
            return f"[file not found: {path}]"
        try:
            content = target.read_text(errors="replace")
        except OSError as e:
            return f"[error reading {path}: {e}]"
        if len(content) > READ_FILE_LIMIT:
            size = target.stat().st_size
            return (
                content[:READ_FILE_LIMIT]
                + f"\n\n[truncated — file is {size:,} bytes; showing first {READ_FILE_LIMIT:,} chars]"
            )
        return content

    def classify_changes(self, hunk_map: dict[str, DiffHunk], critique: str = "") -> tuple[list[dict], list[str]]:
        """Ask the model to group diff hunks into logical commits via an agentic loop.

        Returns (commits, gitignore_patterns).
        """
        hunk_context = self.build_hunk_context(hunk_map)

        critique_section = ""
        if critique:
            critique_section = (
                f"\n\nA reviewer rated the previous proposal below 7/10. Their feedback:\n"
                f"{critique}\n"
                f"Revise your commit groupings to address this feedback."
            )

        initial_message = hunk_context + critique_section

        tool_registry = {
            "read_file": read_file,
            "get_diff": get_diff,
            "get_git_log": get_git_log,
            "search_diff": search_diff,
        }

        # Scale turn limit with changeset size: allow ~2 extra turns per 5 hunks,
        # capped at MAX_AGENTIC_TURNS_CAP to avoid runaway loops.
        n_hunks = len(hunk_map)
        max_turns = min(MAX_AGENTIC_TURNS + (n_hunks // 5) * 2, MAX_AGENTIC_TURNS_CAP)
        if max_turns > MAX_AGENTIC_TURNS:
            _log(f"  {_ansi(f'[{n_hunks} hunks — up to {max_turns} turns]', _ANSI_DIM)}")

        return self.client.agentic_loop(
            system_prompt=AGENTIC_SYSTEM_PROMPT,
            initial_user_message=initial_message,
            tool_registry=tool_registry,
            terminal_tool=propose_commits,
            max_turns=max_turns,
            analyzer=self,
        )

    def merge_overlapping_commits(self, commits: list[dict]) -> list[dict]:
        """Merge commits that share hunk IDs (duplicate assignment is a model error).

        Different commits CAN touch different hunks of the same file — that is the
        whole point of hunk-level staging.  We only need to merge when the same
        hunk_id appears in more than one commit, since applying the same patch twice
        would fail.
        """
        if len(commits) <= 1:
            return commits

        # Build a mapping from each hunk_id to the commit indices that claim it
        hunk_to_indices: dict[str, list[int]] = {}
        for i, c in enumerate(commits):
            for h in c.get("hunks", []):
                hunk_to_indices.setdefault(h, []).append(i)

        # Union-find to group commits sharing a hunk
        parent = list(range(len(commits)))

        def find(x):
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a, b):
            ra, rb = find(a), find(b)
            if ra != rb:
                parent[rb] = ra

        for indices in hunk_to_indices.values():
            for idx in indices[1:]:
                union(indices[0], idx)

        # Group commits by their root
        groups: dict[int, list[int]] = {}
        for i in range(len(commits)):
            groups.setdefault(find(i), []).append(i)

        # If no merging needed, return as-is
        if all(len(idxs) == 1 for idxs in groups.values()):
            return commits

        merged = []
        for indices in groups.values():
            if len(indices) == 1:
                merged.append(commits[indices[0]])
                continue

            group = [commits[i] for i in indices]
            _log(f"  {_ansi('⋯', _ANSI_DIM)} merging {len(group)} commits with overlapping hunks")

            all_hunks = list(dict.fromkeys(h for c in group for h in c.get("hunks", [])))
            all_issues = []
            seen_issues: set[tuple] = set()
            for c in group:
                for issue in c.get("issues", []):
                    key = (issue.get("path", ""), issue.get("message", ""))
                    if key not in seen_issues:
                        seen_issues.add(key)
                        all_issues.append(issue)

            commits_desc = json.dumps(group, indent=2)
            prompt = MERGE_PROMPT + commits_desc

            merged.append(self.client.call_with_tool(
                prompt, merge_commits,
                all_hunks=all_hunks, all_issues=all_issues,
            ))

        return merged

    def _build_combined_patch(self, hunks: list[str]) -> str:
        """Combine multiple hunks for the same file into a single valid patch.

        The first hunk's file header is kept; subsequent hunks contribute only
        their @@ sections onward, so git apply sees one coherent multi-hunk patch.
        """
        if len(hunks) == 1:
            return hunks[0]

        result: list[str] = []
        # Keep the full first hunk (file header + @@ lines)
        result.append(hunks[0])
        if not result[-1].endswith("\n"):
            result.append("\n")

        for hunk in hunks[1:]:
            # Skip file-header lines; start from the first @@ line
            in_hunk = False
            for line in hunk.splitlines(keepends=True):
                if line.startswith("@@"):
                    in_hunk = True
                if in_hunk:
                    result.append(line)
            if result and not result[-1].endswith("\n"):
                result.append("\n")

        return "".join(result)

    def execute_commits(self, commits: list[dict], hunk_map: dict[str, DiffHunk]) -> None:
        """Stage and commit each group using git apply --cached for individual hunks."""
        for i, commit in enumerate(commits, 1):
            subject = commit["subject"]
            body = commit.get("body", "").strip()
            breaking = commit.get("breaking_change", "").strip()
            parts = [subject]
            if body:
                parts.append(body)
            trailers = []
            if breaking:
                trailers.append(f"BREAKING CHANGE: {breaking}")
            trailers.append(f"Co-Authored-By: fiddlerwoaroof/git-smart-commit ({self.config.model})")
            parts.append("\n".join(trailers))
            message = "\n\n".join(parts)
            hunk_ids: list[str] = commit.get("hunks", [])

            print(f"\n[{i}/{len(commits)}] {subject}")

            # Group hunk IDs by file path (preserving order)
            by_file: dict[str, list[DiffHunk]] = {}
            untracked_paths: list[str] = []
            for hid in hunk_ids:
                dh = hunk_map.get(hid)
                if dh is None:
                    print(f"  ! Unknown hunk {hid!r}, skipping", file=sys.stderr)
                    continue
                if dh.is_untracked:
                    untracked_paths.append(dh.file_path)
                else:
                    by_file.setdefault(dh.file_path, []).append(dh)

            # Apply hunks per file via git apply --cached
            for fpath, file_hunks in by_file.items():
                patch_text = self._build_combined_patch([dh.patch for dh in file_hunks])
                with tempfile.NamedTemporaryFile(
                    mode="w", suffix=".patch", delete=False, encoding="utf-8"
                ) as f:
                    f.write(patch_text)
                    patch_file = f.name
                try:
                    self.git(["apply", "--cached", patch_file])
                    n = len(file_hunks)
                    print(f"  + {fpath} ({n} hunk{'s' if n != 1 else ''})")
                except RuntimeError as e:
                    print(f"  ! Could not apply patch for {fpath}: {e}", file=sys.stderr)
                finally:
                    os.unlink(patch_file)

            # Add untracked (new) files whole
            for fpath in untracked_paths:
                try:
                    self.git(["add", "--", fpath])
                    print(f"  + {fpath} (new file)")
                except RuntimeError as e:
                    print(f"  ! Could not add {fpath}: {e}", file=sys.stderr)

            # Check something is actually staged
            staged = self.git(["diff", "--cached", "--name-only"]).strip()
            if not staged:
                print("  (nothing staged, skipping)")
                continue

            try:
                self.git(["commit", "-m", message])
                print(f"  ✓ Committed")
            except RuntimeError as e:
                print(f"  ! Error committing: {e}", file=sys.stderr)
                raise

    def update_gitignore(self, patterns: list[str], yes: bool = False) -> None:
        """Offer to append suggested patterns to .gitignore, skipping any already present."""
        if not patterns:
            return

        gitignore_path = self.repo / ".gitignore"

        existing: set[str] = set()
        existing_content = ""
        if gitignore_path.exists():
            existing_content = gitignore_path.read_text()
            for line in existing_content.splitlines():
                stripped = line.strip()
                if stripped and not stripped.startswith("#"):
                    existing.add(stripped)

        new_patterns = [p for p in patterns if p not in existing]
        if not new_patterns:
            return

        print("\nSuggested .gitignore additions:")
        for p in new_patterns:
            print(f"  {p}")

        if not yes:
            try:
                answer = input("Add these to .gitignore? [y/N] ").strip().lower()
            except (KeyboardInterrupt, EOFError):
                return
            if answer not in ("y", "yes"):
                return

        with open(gitignore_path, "a") as f:
            if existing_content and not existing_content.endswith("\n"):
                f.write("\n")
            f.write("\n# Added by git-smart-commit\n")
            for p in new_patterns:
                f.write(f"{p}\n")

        print(f"  Updated .gitignore ({len(new_patterns)} new pattern(s)).")


# ── Terminal color helpers ──────────────────────────────────────────────────────

_ANSI_RESET  = "\033[0m"
_ANSI_BOLD   = "\033[1m"
_ANSI_DIM    = "\033[2m"
_ANSI_RED    = "\033[31m"
_ANSI_GREEN  = "\033[32m"
_ANSI_YELLOW = "\033[33m"
_ANSI_CYAN   = "\033[36m"


def _ansi(text: str, *codes: str) -> str:
    """Wrap text in ANSI escape codes when stdout is a TTY (no-op otherwise)."""
    if not sys.stdout.isatty():
        return text
    return "".join(codes) + text + _ANSI_RESET


def _diff_stats(patch: str) -> tuple[int, int]:
    """Count (added_lines, removed_lines) from a unified diff patch string."""
    added = removed = 0
    for line in patch.splitlines():
        if line.startswith("+") and not line.startswith("+++"):
            added += 1
        elif line.startswith("-") and not line.startswith("---"):
            removed += 1
    return added, removed


# ── Progress logging ────────────────────────────────────────────────────────────
#
# Progress/status messages go to stdout so they are visible in normal use
# (many shells redirect stderr to /dev/null). They are suppressed when
# _QUIET is True (set by --quiet or automatically when --json is active).

_QUIET: bool = False


def _log(msg: str) -> None:
    """Print a progress line to stdout unless quiet mode is active."""
    if not _QUIET:
        print(msg)


def _log_reasoning(text: str) -> None:
    """Print reasoning/thinking tokens dimmed and indented, unless quiet."""
    if _QUIET:
        return
    prefix = _ansi("  │ ", _ANSI_DIM)
    for line in text.splitlines():
        print(prefix + _ansi(line, _ANSI_DIM))


# ── Display helpers ────────────────────────────────────────────────────────────

def print_proposed_commits(commits: list[dict], gitignore: list[str],
                           hunk_map: dict[str, "DiffHunk"] | None = None) -> None:
    width = shutil.get_terminal_size((80, 24)).columns
    n = len(commits)
    print()
    for i, commit in enumerate(commits, 1):
        subject  = commit["subject"]
        body     = commit.get("body", "").strip()
        hunk_ids = commit.get("hunks", [])
        breaking = commit.get("breaking_change", "").strip()
        issues   = commit.get("issues") or []

        # ── box top ──────────────────────────────────────────────────────────
        label       = f"[{i}/{n}] "
        header_len  = len("┌─") + len(label) + len(subject) + 1
        fill        = max(1, width - header_len)
        print(
            "┌─"
            + _ansi(label, _ANSI_DIM)
            + _ansi(subject, _ANSI_BOLD)
            + " " + "─" * fill
        )

        # ── body ─────────────────────────────────────────────────────────────
        if body:
            print("│")
            for line in body.splitlines():
                print(f"│    {line}")

        # ── files ─────────────────────────────────────────────────────────────
        if hunk_ids:
            print("│")
            # Aggregate hunks per file (preserving order)
            by_file: dict[str, list] = {}
            for hid in hunk_ids:
                if hunk_map and hid in hunk_map:
                    dh = hunk_map[hid]
                    by_file.setdefault(dh.file_path, []).append(dh)
                else:
                    fpath = hid.rsplit("#", 1)[0] if "#" in hid else hid
                    by_file.setdefault(fpath, [])

            max_path = max((len(fp) for fp in by_file), default=0)
            for fpath, hunks in by_file.items():
                total_added = total_removed = 0
                is_new = any(getattr(dh, "is_untracked", False) for dh in hunks)
                for dh in hunks:
                    if dh is not None and dh.patch:
                        a, r = _diff_stats(dh.patch)
                        total_added   += a
                        total_removed += r

                if not hunks:
                    # No hunk data available — neutral display
                    print(f"│    {_ansi('~', _ANSI_YELLOW)} {fpath}")
                    continue

                if is_new:
                    marker, color = "+", _ANSI_GREEN
                elif total_added == 0 and total_removed > 0:
                    marker, color = "×", _ANSI_RED
                else:
                    marker, color = "~", _ANSI_YELLOW

                padding   = " " * (max_path - len(fpath))
                stats_str = _ansi(f"+{total_added} -{total_removed}", _ANSI_DIM)
                print(f"│    {_ansi(marker, color)} {_ansi(fpath, color)}{padding}  {stats_str}")

        # ── breaking change ───────────────────────────────────────────────────
        if breaking:
            print("│")
            print(f"│  {_ansi('⚠ BREAKING CHANGE:', _ANSI_RED + _ANSI_BOLD)} {breaking}")

        # ── issues ────────────────────────────────────────────────────────────
        if issues:
            print("│")
            print(f"│  {_ansi('Issues:', _ANSI_YELLOW)}")
            for issue in issues:
                print(f"│    - {issue['path']}: {issue['message']}")

        # ── box bottom ────────────────────────────────────────────────────────
        print("│")
        print("└" + "─" * (width - 1))
        print()

    if gitignore:
        print("  Suggested .gitignore patterns:")
        for pattern in gitignore:
            print(f"    {_ansi(pattern, _ANSI_DIM)}")
        print()


# ── Textual TUI ────────────────────────────────────────────────────────────────

def _build_commit_markup(commit: dict, hunk_map: dict | None, index: int, total: int) -> str:
    """Build a Rich markup string for the commit detail panel."""
    from rich.markup import escape as esc

    subject = esc(commit["subject"])
    lines = [
        f"[dim bold][{index}/{total}][/dim bold]  [bold]{subject}[/bold]",
        "",
    ]

    body = commit.get("body", "").strip()
    if body:
        for line in body.splitlines():
            lines.append(f"  {esc(line)}")
        lines.append("")

    hunk_ids = commit.get("hunks", [])
    if hunk_ids:
        by_file: dict[str, list] = {}
        for hid in hunk_ids:
            if hunk_map and hid in hunk_map:
                dh = hunk_map[hid]
                by_file.setdefault(dh.file_path, []).append(dh)
            else:
                fpath = hid.rsplit("#", 1)[0] if "#" in hid else hid
                by_file.setdefault(fpath, [])

        max_path = max((len(fp) for fp in by_file), default=0)
        for fpath, hunks in by_file.items():
            total_added = total_removed = 0
            is_new = any(getattr(dh, "is_untracked", False) for dh in hunks)
            for dh in hunks:
                if dh is not None and dh.patch:
                    a, r = _diff_stats(dh.patch)
                    total_added += a
                    total_removed += r

            padding  = " " * (max_path - len(fpath))
            stats    = f"+{total_added} -{total_removed}"
            fp_esc   = esc(fpath)

            if not hunks:
                lines.append(f"  [yellow]~ {fp_esc}[/yellow]")
            elif is_new:
                lines.append(f"  [green]+ {fp_esc}[/green]{padding}  [dim]{stats}[/dim]")
            elif total_added == 0 and total_removed > 0:
                lines.append(f"  [red]× {fp_esc}[/red]{padding}  [dim]{stats}[/dim]")
            else:
                lines.append(f"  [yellow]~ {fp_esc}[/yellow]{padding}  [dim]{stats}[/dim]")
        lines.append("")

    breaking = commit.get("breaking_change", "").strip()
    if breaking:
        lines.append(f"  [bold red]⚠ BREAKING CHANGE:[/bold red] {esc(breaking)}")
        lines.append("")

    issues = commit.get("issues") or []
    if issues:
        lines.append(f"  [yellow]Issues found:[/yellow]")
        for issue in issues:
            path = esc(issue.get("path", ""))
            msg  = esc(issue.get("message", ""))
            lines.append(f"    [dim]·[/dim] [dim]{path}:[/dim] {msg}")
        lines.append("")

    return "\n".join(lines)


def _run_confirm_tui(
    commits: list[dict], gitignore: list[str], hunk_map: dict | None
) -> list[dict] | None:
    """Launch the interactive Textual TUI.

    Returns the (possibly edited) commit list if the user confirmed, or
    None if the user cancelled.
    """
    # Textual imports are lazy: no startup cost when TUI is not used.
    from textual.app import App, ComposeResult
    from textual.binding import Binding
    from textual.containers import Horizontal, Vertical
    from textual.screen import ModalScreen
    from textual.widgets import Footer, Header, Input, Label, ListItem, ListView, Static

    class _EditModal(ModalScreen[str | None]):
        """Modal dialog for editing a commit's subject line."""

        # Use ModalScreen selector so the CSS works regardless of class name
        # mangling that can occur with functions-scoped class definitions.
        DEFAULT_CSS = """
        _EditModal { align: center middle; }
        _EditModal > Vertical {
            width: 70; height: auto;
            border: double $accent; padding: 1 2; background: $surface;
        }
        """

        BINDINGS = [Binding("escape", "dismiss_none", "Cancel", priority=True)]

        def __init__(self, current: str) -> None:
            super().__init__()
            self._current = current

        def compose(self) -> ComposeResult:
            with Vertical():
                yield Label("Edit commit subject:")
                yield Input(value=self._current, id="subj-input")
                yield Label("[dim]Enter to save · Esc to cancel[/dim]", markup=True)

        def on_mount(self) -> None:
            self.query_one(Input).focus()

        def on_input_submitted(self, event: Input.Submitted) -> None:
            self.dismiss(event.value.strip() or None)

        def action_dismiss_none(self) -> None:
            self.dismiss(None)

    class _App(App[list[dict] | None]):
        """Two-panel TUI: commit list on the left, details on the right."""

        TITLE = "git-smart-commit — review proposed commits"

        CSS = """
        Screen   { layout: vertical; }
        #panels  { layout: horizontal; height: 1fr; }
        #sidebar { width: 34; border-right: solid $primary-darken-3; }
        ListView { height: 1fr; }
        ListView > ListItem { padding: 0 1; }
        #detail  { width: 1fr; padding: 1 2; overflow-y: auto; }
        """

        BINDINGS = [
            Binding("y", "confirm",      "Proceed",      priority=True),
            Binding("n", "cancel",       "Cancel",       priority=True),
            Binding("q", "cancel",       "Quit",         priority=True),
            Binding("e", "edit_subject", "Edit subject"),
            Binding("k", "cursor_up",    "",             show=False),
            Binding("j", "cursor_down",  "",             show=False),
        ]

        def __init__(
            self,
            commits: list[dict],
            gitignore_patterns: list[str],
            hunk_map: dict | None,
        ) -> None:
            super().__init__()
            self._commits   = [dict(c) for c in commits]
            self._gitignore = gitignore_patterns
            self._hunk_map  = hunk_map
            self._sel       = 0          # plain int — no reactive watcher needed

        # ── layout ───────────────────────────────────────────────────────────

        def compose(self) -> ComposeResult:
            yield Header(show_clock=False)
            with Horizontal(id="panels"):
                with Vertical(id="sidebar"):
                    with ListView(id="list"):
                        for i in range(len(self._commits)):
                            yield ListItem(Label(self._label(i)), id=f"ci-{i}")
                # Initialise detail with the first commit so no on_mount needed.
                init = (
                    _build_commit_markup(
                        self._commits[0], self._hunk_map, 1, len(self._commits)
                    )
                    if self._commits else ""
                )
                yield Static(init, id="detail", markup=True)
            yield Footer()

        # ── helpers ──────────────────────────────────────────────────────────

        def _label(self, i: int) -> str:
            subj = self._commits[i]["subject"]
            if len(subj) > 27:
                subj = subj[:26] + "…"
            return f"{'▶' if i == self._sel else ' '} {i + 1}. {subj}"

        def _set_sel(self, idx: int) -> None:
            """Change selection: refresh both sidebar labels and the detail pane."""
            old, self._sel = self._sel, idx
            for i in (old, idx):
                try:
                    self.query_one(f"#ci-{i}", ListItem).query_one(Label).update(
                        self._label(i)
                    )
                except Exception:
                    pass
            try:
                self.query_one("#detail", Static).update(
                    _build_commit_markup(
                        self._commits[idx], self._hunk_map, idx + 1, len(self._commits)
                    )
                )
            except Exception:
                pass

        # ── events ───────────────────────────────────────────────────────────

        def on_list_view_highlighted(self, event: ListView.Highlighted) -> None:
            if event.item is None:
                return
            try:
                idx = int(event.item.id.split("-", 1)[1])
            except Exception:
                return
            if idx != self._sel:
                self._set_sel(idx)

        # ── actions ──────────────────────────────────────────────────────────

        def action_cursor_up(self) -> None:
            try:
                self.query_one("#list", ListView).action_cursor_up()
            except Exception:
                pass

        def action_cursor_down(self) -> None:
            try:
                self.query_one("#list", ListView).action_cursor_down()
            except Exception:
                pass

        def action_confirm(self) -> None:
            self.exit(self._commits)

        def action_cancel(self) -> None:
            self.exit(None)

        def action_edit_subject(self) -> None:
            idx = self._sel

            def _apply(new_subject: str | None) -> None:
                if not new_subject:
                    return
                self._commits[idx]["subject"] = new_subject
                try:
                    self.query_one(f"#ci-{idx}", ListItem).query_one(Label).update(
                        self._label(idx)
                    )
                    self.query_one("#detail", Static).update(
                        _build_commit_markup(
                            self._commits[idx], self._hunk_map,
                            idx + 1, len(self._commits),
                        )
                    )
                except Exception:
                    pass

            self.push_screen(_EditModal(self._commits[idx]["subject"]), _apply)

    return _App(commits, gitignore, hunk_map).run()


# ── Main ───────────────────────────────────────────────────────────────────────

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Group git changes into logical commits using Qwen3-Coder.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--repo", default=".", help="Path to git repository")
    parser.add_argument("--dry-run", action="store_true", help="Show proposed commits without executing")
    parser.add_argument("--model", default=None, help=f"Model name (default: {DEFAULT_MODEL}, or smart-commit.model in git config)")
    parser.add_argument("--yes", "-y", action="store_true", help="Commit without confirmation")
    parser.add_argument("--quiet", "-q", action="store_true", help="Suppress progress output (for scripting/CI)")
    parser.add_argument("--json", dest="json_out", action="store_true", help="Output JSON and exit")
    parser.add_argument(
        "--api-base", default=None,
        help="API base URL (default: http://localhost:11434 for Ollama). "
             "Use https://openrouter.ai/api/v1 for OpenRouter. "
             "Or set LLM_API_BASE env var.",
    )
    parser.add_argument(
        "--api-key", default=None,
        help="API key for authenticated endpoints (or set LLM_API_KEY env var)",
    )
    parser.add_argument(
        "--critique", default="",
        metavar="TEXT",
        help="Agent feedback on a previous proposal (triggers revised classification)",
    )
    parser.add_argument(
        "--plan", metavar="FILE",
        help="Execute a previously saved commit plan (JSON from --json / --save-plan). "
             "Skips classification entirely — the exact plan is replayed.",
    )
    parser.add_argument(
        "--save-plan", metavar="FILE",
        help="When used with --json, also write the plan to FILE for later use with --plan.",
    )
    parser.add_argument(
        "--setup", action="store_true",
        help="Run the interactive configuration wizard and exit.",
    )

    args = parser.parse_args()
    global _QUIET
    _QUIET = args.quiet or args.json_out  # suppress progress when outputting JSON
    repo = Path(args.repo).resolve()

    # Setup wizard: run before git-repo validation so --setup --global works
    # from any directory (not just inside a git repo).
    if args.setup:
        # Try to resolve a repo for "local" scope option; silently ignore if
        # the current directory is not a git repo.
        setup_repo: Path | None = None
        check = subprocess.run(
            ["git", "rev-parse", "--git-dir"],
            cwd=repo,
            capture_output=True,
            text=True,
        )
        if check.returncode == 0:
            setup_repo = repo
        run_setup_wizard(setup_repo)
        return

    # Verify it's a git repo early so we can read git config
    try:
        _git(["rev-parse", "--git-dir"], repo)
    except RuntimeError:
        print(f"Error: {repo} is not a git repository.", file=sys.stderr)
        sys.exit(3)

    # Configure API backend.
    # Priority: CLI arg > env var > git config (smart-commit.*) > built-in default.
    gc = read_git_config(repo)
    api_config = ApiConfig(
        base_url=(
            args.api_base
            or os.environ.get("LLM_API_BASE")
            or gc.get("smart-commit.api-base")
            or OLLAMA_BASE_URL
        ).rstrip("/"),
        model=(
            args.model
            or os.environ.get("LLM_MODEL")
            or gc.get("smart-commit.model")
            or DEFAULT_MODEL
        ),
        api_key=(
            args.api_key
            or os.environ.get("LLM_API_KEY")
            or gc.get("smart-commit.api-key")
        ),
    )
    client = LLMClient(api_config)
    analyzer = GitAnalyzer(repo, client)

    # Collect hunks (unstages staged files first so everything is uniform)
    _log("Scanning for changes…")
    hunk_map = analyzer.collect_hunks()
    if not hunk_map:
        print("No changes found.", file=sys.stderr)
        sys.exit(1)

    # Plan replay mode: load a previously saved plan, skip classification
    if args.plan:
        try:
            with open(args.plan) as f:
                plan = json.load(f)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Error: could not load plan from {args.plan}: {e}", file=sys.stderr)
            sys.exit(1)
        commits = plan["commits"]
        gitignore = plan.get("gitignore", [])
        # Restore saved patches into hunk_map if present (makes plan self-contained)
        for hunk_id, patch in plan.get("hunk_patches", {}).items():
            if hunk_id not in hunk_map:
                # Best-effort: reconstruct a DiffHunk from the saved patch
                fpath = hunk_id.rsplit("#", 1)[0] if "#" in hunk_id else hunk_id
                hunk_map[hunk_id] = DiffHunk(
                    hunk_id=hunk_id, file_path=fpath, patch=patch
                )
        if args.dry_run:
            print_proposed_commits(commits, gitignore, hunk_map)
            return
        if args.yes:
            print_proposed_commits(commits, gitignore, hunk_map)
        elif sys.stdout.isatty():
            result = _run_confirm_tui(commits, gitignore, hunk_map)
            if result is None:
                print("Cancelled.")
                return
            commits = result
        else:
            print_proposed_commits(commits, gitignore, hunk_map)
            try:
                answer = input("Proceed with these commits? [y/N] ").strip().lower()
            except (KeyboardInterrupt, EOFError):
                return
            if answer not in ("y", "yes"):
                print("Cancelled.")
                return
        analyzer.execute_commits(commits, hunk_map)
        analyzer.update_gitignore(gitignore, yes=args.yes)
        print("\nDone.")
        return

    n_files = len({dh.file_path for dh in hunk_map.values()})
    n_hunks = len(hunk_map)
    _log(f"Found {n_hunks} hunk(s) across {n_files} file(s). Analyzing…")

    commits, gitignore = analyzer.classify_changes(hunk_map, critique=args.critique)

    usage = analyzer.client.usage
    if usage.total_tokens > 0:
        _log(f"  {_ansi(str(usage), _ANSI_DIM)}")

    # JSON mode: dump plan (including patches for self-contained replay) and exit
    if args.json_out:
        hunk_patches = {hid: dh.patch for hid, dh in hunk_map.items() if not dh.is_untracked}
        plan_json = json.dumps(
            {"commits": commits, "hunk_patches": hunk_patches, "gitignore": gitignore},
            indent=2,
        )
        print(plan_json)
        if args.save_plan:
            with open(args.save_plan, "w") as f:
                f.write(plan_json)
        return

    if args.dry_run:
        print_proposed_commits(commits, gitignore, hunk_map)
        return

    # Interactive TUI when stdout is a terminal; plain-text fallback otherwise.
    if args.yes:
        print_proposed_commits(commits, gitignore, hunk_map)
    elif sys.stdout.isatty():
        result = _run_confirm_tui(commits, gitignore, hunk_map)
        if result is None:
            print("Cancelled.")
            return
        commits = result
    else:
        print_proposed_commits(commits, gitignore, hunk_map)
        try:
            answer = input("Proceed with these commits? [y/N] ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            return
        if answer not in ("y", "yes"):
            print("Cancelled.")
            return

    analyzer.execute_commits(commits, hunk_map)
    analyzer.update_gitignore(gitignore, yes=args.yes)
    print("\nDone.")


if __name__ == "__main__":
    main()
