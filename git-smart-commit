#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pydantic",
# ]
# ///
"""
git-smart-commit: Analyze unstaged changes, group into logical commits, execute them.

Uses an LLM for classification. Supports Ollama (default) and any OpenAI-compatible
API (OpenRouter, Anthropic, Together, etc.) via --api-base.

Usage:
    git-smart-commit [--repo PATH] [--dry-run] [--model MODEL] [--yes]
                     [--api-base URL] [--api-key KEY]

Options:
    --repo PATH     Path to git repository (default: current directory)
    --dry-run       Print proposed commits without executing
    --model MODEL   Model name (default: qwen3-coder:30b-a3b-q8_0)
    --yes           Skip confirmation prompt and commit immediately
    --json          Output proposed commits as JSON and exit (implies --dry-run)
    --api-base URL  API base URL (default: http://localhost:11434 for Ollama)
                    Use https://openrouter.ai/api/v1 for OpenRouter, etc.
    --api-key KEY   API key for authenticated endpoints (or set LLM_API_KEY env var)
    --help          Show this message

Exit codes:
    0   Success
    1   No changes found
    2   Model/Ollama error
    3   Git error
    4   User cancelled
"""

from pathlib import Path
from pydantic import BaseModel, Field, ValidationError
from typing import Any, Callable

import argparse
import ast
import httpx
import json
import os
import re
import subprocess
import sys
import textwrap
from dataclasses import dataclass, field

# ── Configuration ──────────────────────────────────────────────────────────────

SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    Given a list of changed files and their diffs, your job is to group them into
    one or more commits. Each commit should represent a single logical change
    (e.g. "add feature X", "fix bug in Y", "update dependencies",
    "refactor Z").

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: You can only commit WHOLE FILES. Partial staging is NOT supported. If a single file contains multiple unrelated changes, you MUST group them together into one single commit. Do not split a file across multiple commits.
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Keep related changes together (same feature, same module, same concern).
    - Separate unrelated concerns into different commits, provided they do not violate the whole-file rule above.
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).

    You have access to a read_file tool. Use it when the diff alone is not
    enough context — for example, to see which class a changed function belongs
    to, what imports already exist, or how unchanged surrounding code looks.
    Call read_file as many times as needed, then call propose_commits when ready.
""")


AGENTIC_SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    You have investigation tools to gather information before proposing commits:
    - read_file(path): Read the full current content of any file in the repository
    - get_diff(path, staged): Get the raw diff for a specific file on demand
      (staged=true for staged only, staged=false for unstaged only, omit for both)
    - get_git_log(n): Get the last n commit subjects to learn this project's conventions
    - search_diff(pattern, context_lines): Search all diffs for a regex pattern

    Investigation strategy:
    - Start with the file list and stats you receive
    - Use get_git_log early to learn this project's commit style conventions
    - Use get_diff on files whose grouping is unclear from stats alone
    - Use read_file when you need full file context (imports, class structure, etc.)
    - Use search_diff to find patterns across all changes (debug prints, TODOs, etc.)
    - Investigate as many files as needed, then call propose_commits when confident

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: You can only commit WHOLE FILES. Partial staging is NOT supported. If a single file contains multiple unrelated changes, you MUST group them together into one single commit. Do not split a file across multiple commits.
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Keep related changes together (same feature, same module, same concern).
    - Separate unrelated concerns into different commits, provided they do not violate the whole-file rule above.
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).

    When you have investigated enough to be confident in your groupings, call the
    propose_commits tool with your answer.
""")


OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "qwen3-coder:30b-a3b-q8_0"

DIFF_CHARS_PER_FILE = 6000    # files larger than this get hunk-summarized
DIFF_TOTAL_CHARS   = 120000   # hard cap on total context sent to classifier
HUNK_SUMMARIZE_CTX = 8192     # smaller context for summarization calls (faster)
PRIOR_HUNK_WINDOW  = 4        # how many previous hunk summaries to include as context

MAX_AGENTIC_TURNS  = 20       # safeguard for the agentic investigation loop
READ_FILE_LIMIT    = 50_000   # max chars returned by the read_file tool


@dataclass
class ApiConfig:
    """API connection and model configuration."""
    base_url: str = OLLAMA_BASE_URL
    model: str = DEFAULT_MODEL
    api_key: str | None = None
    num_ctx: int = 128000

    @property
    def is_ollama(self) -> bool:
        """True when targeting a native Ollama endpoint (not OpenAI-compatible)."""
        # OpenAI-compatible APIs use /v1 in the path; Ollama doesn't.
        return "/v1" not in self.base_url

    @property
    def chat_url(self) -> str:
        if self.is_ollama:
            return f"{self.base_url}/api/chat"
        return f"{self.base_url}/chat/completions"

    @property
    def auth_headers(self) -> dict[str, str]:
        if self.api_key:
            return {"Authorization": f"Bearer {self.api_key}"}
        return {}


@dataclass
class ToolCall:
    """Represents a tool call extracted from an LLM response."""
    name: str
    arguments: dict
    call_id: str | None = None


# ── Tool decorator ─────────────────────────────────────────────────────────────
#
# Defining a new tool:
#   1. Define argument types as pydantic BaseModel subclasses
#   2. Define a top-level args model (also a BaseModel) for the tool
#   3. Decorate a function with @tool(ArgsModel)
#
# Schema generation, validation, and parsing are all handled automatically.

def tool(args_model: type[BaseModel]):
    """Decorator factory. @tool(MyArgsModel) attaches LLM tool metadata to a function."""
    def decorator(fn: Callable) -> Callable:
        spec = {
            "type": "function",
            "function": {
                "name": fn.__name__,
                "description": (fn.__doc__ or "").strip(),
                "parameters": args_model.model_json_schema(),
            },
        }
        fn._tool_spec = spec
        fn._tool_model = args_model
        return fn
    return decorator


def _summarize_args(arguments: dict, max_len: int = 80) -> str:
    """Return a compact one-line summary of tool call arguments for logging."""
    raw = json.dumps(arguments, separators=(",", ":"))
    if len(raw) <= max_len:
        return raw
    return raw[:max_len - 3] + "..."


# ── LLM client ────────────────────────────────────────────────────────────────

class LLMClient:
    """Encapsulates all communication with the LLM API (Ollama or OpenAI-compatible)."""

    def __init__(self, config: ApiConfig):
        self.config = config

    def _build_payload(self, prompt: str, tools: list[dict] | None = None,
                       stream: bool = False) -> dict:
        """Build an API request payload, adapting to Ollama or OpenAI format."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
        else:
            payload = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
                # Force the model to call the specific tool
                payload["tool_choice"] = {
                    "type": "function",
                    "function": {"name": tools[0]["function"]["name"]},
                }
        return payload

    def _extract_tool_args(self, data: dict) -> dict | None:
        """Extract tool-call arguments from a response, handling both API formats."""
        if self.config.is_ollama:
            tool_calls = data.get("message", {}).get("tool_calls", [])
            if tool_calls:
                raw = tool_calls[0]["function"]["arguments"]
                return json.loads(raw) if isinstance(raw, str) else raw
            # Fallback: content as JSON
            content = data.get("message", {}).get("content", "").strip()
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            msg = choices[0].get("message", {})
            tool_calls = msg.get("tool_calls", [])
            if tool_calls:
                raw = tool_calls[0]["function"]["arguments"]
                return json.loads(raw) if isinstance(raw, str) else raw
            content = msg.get("content", "").strip()

        # Fallback: parse content as JSON (both backends)
        if content:
            content = content.removeprefix("```json").removeprefix("```").strip().removesuffix("```").strip()
            return json.loads(content)
        return None

    def _build_payload_messages(self, messages: list[dict],
                                tools: list[dict] | None = None,
                                tool_choice: str | dict | None = None,
                                stream: bool = False) -> dict:
        """Build a payload from a full conversation history (messages list)."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
            # Ollama does not support tool_choice — omit it
        else:
            payload = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
            if tool_choice is not None:
                payload["tool_choice"] = tool_choice
        return payload

    def _extract_tool_call(self, data: dict) -> ToolCall | None:
        """Extract a tool call from an API response, handling both Ollama and OpenAI formats."""
        if self.config.is_ollama:
            tool_calls = data.get("message", {}).get("tool_calls", [])
            if tool_calls:
                tc = tool_calls[0]
                raw = tc["function"]["arguments"]
                args = json.loads(raw) if isinstance(raw, str) else raw
                return ToolCall(
                    name=tc["function"]["name"],
                    arguments=args,
                    call_id=tc.get("id"),
                )
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            msg = choices[0].get("message", {})
            tool_calls = msg.get("tool_calls", [])
            if tool_calls:
                tc = tool_calls[0]
                raw = tc["function"]["arguments"]
                args = json.loads(raw) if isinstance(raw, str) else raw
                return ToolCall(
                    name=tc["function"]["name"],
                    arguments=args,
                    call_id=tc.get("id"),
                )
        return None

    def _extract_text_content(self, data: dict) -> str | None:
        """Extract plain text content from a response when no tool call is present."""
        if self.config.is_ollama:
            content = data.get("message", {}).get("content", "").strip()
        else:
            choices = data.get("choices", [])
            if not choices:
                return None
            content = choices[0].get("message", {}).get("content", "").strip()
        return content or None

    def _format_assistant_tool_call(self, data: dict) -> dict:
        """Extract the assistant message from a raw API response for conversation history.

        Using the raw response message guarantees the format Ollama/OpenAI expects to see back.
        """
        if self.config.is_ollama:
            return data.get("message", {"role": "assistant", "content": ""})
        return data.get("choices", [{}])[0].get("message", {"role": "assistant", "content": ""})

    def _format_tool_result(self, call_id: str | None, content: str) -> dict:
        """Build a tool-result message for appending to conversation history."""
        if call_id is not None and not self.config.is_ollama:
            return {"role": "tool", "tool_call_id": call_id, "content": content}
        return {"role": "tool", "content": content}

    def call(self, prompt: str, num_ctx: int | None = None) -> str:
        """Call the LLM with streaming and return the assistant message text."""
        payload = self._build_payload(prompt, stream=True)
        if num_ctx is not None and self.config.is_ollama:
            payload["options"]["num_ctx"] = num_ctx
        headers = {**self.config.auth_headers, "Content-Type": "application/json"}
        with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=60.0, write=30.0, pool=5.0)) as client:
            with client.stream("POST", self.config.chat_url, json=payload, headers=headers) as response:
                response.raise_for_status()
                chunks = []
                for line in response.iter_lines():
                    if not line:
                        continue
                    # OpenAI SSE format: lines prefixed with "data: "
                    if line.startswith("data: "):
                        line = line[6:]
                    if line.strip() == "[DONE]":
                        break
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    if self.config.is_ollama:
                        content = data.get("message", {}).get("content", "")
                        done = data.get("done", False)
                    else:
                        delta = (data.get("choices", [{}])[0]
                                 .get("delta", {}))
                        content = delta.get("content", "")
                        done = data.get("choices", [{}])[0].get("finish_reason") is not None
                    if content:
                        chunks.append(content)
                    if done:
                        break
                return "".join(chunks)

    def call_with_tool(self, prompt: str, tool_fn: Callable, **tool_kwargs) -> Any:
        """Call the LLM forcing tool_fn to be called.

        Parses and validates the response into the tool's args model, then calls
        tool_fn(validated_args, **tool_kwargs) and returns its result.
        """
        spec = tool_fn._tool_spec
        args_model: type[BaseModel] = tool_fn._tool_model
        actual_prompt = prompt
        retries = 0
        failures = []
        while True:
            retries += 1
            payload = self._build_payload(actual_prompt, tools=[spec], stream=False)
            headers = {**self.config.auth_headers, "Content-Type": "application/json"}
            with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
                response = client.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()

            raw_args = self._extract_tool_args(data)
            if raw_args is None:
                raw_args = {}

            # Fix stringified nested values: some models return e.g.
            # {"commits": "[{'subject': ...}]"} instead of a proper nested structure.
            # Detect strings that look like Python lists/dicts and try to parse them.
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass  # leave as-is, let pydantic report the error

            try:
                validated = args_model.model_validate(raw_args).post_process()
                return tool_fn(validated, **tool_kwargs)
            except ValidationError as e:
                failures.append(str(e))
                if retries > 5:
                    raise

                failure_summary = "\n".join(set(failures))
                actual_prompt = textwrap.dedent(f"""
                Your previous ({len(failures)}) attempts failed with a validation error:
                {failure_summary}

                Try again following this prompt exactly:
                {prompt}
                """)

    def agentic_loop(self, system_prompt: str, initial_user_message: str,
                     tool_registry: dict[str, Callable],
                     terminal_tool: Callable, **kwargs) -> Any:
        """Run a multi-turn agentic loop until the terminal tool is called.

        Each turn: send messages → extract tool call → dispatch.
        Investigation tool results are appended to history.
        Validation errors for the terminal tool become tool-result messages.
        On the last turn, only the terminal tool is offered.
        """
        messages: list[dict] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": initial_user_message},
        ]

        terminal_spec = terminal_tool._tool_spec
        terminal_name: str = terminal_spec["function"]["name"]
        terminal_model: type[BaseModel] = terminal_tool._tool_model
        all_tool_specs = [fn._tool_spec for fn in list(tool_registry.values())] + [terminal_spec]

        headers = {**self.config.auth_headers, "Content-Type": "application/json"}

        for turn in range(1, MAX_AGENTIC_TURNS + 1):
            is_last_turn = (turn == MAX_AGENTIC_TURNS)

            if is_last_turn:
                tools = [terminal_spec]
                tool_choice: str | dict | None = (
                    None if self.config.is_ollama
                    else {"type": "function", "function": {"name": terminal_name}}
                )
            else:
                tools = all_tool_specs
                tool_choice = None if self.config.is_ollama else "auto"

            payload = self._build_payload_messages(messages, tools=tools,
                                                   tool_choice=tool_choice, stream=False)

            with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
                response = client.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()

            tool_call = self._extract_tool_call(data)

            if tool_call is None:
                text = self._extract_text_content(data) or ""
                print(f"  [agentic turn {turn}/{MAX_AGENTIC_TURNS}] no tool call, nudging",
                      file=sys.stderr)
                messages.append({"role": "assistant", "content": text})
                messages.append({
                    "role": "user",
                    "content": (
                        f"Please call a tool. Use the investigation tools to gather more "
                        f"information, or call {terminal_name} if you are ready to propose commits."
                    ),
                })
                continue

            print(f"  [agentic turn {turn}/{MAX_AGENTIC_TURNS}] "
                  f"{tool_call.name}({_summarize_args(tool_call.arguments)})",
                  file=sys.stderr)
            messages.append(self._format_assistant_tool_call(data))

            # Coerce stringified nested values (some models return JSON-as-string)
            raw_args = dict(tool_call.arguments)
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass

            if tool_call.name == terminal_name:
                try:
                    validated = terminal_model.model_validate(raw_args).post_process()
                    return terminal_tool(validated, **kwargs)
                except ValidationError as e:
                    messages.append(self._format_tool_result(
                        tool_call.call_id,
                        f"Validation error: {e}\nPlease fix the arguments and call "
                        f"{terminal_name} again.",
                    ))
                    continue

            tool_fn = tool_registry.get(tool_call.name)
            if tool_fn is None:
                result_str = (f"Unknown tool: {tool_call.name!r}. "
                              f"Available: {list(tool_registry.keys())}")
            else:
                try:
                    validated_args = tool_fn._tool_model.model_validate(raw_args).post_process()
                    result = tool_fn(validated_args, **kwargs)
                    result_str = result if isinstance(result, str) else json.dumps(result)
                except Exception as e:
                    result_str = f"Error calling {tool_call.name}: {e}"

            messages.append(self._format_tool_result(tool_call.call_id, result_str))

        raise RuntimeError(
            f"agentic_loop exceeded {MAX_AGENTIC_TURNS} turns without calling {terminal_name}"
        )



# ── Git helpers ────────────────────────────────────────────────────────────────

def _git(args: list[str], cwd: Path) -> str:
    """Run a git command and return stdout. Raises on non-zero exit."""
    result = subprocess.run(
        ["git"] + args,
        cwd=cwd,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed:\n{result.stderr.strip()}")
    return result.stdout


def read_git_config(repo: Path) -> dict[str, str]:
    """Read smart-commit.* keys from git config (local → global → system).

    Returns a dict of whichever keys are set; missing keys are absent.
    Keys: smart-commit.model, smart-commit.api-base, smart-commit.api-key
    """
    keys = ["smart-commit.model", "smart-commit.api-base", "smart-commit.api-key"]
    config: dict[str, str] = {}
    for key in keys:
        result = subprocess.run(
            ["git", "config", "--get", key],
            cwd=repo,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0 and result.stdout.strip():
            config[key] = result.stdout.strip()
    return config


def split_hunks(diff_text: str) -> list[str]:
    """Split a unified diff into individual hunks (each starting with @@)."""
    lines = diff_text.splitlines(keepends=True)
    hunks: list[str] = []
    header_lines: list[str] = []
    current: list[str] = []

    for line in lines:
        if line.startswith("@@"):
            if current:
                hunks.append("".join(current))
            current = header_lines + [line]
        elif line.startswith(("diff --git", "index ", "--- ", "+++ ")):
            header_lines.append(line)
            if current:
                # flush any open hunk first
                hunks.append("".join(current))
                current = []
        else:
            if current:
                current.append(line)
            # lines before the first @@ go into header_lines
            elif not line.startswith("@@"):
                header_lines.append(line)

    if current:
        hunks.append("".join(current))

    return hunks or [diff_text]   # fallback: treat whole diff as one hunk


# ── Tool definitions ───────────────────────────────────────────────────────────

class Issue(BaseModel):
    message: str = Field(description="REQUIRED. A brief description of a code issue, including a line number")
    path: str = Field(description="REQUIRED. The path affected by the issue")

class Commit(BaseModel):
    subject: str = Field(description="REQUIRED. Conventional commit subject line (type(scope): desc), under 72 chars")
    files: list[str] = Field(description="REQUIRED. Repository-relative paths to include in this commit")
    body: str = Field(default="", description="4-10 line plain-text commit body, no markdown")
    issues: list[Issue] = Field(default_factory=list, description="Issues noticed in the code being committed")

    def post_process(self):
        self.body = textwrap.fill(self.body, 80)
        return self

class ProposeCommitsArgs(BaseModel):
    commits: list[Commit] = Field(description="REQUIRED. Logical commit groups")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commits = [commit.post_process() for commit in self.commits]
        return self

sample_issue = Issue(
    message="Unused import 'os' on line 1",
    path="main.py"
)

sample_commit = Commit(
    subject="refactor(main): cleanup imports",
    files=["main.py"],
    body="Removed unnecessary imports to improve load time and code cleanliness.",
    issues=[sample_issue]
)

SAMPLE_OUTPUT = ProposeCommitsArgs(
    commits=[sample_commit],
    gitignore=["*.pyc", "__pycache__/"]
)


@tool(ProposeCommitsArgs)
def propose_commits(result: ProposeCommitsArgs, analyzer: "GitAnalyzer") -> tuple[list[dict], list[str]]:
    """Group working-tree changes into logical commits and suggest .gitignore patterns for junk files."""
    commits = [c.model_dump() for c in result.commits]
    if not commits:
        raise ValueError("Model returned no valid commit groups.")

    commits = analyzer.merge_overlapping_commits(commits)
    return commits, result.gitignore


class MergeCommitsArgs(BaseModel):
    commit: Commit = Field(description="REQUIRED. A single merged commit covering all the provided changes")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commit = self.commit.post_process()
        return self


@tool(MergeCommitsArgs)
def merge_commits(result: MergeCommitsArgs, all_files: list[str], all_issues: list[dict]) -> dict:
    """Merge multiple overlapping commits into a single coherent commit."""
    commit = result.commit.model_dump()
    # Ensure all files and issues are preserved regardless of what the model returns
    commit["files"] = all_files
    commit["issues"] = all_issues
    return commit


class ReadFileArgs(BaseModel):
    path: str = Field(description="Repository-relative path to the file to read")

    def post_process(self): return self


@tool(ReadFileArgs)
def read_file(args: ReadFileArgs, analyzer: "GitAnalyzer") -> str:
    """Read the full current content of a file in the repository.

    Use this when the diff alone lacks context — e.g. to see the surrounding
    class structure, existing imports, or unchanged code near a changed line.
    The result is truncated at 50,000 characters for very large files.
    """
    return analyzer._read_file(args.path)


class GetDiffArgs(BaseModel):
    path: str = Field(description="Repository-relative path of the file")
    staged: bool | None = Field(
        default=None,
        description="true=staged only, false=unstaged only, null/omit=both",
    )

    def post_process(self): return self


@tool(GetDiffArgs)
def get_diff(result: GetDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Get the raw unified diff for a specific file (staged, unstaged, or both)."""
    parts = []

    def _fetch(extra_args: list[str], label: str) -> None:
        try:
            diff = analyzer.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", result.path])
            if diff.strip():
                parts.append(f"=== {label} ===\n{diff}" if result.staged is None else diff)
        except RuntimeError as e:
            parts.append(f"Error ({label}): {e}")

    if result.staged is None:
        _fetch(["--cached"], "staged")
        _fetch([], "unstaged")
    elif result.staged:
        _fetch(["--cached"], "staged")
    else:
        _fetch([], "unstaged")

    result_text = "\n".join(parts) if parts else f"No diff found for {result.path}"

    if len(result_text) > DIFF_CHARS_PER_FILE:
        result_text = result_text[:DIFF_CHARS_PER_FILE] + f"\n\n... (truncated at {DIFF_CHARS_PER_FILE} chars)"

    return result_text


class GetGitLogArgs(BaseModel):
    n: int = Field(default=20, description="Number of recent commits to return (max 50)")

    def post_process(self): return self


@tool(GetGitLogArgs)
def get_git_log(result: GetGitLogArgs, analyzer: "GitAnalyzer") -> str:
    """Get recent commit history to understand this project's commit style conventions."""
    n = min(result.n, 50)
    try:
        log = analyzer.git(["log", f"--max-count={n}", "--oneline", "--no-color"])
        return log.strip() or "No commits found."
    except RuntimeError as e:
        return f"Error: {e}"


class SearchDiffArgs(BaseModel):
    pattern: str = Field(description="Python regex pattern to search for across all diffs")
    context_lines: int = Field(default=2, description="Lines of context around each match")

    def post_process(self): return self


@tool(SearchDiffArgs)
def search_diff(result: SearchDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Search for a regex pattern across all staged and unstaged diffs."""
    try:
        compiled = re.compile(result.pattern)
    except re.error as e:
        return f"Invalid regex: {e}"

    matches: list[dict] = []
    MAX_MATCHES = 50

    for extra_args in (["--cached"], []):
        if len(matches) >= MAX_MATCHES:
            break
        try:
            changed = analyzer.git(
                ["diff"] + extra_args + ["--name-only", "--no-ext-diff"]
            ).splitlines()
            for fname in changed:
                if len(matches) >= MAX_MATCHES:
                    break
                diff = analyzer.git(
                    ["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname]
                )
                lines = diff.splitlines()
                for i, line in enumerate(lines):
                    if compiled.search(line):
                        start = max(0, i - result.context_lines)
                        end = min(len(lines), i + result.context_lines + 1)
                        matches.append({
                            "file": fname,
                            "line_number": i + 1,
                            "match_context": "\n".join(lines[start:end]),
                        })
                        if len(matches) >= MAX_MATCHES:
                            break
        except RuntimeError:
            pass

    if not matches:
        return f"No matches found for pattern: {result.pattern!r}"

    result_lines = [
        f"File: {m['file']} (line {m['line_number']})\n{m['match_context']}"
        for m in matches
    ]
    text = "\n---\n".join(result_lines)
    if len(matches) >= MAX_MATCHES:
        text += f"\n... (limited to {MAX_MATCHES} matches)"
    return text


MERGE_PROMPT = textwrap.dedent("""\
    You are merging multiple proposed commits that all touch overlapping files
    and therefore cannot be committed separately (partial staging is not supported).

    Combine them into a SINGLE commit that:
    - Has one conventional commit subject line (type(scope): desc), under 72 chars
    - Has a body (4-10 lines) summarizing ALL the changes coherently
    - Lists ALL affected files (deduped)
    - Collects ALL issues (deduped)
    - If any commit is a feat type, it must take priority over the others (although the others should be mentioned in the commit body)
    - chore should only be used if no other category applies.

    Do NOT just concatenate the subjects. Write a new, coherent subject and body
    that covers the full set of changes as a single logical unit.

    Call the merge_commits tool with your answer.

    Here are the commits to merge:
""")


# ── GitAnalyzer ────────────────────────────────────────────────────────────────

class GitAnalyzer:
    """Encapsulates git operations and commit analysis for a repository."""

    def __init__(self, repo: Path, client: LLMClient):
        self.repo = repo
        self.client = client

    @property
    def config(self) -> ApiConfig:
        return self.client.config

    def git(self, args: list[str]) -> str:
        return _git(args, self.repo)

    def get_changed_files(self) -> list[str]:
        """Return list of files with unstaged or staged changes."""
        staged = self.git(["diff", "--cached", "--name-only"]).splitlines()
        unstaged = self.git(["diff", "--name-only"]).splitlines()
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        seen = set()
        files = []
        for f in staged + unstaged + untracked:
            if f and f not in seen:
                seen.add(f)
                files.append(f)
        return files

    def build_file_metadata(self) -> str:
        """Return a compact stats-only summary: diff --stat for staged/unstaged + untracked list.

        No full diffs — the LLM requests specific diffs lazily via get_diff.
        """
        parts: list[str] = []
        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if stat:
                parts.append(f"{label}:\n{stat}")
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))
        return "\n\n".join(parts)

    def _summarize_file_diff(self, fname: str, diff_text: str) -> str:
        """Ask the model to summarize each hunk, passing the last N summaries as context."""
        hunks = split_hunks(diff_text)
        hunk_summaries: list[str] = []

        for i, hunk in enumerate(hunks, 1):
            prior_context = ""
            if hunk_summaries:
                window = hunk_summaries[-PRIOR_HUNK_WINDOW:]
                start_idx = i - len(window)
                prior_context = "Previous hunks in this file:\n" + "\n".join(
                    f"  Hunk {start_idx + j}: {s}" for j, s in enumerate(window)
                ) + "\n\n"

            prompt = textwrap.dedent(f"""\
                Summarize hunk {i} of {len(hunks)} from '{fname}' in 1-3 plain English sentences.
                Focus on WHAT changed. Preserve verbatim any code that looks buggy or suspicious
                (wrong arguments, misused APIs, incorrect syntax). Output only the summary.

                {prior_context}Hunk {i}:
                {hunk}
            """)

            summary = self.client.call(prompt, num_ctx=HUNK_SUMMARIZE_CTX)
            hunk_summaries.append(summary.strip())

        print()
        return f"[summarized — {len(hunks)} hunk(s)]\n" + "\n".join(
            f"  Hunk {i}: {s}" for i, s in enumerate(hunk_summaries, 1)
        )

    def build_diff_summary(self) -> str:
        """Return diff context for the classifier.

        Small files: include raw diff.
        Large files: summarize each hunk via a fast model call, then include summaries.
        """
        parts: list[str] = []
        total_chars = 0

        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if not stat:
                continue
            parts.append(f"{label}:\n{stat}\n")

            changed = self.git(["diff"] + extra_args + ["--name-only", "--no-ext-diff"]).splitlines()
            for fname in changed:
                file_diff = self.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname])

                if len(file_diff) <= DIFF_CHARS_PER_FILE:
                    content = file_diff
                else:
                    print(f"  Summarizing large diff: {fname} ({len(file_diff):,} chars)…",
                          file=sys.stderr)
                    content = self._summarize_file_diff(fname, file_diff)

                parts.append(content)
                total_chars += len(content)

                if total_chars >= DIFF_TOTAL_CHARS:
                    parts.append("\n... (remaining diffs omitted — total size limit reached)")
                    break
            else:
                continue
            break   # hit total limit inside inner loop

        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))

        return "\n".join(parts)

    def _read_file(self, path: str) -> str:
        """Read a repo file, rejecting path traversal and truncating large files."""
        target = (self.repo / path).resolve()
        if not str(target).startswith(str(self.repo) + os.sep) and target != self.repo:
            return f"[error: path traversal rejected for '{path}']"
        if not target.exists():
            return f"[file not found: {path}]"
        try:
            content = target.read_text(errors="replace")
        except OSError as e:
            return f"[error reading {path}: {e}]"
        if len(content) > READ_FILE_LIMIT:
            size = target.stat().st_size
            return (
                content[:READ_FILE_LIMIT]
                + f"\n\n[truncated — file is {size:,} bytes; showing first {READ_FILE_LIMIT:,} chars]"
            )
        return content

    def classify_changes(self, files: list[str], diff_summary: str = "", critique: str = "") -> tuple[list[dict], list[str]]:
        """Ask the model to group files into logical commits using an agentic investigation loop.

        Returns (commits, gitignore_patterns).
        diff_summary is accepted for backward compatibility but unused — the LLM
        fetches diffs lazily via the get_diff investigation tool.
        """
        file_list = "\n".join(f"  - {f}" for f in files)
        metadata = self.build_file_metadata()

        critique_section = ""
        if critique:
            critique_section = (
                f"\n\nA reviewer rated the previous proposal below 7/10. Their feedback:\n"
                f"{critique}\n"
                f"Revise your commit groupings to address this feedback."
            )

        initial_message = (
            f"Changed files:\n{file_list}\n\n"
            f"Stats:\n{metadata}"
            f"{critique_section}"
        )

        tool_registry = {
            "read_file": read_file,
            "get_diff": get_diff,
            "get_git_log": get_git_log,
            "search_diff": search_diff,
        }

        return self.client.agentic_loop(
            system_prompt=AGENTIC_SYSTEM_PROMPT,
            initial_user_message=initial_message,
            tool_registry=tool_registry,
            terminal_tool=propose_commits,
            analyzer=self,
        )

    def merge_overlapping_commits(self, commits: list[dict]) -> list[dict]:
        """Merge commits whose file sets overlap, since partial staging isn't supported.

        Uses union-find to detect groups, then asks the model to write a single
        coherent commit message for each merged group.
        """
        if len(commits) <= 1:
            return commits

        # Build a mapping from each file to the indices of commits that touch it
        file_to_indices: dict[str, list[int]] = {}
        for i, c in enumerate(commits):
            for f in c.get("files", []):
                file_to_indices.setdefault(f, []).append(i)

        # Union-find to group commits sharing files
        parent = list(range(len(commits)))

        def find(x):
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a, b):
            ra, rb = find(a), find(b)
            if ra != rb:
                parent[rb] = ra

        for indices in file_to_indices.values():
            for idx in indices[1:]:
                union(indices[0], idx)

        # Group commits by their root
        groups: dict[int, list[int]] = {}
        for i in range(len(commits)):
            groups.setdefault(find(i), []).append(i)

        # Check if any merging is actually needed
        if all(len(idxs) == 1 for idxs in groups.values()):
            return commits

        merged = []
        for indices in groups.values():
            if len(indices) == 1:
                merged.append(commits[indices[0]])
                continue

            group = [commits[i] for i in indices]
            print(f"  Merging {len(group)} overlapping commits…", file=sys.stderr)

            # Collect all files and issues for the merged commit
            all_files = list(dict.fromkeys(f for c in group for f in c.get("files", [])))
            all_issues = []
            seen_issues = set()
            for c in group:
                for issue in c.get("issues", []):
                    key = (issue.get("path", ""), issue.get("message", ""))
                    if key not in seen_issues:
                        seen_issues.add(key)
                        all_issues.append(issue)

            # Format the commits for the merge prompt
            commits_desc = json.dumps(group, indent=2)
            prompt = MERGE_PROMPT + commits_desc

            merged.append(self.client.call_with_tool(
                prompt, merge_commits,
                all_files=all_files, all_issues=all_issues,
            ))

        return merged

    def execute_commits(self, commits: list[dict]) -> None:
        """Stage and commit each group in order."""
        for i, commit in enumerate(commits, 1):
            subject = commit["subject"]
            body = commit.get("body", "").strip()
            message = f"{subject}\n\n{body}" if body else subject
            files = commit["files"]

            print(f"\n[{i}/{len(commits)}] {subject}")

            # Stage the files for this commit
            for f in files:
                try:
                    self.git(["add", "--", f])
                    print(f"  + {f}")
                except RuntimeError as e:
                    print(f"  ! Could not stage {f}: {e}", file=sys.stderr)

            # Check something is actually staged
            staged = self.git(["diff", "--cached", "--name-only"]).strip()
            if not staged:
                print("  (nothing staged, skipping)")
                continue

            try:
                self.git(["commit", "-m", message])
                print(f"  ✓ Committed")
            except RuntimeError as e:
                print(f"   Error committing: {e}", file=sys.stderr)
                raise

    def update_gitignore(self, patterns: list[str], yes: bool = False) -> None:
        """Offer to append suggested patterns to .gitignore, skipping any already present."""
        if not patterns:
            return

        gitignore_path = self.repo / ".gitignore"

        existing: set[str] = set()
        existing_content = ""
        if gitignore_path.exists():
            existing_content = gitignore_path.read_text()
            for line in existing_content.splitlines():
                stripped = line.strip()
                if stripped and not stripped.startswith("#"):
                    existing.add(stripped)

        new_patterns = [p for p in patterns if p not in existing]
        if not new_patterns:
            return

        print("\nSuggested .gitignore additions:")
        for p in new_patterns:
            print(f"  {p}")

        if not yes:
            try:
                answer = input("Add these to .gitignore? [y/N] ").strip().lower()
            except (KeyboardInterrupt, EOFError):
                return
            if answer not in ("y", "yes"):
                return

        with open(gitignore_path, "a") as f:
            if existing_content and not existing_content.endswith("\n"):
                f.write("\n")
            f.write("\n# Added by git-smart-commit\n")
            for p in new_patterns:
                f.write(f"{p}\n")

        print(f"  Updated .gitignore ({len(new_patterns)} new pattern(s)).")


# ── Display helpers ────────────────────────────────────────────────────────────

def print_proposed_commits(commits: list[dict], gitignore: list[str]) -> None:
    print("\nProposed commits:")
    print("─" * 60)
    for i, commit in enumerate(commits, 1):
        print(f"\n  [{i}] {commit['subject']}")
        body = commit.get("body", "").strip()
        print()
        if body:
            for line in body.splitlines():
                print(f"      {line}")
        for f in commit.get("files", []):
            print(f"       + {f}")
        if (issues := commit.get("issues")) != []:
            print("  Issues in this commit:")
            for issue in issues:
                print(f"    - {issue['path']}: {issue['message']}")
    if gitignore:
        print("\n  Suggested .gitignore patterns:")
        for pattern in gitignore:
            print(f"       {pattern}")
    print()


# ── Main ───────────────────────────────────────────────────────────────────────

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Group git changes into logical commits using Qwen3-Coder.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--repo", default=".", help="Path to git repository")
    parser.add_argument("--dry-run", action="store_true", help="Show proposed commits without executing")
    parser.add_argument("--model", default=None, help=f"Model name (default: {DEFAULT_MODEL}, or smart-commit.model in git config)")
    parser.add_argument("--yes", "-y", action="store_true", help="Commit without confirmation")
    parser.add_argument("--json", dest="json_out", action="store_true", help="Output JSON and exit")
    parser.add_argument(
        "--api-base", default=None,
        help="API base URL (default: http://localhost:11434 for Ollama). "
             "Use https://openrouter.ai/api/v1 for OpenRouter. "
             "Or set LLM_API_BASE env var.",
    )
    parser.add_argument(
        "--api-key", default=None,
        help="API key for authenticated endpoints (or set LLM_API_KEY env var)",
    )
    parser.add_argument(
        "--critique", default="",
        metavar="TEXT",
        help="Agent feedback on a previous proposal (triggers revised classification)",
    )
    parser.add_argument(
        "--plan", metavar="FILE",
        help="Execute a previously saved commit plan (JSON from --json / --save-plan). "
             "Skips classification entirely — the exact plan is replayed.",
    )
    parser.add_argument(
        "--save-plan", metavar="FILE",
        help="When used with --json, also write the plan to FILE for later use with --plan.",
    )

    args = parser.parse_args()
    repo = Path(args.repo).resolve()

    # Verify it's a git repo early so we can read git config
    try:
        _git(["rev-parse", "--git-dir"], repo)
    except RuntimeError:
        print(f"Error: {repo} is not a git repository.", file=sys.stderr)
        sys.exit(3)

    # Configure API backend.
    # Priority: CLI arg > env var > git config (smart-commit.*) > built-in default.
    gc = read_git_config(repo)
    api_config = ApiConfig(
        base_url=(
            args.api_base
            or os.environ.get("LLM_API_BASE")
            or gc.get("smart-commit.api-base")
            or OLLAMA_BASE_URL
        ).rstrip("/"),
        model=(
            args.model
            or os.environ.get("LLM_MODEL")
            or gc.get("smart-commit.model")
            or DEFAULT_MODEL
        ),
        api_key=(
            args.api_key
            or os.environ.get("LLM_API_KEY")
            or gc.get("smart-commit.api-key")
        ),
    )
    client = LLMClient(api_config)
    analyzer = GitAnalyzer(repo, client)

    # Plan replay mode: load a previously saved plan, skip classification
    if args.plan:
        try:
            with open(args.plan) as f:
                plan = json.load(f)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Error: could not load plan from {args.plan}: {e}", file=sys.stderr)
            sys.exit(1)
        commits = plan["commits"]
        gitignore = plan.get("gitignore", [])
        print_proposed_commits(commits, gitignore)
        if not args.dry_run:
            if not args.yes:
                try:
                    answer = input("Proceed with these commits? [y/N] ").strip().lower()
                except (KeyboardInterrupt, EOFError):
                    return
                if answer not in ("y", "yes"):
                    print("Cancelled.")
                    return
            analyzer.execute_commits(commits)
            analyzer.update_gitignore(gitignore, yes=args.yes)
            print("\nDone.")
        return

    # Collect changed files
    print("Scanning for changes...", file=sys.stderr)
    files = analyzer.get_changed_files()
    if not files:
        print("No changes found.", file=sys.stderr)
        sys.exit(1)

    print(f"Found {len(files)} changed file(s). Analyzing...", file=sys.stderr)

    commits, gitignore = analyzer.classify_changes(files, critique=args.critique)

    # JSON mode: dump and optionally save plan to file
    if args.json_out:
        plan_json = json.dumps({"commits": commits, "gitignore": gitignore}, indent=2)
        print(plan_json)
        if args.save_plan:
            with open(args.save_plan, "w") as f:
                f.write(plan_json)
        return

    # Display proposed commits
    print_proposed_commits(commits, gitignore)

    if args.dry_run:
        return

    # Confirm unless --yes
    if not args.yes:
        try:
            answer = input("Proceed with these commits? [y/N] ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            return
        if answer not in ("y", "yes"):
            print("Cancelled.")
            return

    analyzer.execute_commits(commits)
    analyzer.update_gitignore(gitignore, yes=args.yes)
    print("\nDone.")


if __name__ == "__main__":
    main()
