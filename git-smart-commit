#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pydantic",
# ]
# ///
"""
git-smart-commit: Analyze unstaged changes, group into logical commits, execute them.

Uses Qwen3-Coder-30B-A3B via Ollama for classification.

Usage:
    git-smart-commit [--repo PATH] [--dry-run] [--model MODEL] [--yes]

Options:
    --repo PATH     Path to git repository (default: current directory)
    --dry-run       Print proposed commits without executing
    --model MODEL   Ollama model to use (default: qwen2.5-coder:32b)
    --yes           Skip confirmation prompt and commit immediately
    --json          Output proposed commits as JSON and exit (implies --dry-run)
    --help          Show this message

Exit codes:
    0   Success
    1   No changes found
    2   Model/Ollama error
    3   Git error
    4   User cancelled
"""

import argparse
import json
import subprocess
import sys
import textwrap
from pathlib import Path
from typing import Any, Callable

import httpx
from pydantic import BaseModel, Field

# ── Configuration ──────────────────────────────────────────────────────────────

OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "qwen3-coder:30b-a3b-q8_0"
OLLAMA_TIMEOUT = 300.0
NUM_CTX = 128000

# ── Tool registry ──────────────────────────────────────────────────────────────
#
# Defining a new tool:
#   1. Define argument types as pydantic BaseModel subclasses
#   2. Define a top-level args model (also a BaseModel) for the tool
#   3. Decorate a function with @tool(ArgsModel)
#
# Schema generation, validation, and parsing are all handled automatically.

_TOOL_REGISTRY: dict[str, dict] = {}


def tool(args_model: type[BaseModel]):
    """Decorator factory. @tool(MyArgsModel) registers a function as an Ollama tool."""
    def decorator(fn: Callable) -> Callable:
        spec = {
            "type": "function",
            "function": {
                "name": fn.__name__,
                "description": (fn.__doc__ or "").strip(),
                "parameters": args_model.model_json_schema(),
            },
        }
        _TOOL_REGISTRY[fn.__name__] = {"spec": spec, "model": args_model, "fn": fn}
        fn._tool_spec = spec
        fn._tool_model = args_model
        return fn
    return decorator


def call_ollama_with_tool(
    model: str,
    prompt: str,
    tool_fn: Callable,
    num_ctx: int = NUM_CTX,
) -> BaseModel:
    """Call Ollama forcing tool_fn to be called. Returns a validated pydantic model instance."""
    spec = tool_fn._tool_spec
    args_model: type[BaseModel] = tool_fn._tool_model
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "tools": [spec],
        "stream": False,
        "keep_alive": "1h",
        "options": {
            "temperature": 0.1,
            "num_ctx": num_ctx,
        },
    }
    try:
        with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
            response = client.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload)
            response.raise_for_status()
            data = response.json()
    except httpx.ConnectError:
        print("Error: Cannot connect to Ollama. Is it running? (ollama serve)", file=sys.stderr)
        sys.exit(2)
    except httpx.ReadTimeout:
        print("Error: Ollama timed out. Try a smaller model or reduce diff size.", file=sys.stderr)
        sys.exit(2)
    except httpx.HTTPStatusError as e:
        print(f"Error: Ollama returned {e.response.status_code}: {e.response.text}", file=sys.stderr)
        sys.exit(2)

    tool_calls = data.get("message", {}).get("tool_calls", [])
    if tool_calls:
        raw_args = tool_calls[0]["function"]["arguments"]
        # Ollama may return arguments as a string or already-parsed dict
        if isinstance(raw_args, str):
            raw_args = json.loads(raw_args)
    else:
        # Fallback: parse message content as JSON
        content = data.get("message", {}).get("content", "").strip()
        content = content.lstrip("```json").lstrip("```").rstrip("```").strip()
        try:
            raw_args = json.loads(content)
        except json.JSONDecodeError:
            print(f"Error: Model did not call tool and returned invalid JSON.\nRaw: {content}", file=sys.stderr)
            sys.exit(2)

    try:
        return args_model.model_validate(raw_args)
    except Exception as e:
        print(f"Error: Tool arguments failed validation: {e}", file=sys.stderr)
        sys.exit(2)



# ── Git helpers ────────────────────────────────────────────────────────────────


def git(args: list[str], cwd: Path) -> str:
    """Run a git command and return stdout. Raises on non-zero exit."""
    result = subprocess.run(
        ["git"] + args,
        cwd=cwd,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed:\n{result.stderr.strip()}")
    return result.stdout


def get_changed_files(repo: Path) -> list[str]:
    """Return list of files with unstaged or staged changes."""
    # Both staged (cached) and unstaged, deduped
    staged = git(["diff", "--cached", "--name-only"], repo).splitlines()
    unstaged = git(["diff", "--name-only"], repo).splitlines()
    untracked = git(["ls-files", "--others", "--exclude-standard"], repo).splitlines()
    seen = set()
    files = []
    for f in staged + unstaged + untracked:
        if f and f not in seen:
            seen.add(f)
            files.append(f)
    return files


DIFF_CHARS_PER_FILE = 6000    # files larger than this get hunk-summarized
DIFF_TOTAL_CHARS   = 120000   # hard cap on total context sent to classifier
HUNK_SUMMARIZE_CTX = 8192     # smaller context for summarization calls (faster)


def split_hunks(diff_text: str) -> list[str]:
    """Split a unified diff into individual hunks (each starting with @@)."""
    lines = diff_text.splitlines(keepends=True)
    hunks: list[str] = []
    header_lines: list[str] = []
    current: list[str] = []

    for line in lines:
        if line.startswith("@@"):
            if current:
                hunks.append("".join(current))
            current = header_lines + [line]
        elif line.startswith(("diff --git", "index ", "--- ", "+++ ")):
            header_lines.append(line)
            if current:
                # flush any open hunk first
                hunks.append("".join(current))
                current = []
        else:
            if current:
                current.append(line)
            # lines before the first @@ go into header_lines
            elif not line.startswith("@@"):
                header_lines.append(line)

    if current:
        hunks.append("".join(current))

    return hunks or [diff_text]   # fallback: treat whole diff as one hunk


PRIOR_HUNK_WINDOW = 3   # how many previous hunk summaries to include as context


def summarize_file_diff(model: str, fname: str, diff_text: str) -> str:
    """Ask the model to summarize each hunk, passing the last 3 summaries as context."""
    hunks = split_hunks(diff_text)
    hunk_summaries: list[str] = []

    for i, hunk in enumerate(hunks, 1):
        hunk_snippet = hunk if len(hunk) <= 4000 else hunk[:4000] + "\n... (hunk truncated)"

        prior_context = ""
        if hunk_summaries:
            window = hunk_summaries[-PRIOR_HUNK_WINDOW:]
            start_idx = i - len(window)
            prior_context = "Previous hunks in this file:\n" + "\n".join(
                f"  Hunk {start_idx + j}: {s}" for j, s in enumerate(window)
            ) + "\n\n"

        prompt = textwrap.dedent(f"""\
            Summarize hunk {i} of {len(hunks)} from '{fname}' in 1-3 plain English sentences.
            Focus on WHAT changed and WHY it matters (if inferable). No jargon, no bullet points.
            Do not mention line numbers. Output only the summary, nothing else.

            {prior_context}Hunk {i}:
            {hunk_snippet}
        """)
        summary = call_ollama(model, prompt, num_ctx=HUNK_SUMMARIZE_CTX)
        hunk_summaries.append(summary.strip())

    return f"[summarized — {len(hunks)} hunk(s)]\n" + "\n".join(
        f"  Hunk {i}: {s}" for i, s in enumerate(hunk_summaries, 1)
    )


def build_diff_summary(repo: Path, model: str) -> str:
    """Return diff context for the classifier.

    Small files: include raw diff.
    Large files: summarize each hunk via a fast model call, then include summaries.
    """
    parts: list[str] = []
    total_chars = 0

    for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
        stat = git(["diff"] + extra_args + ["--stat", "--no-color"], repo).strip()
        if not stat:
            continue
        parts.append(f"{label}:\n{stat}\n")

        changed = git(["diff"] + extra_args + ["--name-only"], repo).splitlines()
        for fname in changed:
            file_diff = git(["diff"] + extra_args + ["--no-color", "--", fname], repo)

            if len(file_diff) <= DIFF_CHARS_PER_FILE:
                content = file_diff
            else:
                print(f"  Summarizing large diff: {fname} ({len(file_diff):,} chars)…",
                      file=sys.stderr)
                content = summarize_file_diff(model, fname, file_diff)

            parts.append(content)
            total_chars += len(content)

            if total_chars >= DIFF_TOTAL_CHARS:
                parts.append("\n... (remaining diffs omitted — total size limit reached)")
                break
        else:
            continue
        break   # hit total limit inside inner loop

    untracked = git(["ls-files", "--others", "--exclude-standard"], repo).splitlines()
    if untracked:
        parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))

    return "\n".join(parts)


# ── Ollama client ──────────────────────────────────────────────────────────────

def call_ollama(model: str, prompt: str, num_ctx: int = NUM_CTX) -> str:
    """Call Ollama chat completions and return the assistant message text."""
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True,
        "keep_alive": "1h",
        "options": {
            "temperature": 0.1,   # low temp for deterministic classification
            "num_ctx": num_ctx,
        },
    }
    with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=60.0, write=30.0, pool=5.0)) as client:
        with client.stream("POST", f"{OLLAMA_BASE_URL}/api/chat", json=payload) as response:
            response.raise_for_status()
            chunks = []
            for line in response.iter_lines():
                if not line:
                    continue
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                content = data.get("message", {}).get("content", "")
                if content:
                    chunks.append(content)
                if data.get("done"):
                    break
            return "".join(chunks)


# ── Tool definitions ───────────────────────────────────────────────────────────

class Commit(BaseModel):
    subject: str = Field(description="Conventional commit subject line (type(scope): desc), under 72 chars")
    files: list[str] = Field(description="Repository-relative paths to include in this commit")
    body: str = Field(default="", description="4-10 line plain-text commit body, no markdown")


class ProposeCommitsArgs(BaseModel):
    commits: list[Commit] = Field(description="Logical commit groups")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")


@tool(ProposeCommitsArgs)
def propose_commits() -> None:
    """Group working-tree changes into logical commits and suggest .gitignore patterns for junk files."""


SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    Given a list of changed files and their diffs, your job is to group them into
    one or more commits. Each commit should represent a single logical change
    (e.g. "add feature X", "fix bug in Y", "update dependencies", "refactor Z").

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    Rules:
    - Keep related changes together (same feature, same module, same concern)
    - Separate unrelated concerns into different commits
    - Dependency/lockfile changes belong with the commit that caused them
    - Test files belong with the code they test
    - Use conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
      Types: feat, fix, refactor, chore, docs, test, style, build, ci
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.

    Call the propose_commits tool with your answer.
""")


def classify_changes(model: str, files: list[str], diff_summary: str) -> tuple[list[dict], list[str]]:
    """Ask the model to group files into logical commits. Returns (commits, gitignore_patterns)."""
    file_list = "\n".join(f"  - {f}" for f in files)
    prompt = (
        f"{SYSTEM_PROMPT}\n\n"
        f"Changed files:\n{file_list}\n\n"
        f"Diffs:\n{diff_summary}"
    )

    result: ProposeCommitsArgs = call_ollama_with_tool(model, prompt, propose_commits)

    commits = [c.model_dump() for c in result.commits]
    if not commits:
        print("Error: Model returned no valid commit groups.", file=sys.stderr)
        sys.exit(2)

    return commits, result.gitignore




# ── Commit execution ───────────────────────────────────────────────────────────

def execute_commits(repo: Path, commits: list[dict]) -> None:
    """Stage and commit each group in order."""
    for i, commit in enumerate(commits, 1):
        subject = commit["subject"]
        body = commit.get("body", "").strip()
        message = f"{subject}\n\n{body}" if body else subject
        files = commit["files"]

        print(f"\n[{i}/{len(commits)}] {subject}")

        # Stage the files for this commit
        for f in files:
            try:
                git(["add", "--", f], repo)
                print(f"  + {f}")
            except RuntimeError as e:
                print(f"  ! Could not stage {f}: {e}", file=sys.stderr)

        # Check something is actually staged
        staged = git(["diff", "--cached", "--name-only"], repo).strip()
        if not staged:
            print("  (nothing staged, skipping)")
            continue

        try:
            git(["commit", "-m", message], repo)
            print(f"  ✓ Committed")
        except RuntimeError as e:
            print(f"  Error committing: {e}", file=sys.stderr)
            sys.exit(3)


# ── Display helpers ────────────────────────────────────────────────────────────

def print_proposed_commits(commits: list[dict], gitignore: list[str]) -> None:
    print("\nProposed commits:")
    print("─" * 60)
    for i, commit in enumerate(commits, 1):
        print(f"\n  [{i}] {commit['subject']}")
        body = commit.get("body", "").strip()
        if body:
            for line in body.splitlines():
                print(f"       {line}")
        for f in commit.get("files", []):
            print(f"       + {f}")
    if gitignore:
        print("\n  Suggested .gitignore patterns:")
        for pattern in gitignore:
            print(f"       {pattern}")
    print()


# ── Main ───────────────────────────────────────────────────────────────────────

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Group git changes into logical commits using Qwen3-Coder.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--repo", default=".", help="Path to git repository")
    parser.add_argument("--dry-run", action="store_true", help="Show proposed commits without executing")
    parser.add_argument("--model", default=DEFAULT_MODEL, help=f"Ollama model (default: {DEFAULT_MODEL})")
    parser.add_argument("--yes", "-y", action="store_true", help="Commit without confirmation")
    parser.add_argument("--json", dest="json_out", action="store_true", help="Output JSON and exit")

    args = parser.parse_args()
    repo = Path(args.repo).resolve()

    # Verify it's a git repo
    try:
        git(["rev-parse", "--git-dir"], repo)
    except RuntimeError:
        print(f"Error: {repo} is not a git repository.", file=sys.stderr)
        sys.exit(3)

    # Collect changed files
    print("Scanning for changes...", file=sys.stderr)
    files = get_changed_files(repo)
    if not files:
        print("No changes found.", file=sys.stderr)
        sys.exit(1)

    print(f"Found {len(files)} changed file(s). Analyzing...", file=sys.stderr)

    # Build stat summary and classify
    diff_summary = build_diff_summary(repo, args.model)
    commits, gitignore = classify_changes(args.model, files, diff_summary)

    # JSON mode: just dump and exit
    if args.json_out:
        print(json.dumps({"commits": commits, "gitignore": gitignore}, indent=2))
        return

    # Display proposed commits
    print_proposed_commits(commits, gitignore)

    if args.dry_run:
        return

    # Confirm unless --yes
    if not args.yes:
        try:
            answer = input("Proceed with these commits? [y/N] ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            print("\nCancelled.")
            sys.exit(4)
        if answer not in ("y", "yes"):
            print("Cancelled.")
            sys.exit(4)

    execute_commits(repo, commits)
    print("\nDone.")


if __name__ == "__main__":
    main()
