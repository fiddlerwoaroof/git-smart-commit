#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pydantic",
#   "textual>=0.50.0",
# ]
# ///
"""
git-smart-commit: Analyze unstaged changes, group into logical commits, execute them.

Uses an LLM for classification. Supports Ollama (default) and any OpenAI-compatible
API (OpenRouter, Anthropic, Together, etc.) via --api-base.

Usage:
    git-smart-commit [--repo PATH] [--dry-run] [--model MODEL] [--yes]
                     [--api-base URL] [--api-key KEY]
    git-smart-commit --setup

Options:
    --repo PATH     Path to git repository (default: current directory)
    --dry-run       Print proposed commits without executing
    --model MODEL   Model name (default: qwen3-coder:30b-a3b-q8_0)
    --yes           Skip confirmation prompt and commit immediately
    --json          Output proposed commits as JSON and exit (implies --dry-run)
    --quiet, -q     Suppress progress output (for scripting/CI)
    --api-base URL  API base URL (default: http://localhost:11434 for Ollama)
                    Use https://openrouter.ai/api/v1 for OpenRouter, etc.
    --api-key KEY   API key for authenticated endpoints (or set LLM_API_KEY env var)
    --setup         Run interactive configuration wizard and exit
    --help          Show this message

Git config keys (set with `git config [--global] <key> <value>`):
    smart-commit.model        Default model name
    smart-commit.api-base     Default API base URL
    smart-commit.api-key      Default API key
    smart-commit.issue-pattern  Extra pattern for the model to flag (repeatable:
                                  git config --add smart-commit.issue-pattern "...")

Config file: .smart-commit.toml in the repo root
    issue_patterns = ["warn about os.system()", "flag TODO comments"]
"""

from pathlib import Path
from pydantic import BaseModel, Field, ValidationError
from typing import Any, Callable

import argparse
import ast
import getpass
import httpx
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import textwrap
from dataclasses import dataclass, field


def wrap_markdown(text: str, width: int = 80) -> str:
    """Wrap text at width, preserving markdown structure.

    - Code blocks (```) are not wrapped
    - Inline code (`...`) within a line is preserved
    - List items wrap with a hanging indent; continuation lines (indented to
      at least the marker end) are wrapped with the same indent
    - Blockquotes are preserved with the correct continuation prefix
    - Regular paragraphs preserve their leading indent on continuation lines
    """
    if not text:
        return text

    lines = text.split('\n')
    result = []
    in_code_block = False
    # Number of leading spaces that mark a list continuation line; None when
    # we are not inside a list item.
    list_continuation_indent: int | None = None

    for line in lines:
        stripped = line.lstrip()
        indent = len(line) - len(stripped)

        # Code block fence toggles verbatim mode
        if stripped.startswith('```'):
            in_code_block = not in_code_block
            list_continuation_indent = None
            result.append(line)
            continue

        if in_code_block:
            result.append(line)
            continue

        # Blank line resets list state
        if not stripped:
            list_continuation_indent = None
            result.append(line)
            continue

        # New list item
        list_match = re.match(r'^(\s*)([-*+]|\d+\.)\s+', line)
        if list_match:
            marker_end = list_match.end()
            list_continuation_indent = marker_end
            content = line[marker_end:]
            wrapped_content = _wrap_line_preserving_inline(content, width - marker_end)
            wrapped_lines = wrapped_content.split('\n')
            result.append(line[:marker_end] + wrapped_lines[0])
            for wrapped_line in wrapped_lines[1:]:
                result.append(' ' * marker_end + wrapped_line)
            continue

        # List continuation line: indented to at least the marker position
        if list_continuation_indent is not None and indent >= list_continuation_indent:
            cont_prefix = ' ' * indent
            wrapped_content = _wrap_line_preserving_inline(stripped, width - indent)
            wrapped_lines = wrapped_content.split('\n')
            result.append(cont_prefix + wrapped_lines[0])
            for wrapped_line in wrapped_lines[1:]:
                result.append(cont_prefix + wrapped_line)
            continue

        # Not a list continuation — reset list state
        list_continuation_indent = None

        # Blockquote
        if stripped.startswith('>'):
            quote_match = re.match(r'^(\s*)(>+\s?)', line)
            if quote_match:
                full_prefix = quote_match.group(1) + quote_match.group(2)
                content = line[len(full_prefix):]
                wrapped_content = _wrap_line_preserving_inline(content, width - len(full_prefix))
                wrapped_lines = wrapped_content.split('\n')
                result.append(full_prefix + wrapped_lines[0])
                for wrapped_line in wrapped_lines[1:]:
                    result.append(full_prefix + wrapped_line)
                continue

        # Regular paragraph — preserve leading indent on all continuation lines
        cont_prefix = ' ' * indent
        wrapped_content = _wrap_line_preserving_inline(stripped, width - indent)
        wrapped_lines = wrapped_content.split('\n')
        result.append(cont_prefix + wrapped_lines[0])
        for wrapped_line in wrapped_lines[1:]:
            result.append(cont_prefix + wrapped_line)

    return '\n'.join(result)


def _wrap_line_preserving_inline(text: str, width: int) -> str:
    """Wrap a line while preserving inline code spans."""
    if len(text) <= width:
        return text

    # Split on inline code to preserve it
    parts = re.split(r'(`[^`]+`)', text)
    lines = []
    current_line = ''

    for part in parts:
        if not part:
            continue
        if part.startswith('`') and part.endswith('`'):
            # Inline code - add to current line if it fits, else start new line
            if current_line and len(current_line) + 1 + len(part) > width:
                lines.append(current_line.rstrip())
                current_line = part
            else:
                if current_line:
                    current_line += ' '
                current_line += part
        else:
            # Regular text - wrap it
            words = part.split(' ')
            for word in words:
                if not word:
                    continue
                if current_line and len(current_line) + 1 + len(word) > width:
                    lines.append(current_line.rstrip())
                    current_line = word
                else:
                    if current_line:
                        current_line += ' '
                    current_line += word

    if current_line:
        lines.append(current_line.rstrip())

    return '\n'.join(lines)


# ── Configuration ──────────────────────────────────────────────────────────────

SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    Given a list of changed files and their diffs, your job is to group them into
    one or more commits. Each commit should represent a single logical change
    (e.g. "add feature X", "fix bug in Y", "update dependencies",
    "refactor Z").

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Changes are represented as individual diff hunks, each identified by an ID
      like "src/main.py::1". Untracked (new) files also use "filepath::1".
    - You CAN split a file across multiple commits by assigning different hunks
      from that file to different commits. Prefer splitting when hunks are unrelated.
    - Keep related hunks together (same feature, same module, same concern).
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).
    - BREAKING CHANGE detection: if a commit removes or renames a public function,
      class, or module; deletes an exported symbol; or changes a function/method
      signature incompatibly (added required param, removed param, changed type),
      set breaking_change to a concise description of what broke and how callers
      must update. Also add '!' after the type in the subject (e.g. 'feat!(api):').
      Leave breaking_change empty for purely additive or internal changes.

    You have access to a read_file tool. Use it when the diff alone is not
    enough context — for example, to see which class a changed function belongs
    to, what imports already exist, or how unchanged surrounding code looks.
    Call read_file as many times as needed, then call propose_commits when ready.
""")


AGENTIC_SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    You have investigation tools to gather information and emit commits incrementally:
    - read_file(path): Read the full current content of any file in the repository
    - get_diff(path, staged): Get the raw diff for a specific file on demand
      (staged=true for staged only, staged=false for unstaged only, omit for both)
    - get_git_log(n): Get the last n commit subjects to learn this project's conventions
    - search_diff(pattern, context_lines): Search all diffs for a regex pattern
    - query_tool_result(result_id, question): When any prior content (the initial diff
      context or a tool result) was summarized to save context, use this to ask a targeted
      question about the original full output. The result_id is shown in the summary text
      (e.g. "[Summarized — id: 'r1'...]"). Use this instead of re-fetching data you already
      have — the initial diff context uses id 'r1' when summarized.
    - emit_commit(commit): Emit a single logical commit. Call this as soon as you identify
      a logical grouping of hunks. It validates hunk IDs, rejects duplicates, and returns
      the remaining unassigned hunks so you can plan the next commit.
    - finalize_commits(gitignore): Finalize all emitted commits. Call this once after you
      have emitted all commits via emit_commit. Pass any suggested .gitignore patterns here.

    ## Context management
    Tool results are automatically summarized when they exceed a size threshold.
    You will see a header like: [Summarized — id: 'rN'. Use query_tool_result to retrieve original content]
    followed by a compact summary.

    Key rules:
    - Do NOT re-request content via read_file after seeing a summary. Use query_tool_result(result_id) instead.
    - Summary IDs (like 'r5') are stable and can be queried multiple times.
    - query_tool_result returns the full original content for any summarized result.
    - Plan your investigation to minimize re-reads: extract what you need from summaries first.

    Investigation strategy:
    - Start with the file list and stats you receive
    - Use get_git_log early to learn this project's commit style conventions
    - Use get_diff on files whose grouping is unclear from stats alone
    - Use read_file when you need full file context (imports, class structure, etc.)
    - Use search_diff to find patterns across all changes (debug prints, TODOs, etc.)
    - If the initial diff context or a prior tool result was summarized, use
      query_tool_result with the id shown in the summary before re-fetching it
    - As you identify logical groupings of hunks, call emit_commit immediately for
      each commit. Check the remaining unassigned hunks in the response.
    - When all hunks are assigned (or remaining hunks should be ignored), call
      finalize_commits

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Changes are represented as individual diff hunks, each identified by an ID
      like "src/main.py::1". Untracked (new) files also use "filepath::1".
    - You CAN split a file across multiple commits by assigning different hunks
      from that file to different commits. Prefer splitting when hunks are unrelated.
    - Keep related hunks together (same feature, same module, same concern).
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).
    - BREAKING CHANGE detection: if a commit removes or renames a public function,
      class, or module; deletes an exported symbol; or changes a function/method
      signature incompatibly (added required param, removed param, changed type),
      set breaking_change to a concise description of what broke and how callers
      must update. Also add '!' after the type in the subject (e.g. 'feat!(api):').
      Leave breaking_change empty for purely additive or internal changes.

    When you have investigated enough to be confident in a grouping, call emit_commit
    immediately for that commit. Continue investigating and emitting commits until all
    hunks are assigned or remaining hunks should be ignored, then call finalize_commits.
""")


OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "gpt-oss:20b"

DIFF_CHARS_PER_FILE = 6000    # files larger than this get hunk-summarized
DIFF_TOTAL_CHARS   = 120000   # hard cap on total context sent to classifier
PRIOR_HUNK_WINDOW  = 4        # how many previous hunk summaries to include as context

MAX_AGENTIC_TURNS     = 20    # base safeguard for the agentic investigation loop
MAX_AGENTIC_TURNS_CAP = 50    # absolute ceiling when scaling with hunk count
READ_FILE_LIMIT    = 50_000   # max chars returned by the read_file tool

CONTEXT_TRIM_THRESHOLD        = 300_000  # total message chars before summarizing old results
TOOL_RESULT_SUMMARIZE_SKIP    = 500      # results shorter than this are never summarized
TOOL_RESULT_SUMMARIZE_INPUT   = 20_000   # max chars fed to the summarizer model
TURNS_WARN_AT                 = 6        # remaining turns at which to inject a commit warning

# When compressing old context, the *most recent* tool result is kept verbatim unless it
# exceeds this size — the model is likely still working with it and losing detail hurts quality.
RECENT_TOOL_RESULT_CHARS      = 10_000

# Ollama runs locally; a much lower trim threshold prevents timeouts from huge prompts.
OLLAMA_CONTEXT_TRIM_THRESHOLD = 20_000

# Preset backends for the setup wizard
SETUP_BACKENDS = [
    {
        "key": "ollama",
        "label": "Ollama  (local, no API key needed)",
        "api_base": OLLAMA_BASE_URL,
        "default_model": DEFAULT_MODEL,
        "needs_key": False,
    },
    {
        "key": "openrouter",
        "label": "OpenRouter  (cloud, API key required)",
        "api_base": "https://openrouter.ai/api/v1",
        "default_model": "anthropic/claude-3.5-sonnet",
        "needs_key": True,
    },
    {
        "key": "anthropic",
        "label": "Anthropic  (cloud, API key required)",
        "api_base": "https://api.anthropic.com/v1",
        "default_model": "claude-sonnet-4-6",
        "needs_key": True,
    },
    {
        "key": "custom",
        "label": "Custom endpoint",
        "api_base": None,
        "default_model": None,
        "needs_key": None,
    },
]


@dataclass
class ApiConfig:
    """API connection and model configuration."""
    base_url: str = OLLAMA_BASE_URL
    model: str = DEFAULT_MODEL
    api_key: str | None = None
    num_ctx: int = 128000

    @property
    def is_ollama(self) -> bool:
        """True when targeting a native Ollama endpoint (not OpenAI-compatible)."""
        # OpenAI-compatible APIs use /v1 in the path; Ollama doesn't.
        return "/v1" not in self.base_url

    @property
    def chat_url(self) -> str:
        if self.is_ollama:
            return f"{self.base_url}/api/chat"
        return f"{self.base_url}/chat/completions"

    @property
    def auth_headers(self) -> dict[str, str]:
        if self.api_key:
            return {"Authorization": f"Bearer {self.api_key}"}
        return {}

    @property
    def context_trim_threshold(self) -> int:
        """Total message chars at which old content starts being summarised.

        Ollama runs locally and is slow; trigger compression much earlier so that
        the prompt sent to the model stays well within a size it can handle quickly.
        """
        return OLLAMA_CONTEXT_TRIM_THRESHOLD if self.is_ollama else CONTEXT_TRIM_THRESHOLD


@dataclass
class DiffHunk:
    """A single diff hunk that can be independently staged via git apply --cached."""
    hunk_id: str        # e.g. "src/main.py::1" (same scheme for tracked and untracked files)
    file_path: str      # repository-relative path
    patch: str          # complete patch text for git apply --cached; empty for untracked
    is_untracked: bool = False
    is_binary: bool = False


@dataclass
class ToolCall:
    """Represents a tool call extracted from an LLM response."""
    name: str
    arguments: dict
    call_id: str | None = None


# ── Tool decorator ─────────────────────────────────────────────────────────────
#
# Defining a new tool:
#   1. Define argument types as pydantic BaseModel subclasses
#   2. Define a top-level args model (also a BaseModel) for the tool
#   3. Decorate a function with @tool(ArgsModel)
#
# Schema generation, validation, and parsing are all handled automatically.

def tool(args_model: type[BaseModel]):
    """Decorator factory. @tool(MyArgsModel) attaches LLM tool metadata to a function."""
    def decorator(fn: Callable) -> Callable:
        spec = {
            "type": "function",
            "function": {
                "name": fn.__name__,
                "description": (fn.__doc__ or "").strip(),
                "parameters": args_model.model_json_schema(),
            },
        }
        fn._tool_spec = spec
        fn._tool_model = args_model
        return fn
    return decorator


def _summarize_args(arguments: dict, max_len: int = 80) -> str:
    """Return a compact one-line summary of tool call arguments for logging."""
    raw = json.dumps(arguments, separators=(",", ":"))
    if len(raw) <= max_len:
        return raw
    return raw[:max_len - 3] + "..."


def _total_message_chars(messages: list[dict]) -> int:
    """Count total characters across all message content fields."""
    return sum(len(str(m.get("content", "") or "")) for m in messages)


# ── LLM client ────────────────────────────────────────────────────────────────

@dataclass
class TokenUsage:
    """Accumulated token usage across all API calls in a session."""
    prompt_tokens: int = 0
    completion_tokens: int = 0

    def __add__(self, other: "TokenUsage") -> "TokenUsage":
        return TokenUsage(
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            completion_tokens=self.completion_tokens + other.completion_tokens,
        )

    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.completion_tokens

    def __str__(self) -> str:
        return (f"{self.total_tokens:,} tokens "
                f"({self.prompt_tokens:,} in / {self.completion_tokens:,} out)")


class ResponseParser:
    """Backend-specific API response parsing.

    Subclass and override get_message() for new backends.  Override
    REASONING_FIELDS to control which response fields are checked for
    thinking/reasoning tokens (checked in order, first non-empty wins).
    """

    REASONING_FIELDS: tuple[str, ...] = ("thinking", "reasoning", "reasoning_content")

    def get_message(self, data: dict) -> dict:
        """Extract the assistant message dict from a raw API response."""
        raise NotImplementedError

    def extract_reasoning(self, data: dict) -> str | None:
        msg = self.get_message(data)
        for field in self.REASONING_FIELDS:
            value = msg.get(field)
            if value and isinstance(value, str) and value.strip():
                return value.strip()
        return None

    def extract_tool_call(self, data: dict) -> "ToolCall | None":
        msg = self.get_message(data)
        tool_calls = msg.get("tool_calls", [])
        if tool_calls:
            tc = tool_calls[0]
            raw = tc["function"]["arguments"]
            args = json.loads(raw) if isinstance(raw, str) else raw
            return ToolCall(name=tc["function"]["name"], arguments=args,
                            call_id=tc.get("id"))
        return None

    def extract_tool_args(self, data: dict) -> dict | None:
        """Extract tool-call arguments, falling back to content-as-JSON."""
        tc = self.extract_tool_call(data)
        if tc is not None:
            return tc.arguments
        content = self.get_message(data).get("content", "").strip()
        if content:
            content = content.removeprefix("```json").removeprefix("```").strip().removesuffix("```").strip()
            return json.loads(content)
        return None

    def extract_text(self, data: dict) -> str | None:
        content = self.get_message(data).get("content", "").strip()
        return content or None

    def format_assistant(self, data: dict) -> dict:
        return self.get_message(data) or {"role": "assistant", "content": ""}

    def format_tool_result(self, call_id: str | None, content: str) -> dict:
        return {"role": "tool", "content": content}

    def extract_usage(self, data: dict) -> TokenUsage:
        raise NotImplementedError


class OllamaParser(ResponseParser):
    """Response parser for the native Ollama /api/chat endpoint."""

    def get_message(self, data: dict) -> dict:
        return data.get("message", {})

    def extract_usage(self, data: dict) -> TokenUsage:
        return TokenUsage(
            prompt_tokens=data.get("prompt_eval_count", 0),
            completion_tokens=data.get("eval_count", 0),
        )


class OpenAIParser(ResponseParser):
    """Response parser for OpenAI-compatible /chat/completions endpoints."""

    def get_message(self, data: dict) -> dict:
        choices = data.get("choices", [])
        return choices[0].get("message", {}) if choices else {}

    def format_tool_result(self, call_id: str | None, content: str) -> dict:
        if call_id is not None:
            return {"role": "tool", "tool_call_id": call_id, "content": content}
        return {"role": "tool", "content": content}

    def extract_usage(self, data: dict) -> TokenUsage:
        usage = data.get("usage", {})
        return TokenUsage(
            prompt_tokens=usage.get("prompt_tokens", 0),
            completion_tokens=usage.get("completion_tokens", 0),
        )


class LLMClient:
    """Encapsulates all communication with the LLM API (Ollama or OpenAI-compatible)."""

    def __init__(self, config: ApiConfig):
        self.config = config
        self.usage = TokenUsage()
        self.parser: ResponseParser = OllamaParser() if config.is_ollama else OpenAIParser()
        self._http = httpx.Client(
            timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0),
        )

    # -- resource management ---------------------------------------------------

    def close(self) -> None:
        """Close the underlying HTTP connection pool."""
        self._http.close()

    def __enter__(self) -> "LLMClient":
        return self

    def __exit__(self, *exc: object) -> None:
        self.close()

    def _build_payload(self, prompt: str, tools: list[dict] | None = None,
                       stream: bool = False) -> dict:
        """Build an API request payload, adapting to Ollama or OpenAI format."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
        else:
            payload = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
                # Force the model to call the specific tool
                payload["tool_choice"] = {
                    "type": "function",
                    "function": {"name": tools[0]["function"]["name"]},
                }
        return payload

    def _extract_tool_args(self, data: dict) -> dict | None:
        """Extract tool-call arguments from a response, handling both API formats."""
        return self.parser.extract_tool_args(data)

    def _build_payload_messages(self, messages: list[dict],
                                tools: list[dict] | None = None,
                                tool_choice: str | dict | None = None,
                                stream: bool = False) -> dict:
        """Build a payload from a full conversation history (messages list)."""
        if self.config.is_ollama:
            payload: dict[str, Any] = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "keep_alive": "1h",
                "options": {
                    "temperature": 0.2,
                    "num_ctx": self.config.num_ctx,
                },
            }
            if tools:
                payload["tools"] = tools
            # Ollama does not support tool_choice — omit it
        else:
            payload = {
                "model": self.config.model,
                "messages": messages,
                "stream": stream,
                "temperature": 0.2,
            }
            if tools:
                payload["tools"] = tools
            if tool_choice is not None:
                payload["tool_choice"] = tool_choice
        return payload

    def _extract_tool_call(self, data: dict) -> ToolCall | None:
        """Extract a tool call from an API response, handling both Ollama and OpenAI formats."""
        return self.parser.extract_tool_call(data)

    def _extract_text_content(self, data: dict) -> str | None:
        """Extract plain text content from a response when no tool call is present."""
        return self.parser.extract_text(data)

    def _extract_reasoning(self, data: dict) -> str | None:
        """Extract reasoning/thinking tokens from an API response if present."""
        return self.parser.extract_reasoning(data)

    def _format_assistant_tool_call(self, data: dict) -> dict:
        """Extract the assistant message from a raw API response for conversation history.

        Using the raw response message guarantees the format Ollama/OpenAI expects to see back.
        """
        return self.parser.format_assistant(data)

    def _format_tool_result(self, call_id: str | None, content: str) -> dict:
        """Build a tool-result message for appending to conversation history."""
        return self.parser.format_tool_result(call_id, content)

    def _extract_usage(self, data: dict) -> TokenUsage:
        """Extract token usage from an API response (Ollama or OpenAI format)."""
        return self.parser.extract_usage(data)

    def _retry_request(self, fn: Callable, max_retries: int = 3, initial_delay: float = 1.0) -> Any:
        """Execute fn with exponential backoff on transient errors."""
        for attempt in range(max_retries + 1):
            try:
                return fn()
            except (httpx.ConnectError, httpx.ReadTimeout, httpx.WriteTimeout,
                    httpx.PoolTimeout, httpx.ConnectTimeout) as e:
                if attempt == max_retries:
                    raise
                delay = initial_delay * (2 ** attempt)
                _log(f"  {_ansi('↻', _ANSI_DIM)} retry {attempt + 1}/{max_retries} after {delay:.1f}s: {type(e).__name__}")
                import time
                time.sleep(delay)
            except httpx.HTTPStatusError as e:
                # Retry on 429 (rate limit) and 5xx (server errors)
                if e.response.status_code in (429, 500, 502, 503, 504) and attempt < max_retries:
                    delay = initial_delay * (2 ** attempt)
                    # Honor Retry-After header if present
                    retry_after = e.response.headers.get("retry-after")
                    if retry_after:
                        try:
                            delay = max(delay, float(retry_after))
                        except ValueError:
                            pass
                    _log(f"  {_ansi('↻', _ANSI_DIM)} retry {attempt + 1}/{max_retries} after {delay:.1f}s: HTTP {e.response.status_code}")
                    import time
                    time.sleep(delay)
                else:
                    raise

    def call(self, prompt: str, num_ctx: int | None = None) -> str:
        """Call the LLM with streaming and return the assistant message text."""
        payload = self._build_payload(prompt, stream=True)
        if num_ctx is not None and self.config.is_ollama:
            payload["options"]["num_ctx"] = num_ctx
        headers = {**self.config.auth_headers, "Content-Type": "application/json"}

        def _do_stream() -> str:
            with self._http.stream("POST", self.config.chat_url, json=payload, headers=headers) as response:
                response.raise_for_status()
                chunks: list[str] = []
                for line in response.iter_lines():
                    if not line:
                        continue
                    # OpenAI SSE format: lines prefixed with "data: "
                    if line.startswith("data: "):
                        line = line[6:]
                    if line.strip() == "[DONE]":
                        break
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    if self.config.is_ollama:
                        content = data.get("message", {}).get("content", "")
                        done = data.get("done", False)
                    else:
                        delta = (data.get("choices", [{}])[0]
                                 .get("delta", {}))
                        content = delta.get("content", "")
                        done = data.get("choices", [{}])[0].get("finish_reason") is not None
                    if content:
                        chunks.append(content)
                    if done:
                        self.usage = self.usage + self._extract_usage(data)
                        break
                return "".join(chunks)

        return self._retry_request(_do_stream)

    def call_with_tool(self, prompt: str, tool_fn: Callable, **tool_kwargs) -> Any:
        """Call the LLM forcing tool_fn to be called.

        Parses and validates the response into the tool's args model, then calls
        tool_fn(validated_args, **tool_kwargs) and returns its result.
        """
        spec = tool_fn._tool_spec
        args_model: type[BaseModel] = tool_fn._tool_model
        actual_prompt = prompt
        retries = 0
        failures = []
        while True:
            retries += 1
            payload = self._build_payload(actual_prompt, tools=[spec], stream=False)
            headers = {**self.config.auth_headers, "Content-Type": "application/json"}

            def _do_tool_post() -> dict:
                response = self._http.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                return response.json()

            data = self._retry_request(_do_tool_post)
            self.usage = self.usage + self._extract_usage(data)

            raw_args = self._extract_tool_args(data)
            if raw_args is None:
                raw_args = {}

            # Fix stringified nested values: some models return e.g.
            # {"commits": "[{'subject': ...}]"} instead of a proper nested structure.
            # Detect strings that look like Python lists/dicts and try to parse them.
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass  # leave as-is, let pydantic report the error

            try:
                validated = args_model.model_validate(raw_args).post_process()
                return tool_fn(validated, **tool_kwargs)
            except ValidationError as e:
                failures.append(str(e))
                if retries > 5:
                    raise

                failure_summary = "\n".join(set(failures))
                actual_prompt = textwrap.dedent(f"""
                Your previous ({len(failures)}) attempts failed with a validation error:
                {failure_summary}

                Try again following this prompt exactly:
                {prompt}
                """)

    def agentic_loop(self, system_prompt: str, initial_user_message: str,
                     tool_registry: dict[str, Callable],
                     terminal_tool: Callable, max_turns: int = MAX_AGENTIC_TURNS,
                     countdown_message_fn: Callable | None = None,
                     **kwargs) -> Any:
        """Run a multi-turn agentic loop until the terminal tool is called.

        Each turn: send messages → extract tool call → dispatch.
        Investigation tool results are appended to history.
        Validation errors for the terminal tool become tool-result messages.
        On the last turn, only the terminal tool is offered.

        Context management: when total message size exceeds CONTEXT_TRIM_THRESHOLD,
        old tool-result messages are replaced with LLM-generated summaries. The
        original full content is stored and accessible via the query_tool_result
        tool so the model can ask targeted questions without re-fetching.
        A countdown warning is injected when TURNS_WARN_AT turns remain.
        """
        # ── result storage for compressed tool results ─────────────────────────
        result_store: dict[str, str] = {}  # result_id → original full content

        def _summarize_and_store(msg_idx: int, content: str) -> str:
            """Summarize a tool result via LLM with three-level escalation.

            Level 1 -- Normal: 2-4 sentence summary preserving suspicious code.
            Level 2 -- Aggressive: max 3 bullet points if Level 1 didn't shrink.
            Level 3 -- Deterministic truncate: 512-char hard cut, no LLM call.

            The original content is always stored in result_store so
            query_tool_result can retrieve it later.
            """
            result_id = f"r{msg_idx}"
            result_store[result_id] = content
            header = (
                f"[Summarized — id: '{result_id}'. "
                f"Use query_tool_result to ask specific questions about the full output.]\n"
            )
            input_snippet = content[:TOOL_RESULT_SUMMARIZE_INPUT]

            # Level 1 — Normal summary
            prompt_l1 = textwrap.dedent(f"""\
                Summarize this tool output in 2-4 sentences. Focus on filenames,
                key changes, and any suspicious code. Preserve suspicious code verbatim.

                {input_snippet}
            """)
            summary_l1 = self.call(prompt_l1).strip()
            if len(summary_l1) < len(content):
                _log(f"  {_ansi('⋯', _ANSI_DIM)} summarization level 1 (normal) succeeded for #{msg_idx}")
                return header + summary_l1

            # Level 2 — Aggressive bullet points (target half the tokens)
            prompt_l2 = textwrap.dedent(f"""\
                Compress this to bullet points. Maximum 3 bullets, one line each.
                Preserve file names and key identifiers only.

                {input_snippet}
            """)
            summary_l2 = self.call(prompt_l2).strip()
            if len(summary_l2) < len(content):
                _log(f"  {_ansi('⋯', _ANSI_DIM)} summarization level 2 (aggressive) succeeded for #{msg_idx}")
                return header + summary_l2

            # Level 3 — Deterministic truncate (always reduces size)
            _log(f"  {_ansi('⋯', _ANSI_DIM)} summarization level 3 (deterministic truncate) for #{msg_idx}")
            truncated = content[:512] + "\n[Truncated — use query_tool_result for full content]"
            return header + truncated

        def _trim_with_summaries(messages: list[dict]) -> None:
            """Summarize oldest large messages until total context is under threshold.

            Compresses (oldest first):
            - The initial diff context (messages[1], role=user) when it's large.
            - Tool results, skipping the most recent one unless it exceeds
              RECENT_TOOL_RESULT_CHARS — the model is likely still using it.

            All compressed content is stored in result_store so query_tool_result
            can answer targeted questions without losing any information.
            """
            threshold = self.config.context_trim_threshold
            # Index of the most recent tool message — kept verbatim if small
            last_tool_idx = max(
                (i for i, m in enumerate(messages) if m.get("role") == "tool"),
                default=-1,
            )
            for i, msg in enumerate(messages):
                if _total_message_chars(messages) <= threshold:
                    break
                role = msg.get("role")
                content = msg.get("content", "")
                if not isinstance(content, str):
                    continue
                if content.startswith("[Summarized"):  # already compressed
                    continue

                if role == "tool":
                    # Protect the most recent tool result so the model can act on it
                    if i == last_tool_idx and len(content) <= RECENT_TOOL_RESULT_CHARS:
                        continue
                    if len(content) <= TOOL_RESULT_SUMMARIZE_SKIP:
                        continue
                    _log(f"  {_ansi('⋯', _ANSI_DIM)} compressing tool result #{i} ({len(content):,} chars)")
                    messages[i] = {**msg, "content": _summarize_and_store(i, content)}

                elif role == "user" and i == 1:
                    # The initial diff context message — also compressible when large
                    if len(content) <= TOOL_RESULT_SUMMARIZE_SKIP:
                        continue
                    _log(f"  {_ansi('⋯', _ANSI_DIM)} compressing initial diff context ({len(content):,} chars)")
                    messages[i] = {**msg, "content": _summarize_and_store(i, content)}

        # ── query_tool_result: closure over result_store and self ───────────────
        @tool(QueryResultArgs)
        def query_tool_result(args: QueryResultArgs, **_: Any) -> str:
            """Access the full (unsummarized) content of a previously summarized tool result.

            When a tool result was compressed to save context, use this to ask a
            targeted question. The answer is generated from the original full content
            so no information is lost.
            """
            full = result_store.get(args.result_id)
            if full is None:
                available = list(result_store.keys()) or ["none"]
                return (
                    f"No stored result with id '{args.result_id}'. "
                    f"Available ids: {available}"
                )
            prompt = textwrap.dedent(f"""\
                The following is the full content of stored tool result '{args.result_id}':

                {full[:50_000]}

                Question: {args.question}

                Answer concisely and directly based only on the content above.
            """)
            return self.call(prompt)

        # ── build extended registry that includes query_tool_result ─────────────
        extended_registry = dict(tool_registry)
        extended_registry["query_tool_result"] = query_tool_result

        messages: list[dict] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": initial_user_message},
        ]

        terminal_spec = terminal_tool._tool_spec
        terminal_name: str = terminal_spec["function"]["name"]
        terminal_model: type[BaseModel] = terminal_tool._tool_model
        all_tool_specs = [fn._tool_spec for fn in extended_registry.values()] + [terminal_spec]

        headers = {**self.config.auth_headers, "Content-Type": "application/json"}

        for turn in range(1, max_turns + 1):
            is_last_turn = (turn == max_turns)
            turns_remaining = max_turns - turn

            if is_last_turn:
                tools = [terminal_spec]
                tool_choice: str | dict | None = (
                    None if self.config.is_ollama
                    else {"type": "function", "function": {"name": terminal_name}}
                )
            else:
                tools = all_tool_specs
                tool_choice = None if self.config.is_ollama else "auto"

            payload = self._build_payload_messages(messages, tools=tools,
                                                   tool_choice=tool_choice, stream=False)

            def _do_agentic_post() -> dict:
                response = self._http.post(self.config.chat_url, json=payload, headers=headers)
                response.raise_for_status()
                return response.json()

            data = self._retry_request(_do_agentic_post)
            self.usage = self.usage + self._extract_usage(data)

            reasoning = self._extract_reasoning(data)
            if reasoning:
                _log_reasoning(reasoning)

            tool_call = self._extract_tool_call(data)

            if tool_call is None:
                text = self._extract_text_content(data) or ""
                _log(
                    f"  {_ansi('⟳', _ANSI_DIM)} {_ansi(f'[{turn}/{max_turns}]', _ANSI_DIM)}"
                    f" {_ansi('(no tool call — nudging)', _ANSI_DIM)}"
                )
                messages.append({"role": "assistant", "content": text})
                messages.append({
                    "role": "user",
                    "content": (
                        f"Please call a tool. Use the investigation tools to gather more "
                        f"information, or call {terminal_name} if you are ready to propose commits."
                    ),
                })
                continue

            _log(
                f"  {_ansi('⟳', _ANSI_DIM)} {_ansi(f'[{turn}/{max_turns}]', _ANSI_DIM)}"
                f" {_ansi(tool_call.name, _ANSI_CYAN)}"
                f"({_ansi(_summarize_args(tool_call.arguments), _ANSI_DIM)})"
            )
            messages.append(self._format_assistant_tool_call(data))

            # Coerce stringified nested values (some models return JSON-as-string)
            raw_args = dict(tool_call.arguments)
            for key, val in raw_args.items():
                if isinstance(val, str) and val.strip().startswith(("[", "{")):
                    try:
                        raw_args[key] = json.loads(val)
                    except json.JSONDecodeError:
                        try:
                            raw_args[key] = ast.literal_eval(val)
                        except (ValueError, SyntaxError):
                            pass

            if tool_call.name == terminal_name:
                try:
                    validated = terminal_model.model_validate(raw_args).post_process()
                    return terminal_tool(validated, **kwargs)
                except ValidationError as e:
                    err_str = (
                        f"Validation error: {e}\nPlease fix the arguments and call "
                        f"{terminal_name} again."
                    )
                    if len(err_str) > TOOL_RESULT_SUMMARIZE_SKIP:
                        err_str = _summarize_and_store(len(messages), err_str)
                    messages.append(self._format_tool_result(
                        tool_call.call_id, err_str,
                    ))
                    continue
                except ValueError as e:
                    err_str = (
                        f"Error: {e}\nPlease fix the arguments and call "
                        f"{terminal_name} again."
                    )
                    if len(err_str) > TOOL_RESULT_SUMMARIZE_SKIP:
                        err_str = _summarize_and_store(len(messages), err_str)
                    messages.append(self._format_tool_result(
                        tool_call.call_id, err_str,
                    ))
                    continue

            tool_fn = extended_registry.get(tool_call.name)
            if tool_fn is None:
                result_str = (f"Unknown tool: {tool_call.name!r}. "
                              f"Available: {list(extended_registry.keys())}")
            else:
                try:
                    validated_args = tool_fn._tool_model.model_validate(raw_args).post_process()
                    result = tool_fn(validated_args, **kwargs)
                    result_str = result if isinstance(result, str) else json.dumps(result)
                except Exception as e:
                    result_str = f"Error calling {tool_call.name}: {e}"

            # Proactive summarization: summarize large tool results immediately
            # so the LLM works from compact summaries and can drill down via
            # query_tool_result. _trim_with_summaries remains as a safety net.
            if len(result_str) > TOOL_RESULT_SUMMARIZE_SKIP:
                result_str = _summarize_and_store(len(messages), result_str)

            messages.append(self._format_tool_result(tool_call.call_id, result_str))

            # Context management: when messages are large, summarize old content
            # (diffs and tool results) and store originals for query_tool_result.
            total_chars = _total_message_chars(messages)
            if total_chars > self.config.context_trim_threshold:
                _log(f"  {_ansi('⋯', _ANSI_DIM)} context {total_chars:,} chars — compressing old results")
                _trim_with_summaries(messages)

            # Inject a countdown warning when few turns remain so the model
            # knows it must call the terminal tool soon.
            if 0 < turns_remaining <= TURNS_WARN_AT:
                if countdown_message_fn is not None:
                    warning_text = countdown_message_fn(turns_remaining)
                else:
                    warning_text = (
                        f"WARNING: Only {turns_remaining} turn(s) remaining. "
                        f"Stop gathering context and call {terminal_name} NOW with "
                        f"your best grouping based on what you already know."
                    )
                messages.append({
                    "role": "user",
                    "content": warning_text,
                })

        raise RuntimeError(
            f"agentic_loop exceeded {max_turns} turns without calling {terminal_name}"
        )



# ── Git helpers ────────────────────────────────────────────────────────────────

def _git(args: list[str], cwd: Path) -> str:
    """Run a git command and return stdout. Raises on non-zero exit."""
    result = subprocess.run(
        ["git"] + args,
        cwd=cwd,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed:\n{result.stderr.strip()}")
    return result.stdout


def read_git_config(repo: Path) -> dict[str, str | list[str]]:
    """Read smart-commit.* keys from git config (local → global → system).

    Returns a dict of whichever keys are set; missing keys are absent.
    Scalar keys: smart-commit.model, smart-commit.api-base, smart-commit.api-key
    Multi-value key: smart-commit.issue-pattern  (list[str])
    """
    scalar_keys = ["smart-commit.model", "smart-commit.api-base", "smart-commit.api-key"]
    config: dict[str, str | list[str]] = {}
    for key in scalar_keys:
        result = subprocess.run(
            ["git", "config", "--get", key],
            cwd=repo,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0 and result.stdout.strip():
            config[key] = result.stdout.strip()
    # Multi-value key: each `git config --add` call appends another value
    result = subprocess.run(
        ["git", "config", "--get-all", "smart-commit.issue-pattern"],
        cwd=repo,
        capture_output=True,
        text=True,
    )
    if result.returncode == 0:
        patterns = [p for p in result.stdout.splitlines() if p.strip()]
        if patterns:
            config["smart-commit.issue-pattern"] = patterns
    return config


def read_issue_patterns(repo: Path, gc: dict) -> list[str]:
    """Return project-specific issue patterns from git config and .smart-commit.toml.

    Sources, merged in this order (duplicates removed, order preserved):
    1. smart-commit.issue-pattern values from git config (local overrides global)
    2. issue_patterns list from <repo>/.smart-commit.toml

    Each pattern is a plain-text description of something the model should
    flag as an issue, e.g. "warn about os.system() — prefer subprocess".
    """
    import tomllib

    patterns: list[str] = list(gc.get("smart-commit.issue-pattern", []))  # type: ignore[arg-type]

    toml_path = repo / ".smart-commit.toml"
    if toml_path.exists():
        try:
            with open(toml_path, "rb") as f:
                data = tomllib.load(f)
            for entry in data.get("issue_patterns", []):
                if isinstance(entry, str) and entry.strip():
                    patterns.append(entry.strip())
        except Exception as exc:
            print(f"Warning: could not read {toml_path}: {exc}", file=sys.stderr)

    # Deduplicate while preserving order
    seen: set[str] = set()
    result: list[str] = []
    for p in patterns:
        if p not in seen:
            seen.add(p)
            result.append(p)
    return result


def _prompt(message: str, default: str | None = None) -> str | None:
    """Prompt the user for input, returning the default on empty entry.

    Returns None if the user presses Ctrl-C or Ctrl-D.
    """
    try:
        raw = input(message).strip()
    except (KeyboardInterrupt, EOFError):
        return None
    return raw if raw else default


def run_setup_wizard(repo: Path | None) -> None:
    """Interactive wizard to configure git-smart-commit via git config.

    Prompts for API backend, model, API key, and config scope (global / local).
    Writes settings with ``git config``.

    Args:
        repo: Path to a git repository (required for local scope).
              May be None when only global scope is needed.
    """
    print()
    print("git-smart-commit setup wizard")
    print("=" * 40)
    print()

    # ── Step 1: choose backend ────────────────────────────────────────────────
    print("Select API backend:")
    for i, b in enumerate(SETUP_BACKENDS, 1):
        marker = "  ← default" if i == 1 else ""
        print(f"  [{i}] {b['label']}{marker}")
    print()

    backend_choice = _prompt("Choice [1]: ", default="1")
    if backend_choice is None:
        print("\nSetup cancelled.")
        return
    try:
        backend_idx = int(backend_choice) - 1
        if not 0 <= backend_idx < len(SETUP_BACKENDS):
            raise ValueError
    except ValueError:
        print("Invalid choice. Setup cancelled.")
        return

    backend = SETUP_BACKENDS[backend_idx]
    print()

    # ── Step 2: API base URL (custom only) ───────────────────────────────────
    api_base: str
    if backend["api_base"] is None:
        val = _prompt("API base URL: ")
        if not val:
            print("URL required. Setup cancelled.")
            return
        api_base = val.rstrip("/")
    else:
        api_base = backend["api_base"]

    # ── Step 3: model ─────────────────────────────────────────────────────────
    default_model: str = backend["default_model"] or ""
    model_prompt = f"Model [{default_model}]: " if default_model else "Model: "
    model_val = _prompt(model_prompt, default=default_model)
    if model_val is None:
        print("\nSetup cancelled.")
        return
    if not model_val:
        print("Model name required. Setup cancelled.")
        return
    model = model_val

    # ── Step 4: API key ───────────────────────────────────────────────────────
    api_key: str | None = None
    needs_key: bool | None = backend["needs_key"]
    if needs_key is True:
        try:
            api_key = getpass.getpass("API key (input hidden): ").strip() or None
        except (KeyboardInterrupt, EOFError):
            print("\nSetup cancelled.")
            return
        if not api_key:
            print("API key required for this backend. Setup cancelled.")
            return
    elif needs_key is None:
        # custom endpoint — key is optional
        try:
            raw_key = getpass.getpass("API key (leave empty if not needed, input hidden): ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nSetup cancelled.")
            return
        api_key = raw_key or None

    print()

    # ── Step 5: config scope ──────────────────────────────────────────────────
    print("Save configuration to:")
    print("  [1] Global  (~/.gitconfig)  ← affects all repos  [default]")
    if repo is not None:
        print("  [2] Local   (.git/config)   ← this repo only")
    print()

    scope_choice = _prompt("Choice [1]: ", default="1")
    if scope_choice is None:
        print("\nSetup cancelled.")
        return

    scope_flag: str
    if scope_choice == "2" and repo is not None:
        scope_flag = "--local"
        scope_label = f"local ({repo})"
    else:
        scope_flag = "--global"
        scope_label = "global (~/.gitconfig)"

    print()

    # ── Step 6: write config ──────────────────────────────────────────────────
    cwd = repo if repo is not None else Path.cwd()

    settings: list[tuple[str, str]] = [
        ("smart-commit.api-base", api_base),
        ("smart-commit.model", model),
    ]
    if api_key:
        settings.append(("smart-commit.api-key", api_key))

    print(f"Writing {scope_label} config...")
    for key, value in settings:
        cmd = ["git", "config", scope_flag, key, value]
        result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"  Error setting {key}: {result.stderr.strip()}", file=sys.stderr)
            sys.exit(1)
        # Redact the API key in output
        display_value = "***" if "api-key" in key else value
        print(f"  git config {scope_flag} {key} {display_value}")

    print()
    print("Done! Run git-smart-commit to use your new settings.")
    if api_key:
        print()
        print("Tip: your API key is stored in plaintext in git config.")
        print("     Consider using the LLM_API_KEY environment variable instead.")
    print()


def _is_binary_diff(diff_text: str) -> bool:
    """Return True if the diff text represents a binary file change."""
    for line in diff_text.splitlines():
        if line.startswith("Binary files ") and line.endswith(" differ"):
            return True
        if line.startswith("GIT binary patch"):
            return True
    return False


def split_hunks(diff_text: str) -> list[str]:
    """Split a unified diff into individual hunks (each starting with @@)."""
    lines = diff_text.splitlines(keepends=True)
    hunks: list[str] = []
    header_lines: list[str] = []
    current: list[str] = []

    for line in lines:
        if line.startswith("@@"):
            if current:
                hunks.append("".join(current))
            current = header_lines + [line]
        elif line.startswith(("diff --git", "index ", "--- ", "+++ ")):
            header_lines.append(line)
            if current:
                # flush any open hunk first
                hunks.append("".join(current))
                current = []
        else:
            if current:
                current.append(line)
            # lines before the first @@ go into header_lines
            elif not line.startswith("@@"):
                header_lines.append(line)

    if current:
        hunks.append("".join(current))

    return hunks or [diff_text]   # fallback: treat whole diff as one hunk


# ── Tool definitions ───────────────────────────────────────────────────────────

class Issue(BaseModel):
    message: str = Field(description="REQUIRED. A brief description of a code issue, including a line number")
    path: str = Field(description="REQUIRED. The path affected by the issue")

class Commit(BaseModel):
    subject: str = Field(description="REQUIRED. Conventional commit subject line (type(scope): desc), under 72 chars. Add '!' after the type when a breaking change is present (e.g. 'feat!(api): rename endpoint').")
    hunks: list[str] = Field(description="REQUIRED. Hunk IDs to include in this commit, e.g. ['src/main.py::1', 'src/main.py::3']. Use ONLY the exact hunk IDs from the Available hunks list.")
    body: str = Field(default="", description="4-10 line plain-text commit body, no markdown")
    breaking_change: str = Field(default="", description="If this commit removes or renames a public API, changes a function signature incompatibly, or deletes an exported symbol, describe what broke and how callers must migrate. Leave empty if there is no breaking change.")
    issues: list[Issue] = Field(default_factory=list, description="Issues noticed in the code being committed")

    def post_process(self):
        self.body = wrap_markdown(self.body, 80)
        self.breaking_change = wrap_markdown(self.breaking_change, 80)
        return self

class ProposeCommitsArgs(BaseModel):
    commits: list[Commit] = Field(description="REQUIRED. Logical commit groups")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commits = [commit.post_process() for commit in self.commits]
        return self

class EmitCommitArgs(BaseModel):
    commit: Commit = Field(description="REQUIRED. A single logical commit to emit")

    def post_process(self):
        self.commit = self.commit.post_process()
        return self

class FinalizeCommitsArgs(BaseModel):
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        return self

sample_issue = Issue(
    message="Unused import 'os' on line 1",
    path="main.py"
)

sample_commit = Commit(
    subject="refactor(main): cleanup imports",
    hunks=["main.py::1"],
    body="Removed unnecessary imports to improve load time and code cleanliness.",
    issues=[sample_issue]
)

SAMPLE_OUTPUT = ProposeCommitsArgs(
    commits=[sample_commit],
    gitignore=["*.pyc", "__pycache__/"]
)


@tool(ProposeCommitsArgs)
# DEPRECATED: replaced by emit_commit + finalize_commits. Kept for Commit model and SAMPLE_OUTPUT references.
def propose_commits(result: ProposeCommitsArgs, analyzer: "GitAnalyzer") -> tuple[list[dict], list[str]]:
    """Group working-tree changes into logical commits and suggest .gitignore patterns for junk files."""
    commits = [c.model_dump() for c in result.commits]
    if not commits:
        raise ValueError("Model returned no valid commit groups.")

    # Mend unknown hunk IDs: try to match by file path, then warn/reject
    hunk_map = getattr(analyzer, '_hunk_map', None)
    if hunk_map is not None:
        valid_ids = set(hunk_map.keys())
        # Build file→hunks index for fuzzy matching
        file_to_hunks: dict[str, list[str]] = {}
        for hid in valid_ids:
            fpath = hid.rsplit("::", 1)[0] if "::" in hid else hid
            file_to_hunks.setdefault(fpath, []).append(hid)

        mended: dict[str, str] = {}
        for c in commits:
            for hid in c.get("hunks", []):
                if hid in valid_ids or hid in mended:
                    continue
                # Try to find the intended hunk by file path
                if "::" in hid:
                    base_path = hid.rsplit("::", 1)[0]
                    candidates = file_to_hunks.get(base_path, [])
                    if len(candidates) == 1:
                        mended[hid] = candidates[0]
                    elif candidates:
                        # Multiple hunks for file — try closest index
                        try:
                            idx = int(hid.rsplit("::", 1)[1])
                            closest = min(candidates,
                                          key=lambda k: abs(int(k.rsplit("::", 1)[1]) - idx))
                            mended[hid] = closest
                        except (ValueError, IndexError):
                            pass

        # Apply corrections
        if mended:
            for old, new in sorted(mended.items()):
                _log(f"  {_ansi('~', _ANSI_DIM)} mended hunk ID {old!r} → {new!r}")
            for c in commits:
                c["hunks"] = [mended.get(h, h) for h in c.get("hunks", [])]

        # Re-check: reject only if EVERY referenced hunk is still invalid
        all_referenced: set[str] = set()
        for c in commits:
            all_referenced.update(c.get("hunks", []))
        still_unknown = all_referenced - valid_ids
        still_valid = all_referenced & valid_ids
        if still_unknown and not still_valid:
            raise ValueError(
                f"ALL hunk IDs are invalid: {sorted(still_unknown)}. "
                f"Valid IDs are: {sorted(valid_ids)}"
            )
        if still_unknown:
            _log(f"  {_ansi('!', _ANSI_DIM)} {len(still_unknown)} unknown hunk(s) "
                 f"will be skipped: {sorted(still_unknown)}")

    commits = analyzer.merge_overlapping_commits(commits)
    return commits, result.gitignore


class MergeCommitsArgs(BaseModel):
    commit: Commit = Field(description="REQUIRED. A single merged commit covering all the provided changes")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commit = self.commit.post_process()
        return self


@tool(MergeCommitsArgs)
def merge_commits(result: MergeCommitsArgs, all_hunks: list[str], all_issues: list[dict]) -> dict:
    """Merge multiple overlapping commits into a single coherent commit."""
    commit = result.commit.model_dump()
    # Ensure all hunks and issues are preserved regardless of what the model returns
    commit["hunks"] = all_hunks
    commit["issues"] = all_issues
    return commit


class ReadFileArgs(BaseModel):
    path: str = Field(description="Repository-relative path to the file to read")

    def post_process(self): return self


@tool(ReadFileArgs)
def read_file(args: ReadFileArgs, analyzer: "GitAnalyzer") -> str:
    """Read the full current content of a file in the repository.

    Use this when the diff alone lacks context — e.g. to see the surrounding
    class structure, existing imports, or unchanged code near a changed line.
    The result is truncated at 50,000 characters for very large files.
    """
    return analyzer._read_file(args.path)


class GetDiffArgs(BaseModel):
    path: str = Field(description="Repository-relative path of the file")
    staged: bool | None = Field(
        default=None,
        description="true=staged only, false=unstaged only, null/omit=both",
    )

    def post_process(self): return self


@tool(GetDiffArgs)
def get_diff(result: GetDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Get the raw unified diff for a specific file (staged, unstaged, or both)."""
    parts = []

    def _fetch(extra_args: list[str], label: str) -> None:
        try:
            diff = analyzer.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", result.path])
            if diff.strip():
                parts.append(f"=== {label} ===\n{diff}" if result.staged is None else diff)
        except RuntimeError as e:
            parts.append(f"Error ({label}): {e}")

    if result.staged is None:
        _fetch(["--cached"], "staged")
        _fetch([], "unstaged")
    elif result.staged:
        _fetch(["--cached"], "staged")
    else:
        _fetch([], "unstaged")

    result_text = "\n".join(parts) if parts else f"No diff found for {result.path}"

    if len(result_text) > DIFF_CHARS_PER_FILE:
        result_text = result_text[:DIFF_CHARS_PER_FILE] + f"\n\n... (truncated at {DIFF_CHARS_PER_FILE} chars)"

    return result_text


class GetGitLogArgs(BaseModel):
    n: int = Field(default=20, description="Number of recent commits to return (max 50)")

    def post_process(self): return self


@tool(GetGitLogArgs)
def get_git_log(result: GetGitLogArgs, analyzer: "GitAnalyzer") -> str:
    """Get recent commit history to understand this project's commit style conventions."""
    n = min(result.n, 50)
    try:
        log = analyzer.git(["log", f"--max-count={n}", "--oneline", "--no-color"])
        return log.strip() or "No commits found."
    except RuntimeError as e:
        return f"Error: {e}"


class SearchDiffArgs(BaseModel):
    pattern: str = Field(description="Python regex pattern to search for across all diffs")
    context_lines: int = Field(default=2, description="Lines of context around each match")

    def post_process(self): return self


@tool(SearchDiffArgs)
def search_diff(result: SearchDiffArgs, analyzer: "GitAnalyzer") -> str:
    """Search for a regex pattern across all staged and unstaged diffs."""
    try:
        compiled = re.compile(result.pattern)
    except re.error as e:
        return f"Invalid regex: {e}"

    matches: list[dict] = []
    MAX_MATCHES = 50

    for extra_args in (["--cached"], []):
        if len(matches) >= MAX_MATCHES:
            break
        try:
            changed = analyzer.git(
                ["diff"] + extra_args + ["--name-only", "--no-ext-diff"]
            ).splitlines()
            for fname in changed:
                if len(matches) >= MAX_MATCHES:
                    break
                diff = analyzer.git(
                    ["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname]
                )
                lines = diff.splitlines()
                for i, line in enumerate(lines):
                    if compiled.search(line):
                        start = max(0, i - result.context_lines)
                        end = min(len(lines), i + result.context_lines + 1)
                        matches.append({
                            "file": fname,
                            "line_number": i + 1,
                            "match_context": "\n".join(lines[start:end]),
                        })
                        if len(matches) >= MAX_MATCHES:
                            break
        except RuntimeError:
            pass

    if not matches:
        return f"No matches found for pattern: {result.pattern!r}"

    result_lines = [
        f"File: {m['file']} (line {m['line_number']})\n{m['match_context']}"
        for m in matches
    ]
    text = "\n---\n".join(result_lines)
    if len(matches) >= MAX_MATCHES:
        text += f"\n... (limited to {MAX_MATCHES} matches)"
    return text


class QueryResultArgs(BaseModel):
    result_id: str = Field(
        description="The result_id shown in the summarized tool output (e.g. 'r5')")
    question: str = Field(
        description="Specific question to answer from the full stored result")

    def post_process(self): return self


MERGE_PROMPT = textwrap.dedent("""\
    You are merging multiple proposed commits that share the same diff hunks
    and therefore cannot be committed separately.

    Combine them into a SINGLE commit that:
    - Has one conventional commit subject line (type(scope): desc), under 72 chars
    - Has a body (4-10 lines) summarizing ALL the changes coherently
    - Lists ALL affected hunks (deduped)
    - Collects ALL issues (deduped)
    - If any commit is a feat type, it must take priority over the others (although the others should be mentioned in the commit body)
    - chore should only be used if no other category applies.
    - If any input commit has a non-empty breaking_change, set breaking_change on the merged commit to a combined description and add '!' to the type.

    Do NOT just concatenate the subjects. Write a new, coherent subject and body
    that covers the full set of changes as a single logical unit.

    Call the merge_commits tool with your answer.

    Here are the commits to merge:
""")


# ── GitAnalyzer ────────────────────────────────────────────────────────────────

class GitAnalyzer:
    """Encapsulates git operations and commit analysis for a repository."""

    def __init__(self, repo: Path, client: LLMClient):
        self.repo = repo
        self.client = client

    @property
    def config(self) -> ApiConfig:
        return self.client.config

    def git(self, args: list[str]) -> str:
        return _git(args, self.repo)

    def check_git_state(self) -> None:
        """Detect in-progress merge, rebase, or cherry-pick and abort early."""
        git_dir = self.repo / ".git"
        checks = [
            (git_dir / "MERGE_HEAD", "merge"),
            (git_dir / "REBASE_HEAD", "rebase"),
            (git_dir / "rebase-merge", "rebase"),
            (git_dir / "rebase-apply", "rebase"),
            (git_dir / "CHERRY_PICK_HEAD", "cherry-pick"),
        ]
        for path, operation in checks:
            if path.exists():
                print(
                    f"Error: a {operation} is currently in progress. "
                    f"Please complete or abort it before running git-smart-commit.",
                    file=sys.stderr,
                )
                sys.exit(1)

    def get_changed_files(self) -> list[str]:
        """Return list of files with unstaged or staged changes."""
        staged = self.git(["diff", "--cached", "--name-only"]).splitlines()
        unstaged = self.git(["diff", "--name-only"]).splitlines()
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        seen = set()
        files = []
        for f in staged + unstaged + untracked:
            if f and f not in seen:
                seen.add(f)
                files.append(f)
        return files

    def collect_hunks(self) -> dict[str, DiffHunk]:
        """Collect all diff hunks from the working tree, indexed by hunk ID.

        Any currently staged changes are first unstaged (git reset HEAD) so that
        everything is treated uniformly as unstaged working-tree changes.  This is
        safe: working-tree content is never modified.

        Returns a dict mapping hunk_id → DiffHunk.  Hunk IDs for unstaged changes
        look like "path/file.py#1"; untracked new files use "path/file.py#1".
        """
        hunk_map: dict[str, DiffHunk] = {}

        # Unstage any staged files so we work uniformly with unstaged diffs
        staged_files = self.git(["diff", "--cached", "--name-only"]).splitlines()
        if staged_files:
            self.git(["reset", "HEAD", "--"] + staged_files)

        # Split each unstaged file's diff into individual hunks
        changed = self.git(["diff", "--name-only", "--no-ext-diff"]).splitlines()
        for fpath in changed:
            diff = self.git(["diff", "--no-color", "--no-ext-diff", "--", fpath])
            if not diff.strip():
                continue
            # Detect binary files: create a single hunk with is_binary=True
            if _is_binary_diff(diff):
                hunk_id = f"{fpath}#1"
                hunk_map[hunk_id] = DiffHunk(
                    hunk_id=hunk_id,
                    file_path=fpath,
                    patch="",
                    is_binary=True,
                )
                continue
            file_hunks = split_hunks(diff)
            for i, patch in enumerate(file_hunks, 1):
                hunk_id = f"{fpath}#{i}"
                hunk_map[hunk_id] = DiffHunk(
                    hunk_id=hunk_id,
                    file_path=fpath,
                    patch=patch,
                )

        # Untracked (new) files are staged whole via git add — use #1 suffix
        # (consistent with tracked-file 1-based indexing so LLMs don't mis-reference)
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        for fpath in untracked:
            hunk_id = f"{fpath}#1"
            hunk_map[hunk_id] = DiffHunk(
                hunk_id=hunk_id,
                file_path=fpath,
                patch="",
                is_untracked=True,
            )

        return hunk_map

    def build_hunk_context(self, hunk_map: dict[str, DiffHunk]) -> str:
        """Build the initial LLM message: hunk list with inline diff content.

        Small hunks are included verbatim; very large ones are summarized.
        Total output is capped at DIFF_TOTAL_CHARS.
        """
        parts: list[str] = []
        total_chars = 0

        # Header: compact hunk list
        hunk_list_lines = []
        for hunk_id, dh in hunk_map.items():
            if dh.is_untracked:
                hunk_list_lines.append(f"  {hunk_id}  (new untracked file)")
            elif dh.is_binary:
                hunk_list_lines.append(f"  {hunk_id}  [binary file]")
            else:
                adds = dh.patch.count("\n+") - dh.patch.count("\n+++")
                dels = dh.patch.count("\n-") - dh.patch.count("\n---")
                hunk_list_lines.append(f"  {hunk_id}  (+{adds}/-{dels})")
        parts.append("Available hunks:\n" + "\n".join(hunk_list_lines))

        # Inline diff content for each hunk
        diff_parts: list[str] = []
        for hunk_id, dh in hunk_map.items():
            if dh.is_untracked:
                diff_parts.append(f"--- {hunk_id} ---\n(new untracked file — no diff)")
                total_chars += 50
            elif dh.is_binary:
                diff_parts.append(f"--- {hunk_id} ---\n[binary file]")
                total_chars += 30
            elif len(dh.patch) <= DIFF_CHARS_PER_FILE:
                diff_parts.append(f"--- {hunk_id} ---\n{dh.patch}")
                total_chars += len(dh.patch)
            else:
                _log(f"  {_ansi('⋯', _ANSI_DIM)} summarizing large hunk: {hunk_id} ({len(dh.patch):,} chars)")
                summary = self._summarize_file_diff(dh.file_path, dh.patch)
                diff_parts.append(f"--- {hunk_id} ---\n{summary}")
                total_chars += len(summary)

            if total_chars >= DIFF_TOTAL_CHARS:
                diff_parts.append("\n... (remaining hunks omitted — total size limit reached)")
                break

        parts.append("\nDiff content:\n" + "\n\n".join(diff_parts))
        return "\n\n".join(parts)

    def build_file_metadata(self) -> str:
        """Return a compact stats-only summary: diff --stat for staged/unstaged + untracked list.

        No full diffs — the LLM requests specific diffs lazily via get_diff.
        """
        parts: list[str] = []
        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if stat:
                parts.append(f"{label}:\n{stat}")
        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))
        return "\n\n".join(parts)

    def _summarize_file_diff(self, fname: str, diff_text: str) -> str:
        """Ask the model to summarize each hunk, passing the last N summaries as context."""
        hunks = split_hunks(diff_text)
        hunk_summaries: list[str] = []

        for i, hunk in enumerate(hunks, 1):
            prior_context = ""
            if hunk_summaries:
                window = hunk_summaries[-PRIOR_HUNK_WINDOW:]
                start_idx = i - len(window)
                prior_context = "Previous hunks in this file:\n" + "\n".join(
                    f"  Hunk {start_idx + j}: {s}" for j, s in enumerate(window)
                ) + "\n\n"

            prompt = textwrap.dedent(f"""\
                Summarize hunk {i} of {len(hunks)} from '{fname}' in 1-3 plain English sentences.
                Focus on WHAT changed. Preserve verbatim any code that looks buggy or suspicious
                (wrong arguments, misused APIs, incorrect syntax). Output only the summary.

                {prior_context}Hunk {i}:
                {hunk}
            """)

            summary = self.client.call(prompt)
            hunk_summaries.append(summary.strip())

        print()
        return f"[summarized — {len(hunks)} hunk(s)]\n" + "\n".join(
            f"  Hunk {i}: {s}" for i, s in enumerate(hunk_summaries, 1)
        )

    def build_diff_summary(self) -> str:
        """Return diff context for the classifier.

        Small files: include raw diff.
        Large files: summarize each hunk via a fast model call, then include summaries.
        """
        parts: list[str] = []
        total_chars = 0

        for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
            stat = self.git(["diff"] + extra_args + ["--stat", "--no-color"]).strip()
            if not stat:
                continue
            parts.append(f"{label}:\n{stat}\n")

            changed = self.git(["diff"] + extra_args + ["--name-only", "--no-ext-diff"]).splitlines()
            for fname in changed:
                file_diff = self.git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname])

                if len(file_diff) <= DIFF_CHARS_PER_FILE:
                    content = file_diff
                else:
                    _log(f"  {_ansi('⋯', _ANSI_DIM)} summarizing large diff: {fname} ({len(file_diff):,} chars)")
                    content = self._summarize_file_diff(fname, file_diff)

                parts.append(content)
                total_chars += len(content)

                if total_chars >= DIFF_TOTAL_CHARS:
                    parts.append("\n... (remaining diffs omitted — total size limit reached)")
                    break
            else:
                continue
            break   # hit total limit inside inner loop

        untracked = self.git(["ls-files", "--others", "--exclude-standard"]).splitlines()
        if untracked:
            parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))

        return "\n".join(parts)

    def _read_file(self, path: str) -> str:
        """Read a repo file, rejecting path traversal and truncating large files."""
        resolved_repo = self.repo.resolve()
        target = (self.repo / path).resolve()
        if target == resolved_repo or not str(target).startswith(str(resolved_repo) + os.sep):
            return f"[error: path traversal rejected for '{path}']"
        if not target.exists():
            return f"[file not found: {path}]"
        try:
            content = target.read_text(errors="replace")
        except OSError as e:
            return f"[error reading {path}: {e}]"
        if len(content) > READ_FILE_LIMIT:
            size = target.stat().st_size
            return (
                content[:READ_FILE_LIMIT]
                + f"\n\n[truncated — file is {size:,} bytes; showing first {READ_FILE_LIMIT:,} chars]"
            )
        return content

    def classify_changes(
        self,
        hunk_map: dict[str, DiffHunk],
        critique: str = "",
        issue_patterns: list[str] | None = None,
    ) -> tuple[list[dict], list[str]]:
        """Ask the model to group diff hunks into logical commits via an agentic loop.

        Returns (commits, gitignore_patterns).
        """
        self._hunk_map = hunk_map   # expose for validation in propose_commits
        hunk_context = self.build_hunk_context(hunk_map)

        # ── Local state for incremental commit emission ────────────────────────
        emitted_commits: list[dict] = []
        emitted_hunks: set[str] = set()

        # ── emit_commit: closure over emitted_commits/emitted_hunks/hunk_map ──
        @tool(EmitCommitArgs)
        def emit_commit(args: EmitCommitArgs, **_: Any) -> str:
            """Emit a single logical commit. Call once per commit as you identify groupings.

            Validates hunk IDs, prevents duplicates, and returns remaining
            unassigned hunks so you can plan the next commit.
            """
            # Validate hunk IDs against known hunks
            for hid in args.commit.hunks:
                if hid not in hunk_map:
                    return (
                        f"Error: unknown hunk ID {hid!r}. "
                        f"Valid IDs are: {sorted(hunk_map.keys())}"
                    )

            # Check for hunks already emitted in a previous commit
            for hid in args.commit.hunks:
                if hid in emitted_hunks:
                    return (
                        f"Error: hunk {hid!r} was already assigned to a previous commit. "
                        f"Each hunk can only appear in one commit."
                    )

            args.post_process()
            commit_dict = args.commit.model_dump()
            emitted_commits.append(commit_dict)
            emitted_hunks.update(args.commit.hunks)

            remaining = set(hunk_map.keys()) - emitted_hunks
            return f"Commit emitted: '{args.commit.subject}'. Remaining unassigned hunks: {sorted(remaining)}"

        # ── finalize_commits: terminal tool closure ───────────────────────────
        @tool(FinalizeCommitsArgs)
        def finalize_commits(args: FinalizeCommitsArgs, **_: Any) -> tuple:
            """Finalize all emitted commits. Call after emitting all commits via emit_commit."""
            if not emitted_commits:
                raise ValueError("No commits have been emitted yet. Use emit_commit first.")
            args.post_process()
            merged = self.merge_overlapping_commits(emitted_commits)
            return (merged, args.gitignore)

        critique_section = ""
        if critique:
            critique_section = (
                f"\n\nA reviewer rated the previous proposal below 7/10. Their feedback:\n"
                f"{critique}\n"
                f"Revise your commit groupings to address this feedback."
            )

        initial_message = hunk_context + critique_section

        tool_registry = {
            "read_file": read_file,
            "get_diff": get_diff,
            "get_git_log": get_git_log,
            "search_diff": search_diff,
            "emit_commit": emit_commit,
            "propose_commits": propose_commits,
        }

        # Build system prompt, appending any project-specific issue patterns
        system_prompt = AGENTIC_SYSTEM_PROMPT
        if issue_patterns:
            bullet_list = "\n".join(f"    - {p}" for p in issue_patterns)
            system_prompt = (
                system_prompt
                + f"\n\n    Project-specific patterns to watch for (flag as issues if found):\n"
                + bullet_list
            )

        # Scale turn limit with changeset size: allow ~2 extra turns per 5 hunks,
        # capped at MAX_AGENTIC_TURNS_CAP to avoid runaway loops.
        n_hunks = len(hunk_map)
        max_turns = min(MAX_AGENTIC_TURNS + (n_hunks // 5) * 2, MAX_AGENTIC_TURNS_CAP)
        if max_turns > MAX_AGENTIC_TURNS:
            _log(f"  {_ansi(f'[{n_hunks} hunks — up to {max_turns} turns]', _ANSI_DIM)}")

        # ── countdown closure: context-aware warning based on emission state ──
        all_hunk_ids = set(hunk_map.keys())

        def _countdown_message(turns_remaining: int) -> str:
            if not emitted_commits:
                return (
                    f"WARNING: Only {turns_remaining} turn(s) remaining. "
                    f"You are running low on turns. Start emitting commits "
                    f"with emit_commit, then call finalize_commits."
                )
            remaining_hunks = all_hunk_ids - emitted_hunks
            if remaining_hunks:
                return (
                    f"WARNING: Only {turns_remaining} turn(s) remaining. "
                    f"{len(remaining_hunks)} hunks still unassigned. "
                    f"Emit remaining commits or call finalize_commits NOW."
                )
            return (
                f"WARNING: Only {turns_remaining} turn(s) remaining. "
                f"All hunks assigned. Call finalize_commits NOW."
            )

        return self.client.agentic_loop(
            system_prompt=system_prompt,
            initial_user_message=initial_message,
            tool_registry=tool_registry,
            terminal_tool=finalize_commits,
            max_turns=max_turns,
            countdown_message_fn=_countdown_message,
            analyzer=self,
        )

    def merge_overlapping_commits(self, commits: list[dict]) -> list[dict]:
        """Merge commits that share hunk IDs (duplicate assignment is a model error).

        Different commits CAN touch different hunks of the same file — that is the
        whole point of hunk-level staging.  We only need to merge when the same
        hunk_id appears in more than one commit, since applying the same patch twice
        would fail.
        """
        if len(commits) <= 1:
            return commits

        # Build a mapping from each hunk_id to the commit indices that claim it
        hunk_to_indices: dict[str, list[int]] = {}
        for i, c in enumerate(commits):
            for h in c.get("hunks", []):
                hunk_to_indices.setdefault(h, []).append(i)

        # Union-find to group commits sharing a hunk
        parent = list(range(len(commits)))

        def find(x):
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a, b):
            ra, rb = find(a), find(b)
            if ra != rb:
                parent[rb] = ra

        for indices in hunk_to_indices.values():
            for idx in indices[1:]:
                union(indices[0], idx)

        # Group commits by their root
        groups: dict[int, list[int]] = {}
        for i in range(len(commits)):
            groups.setdefault(find(i), []).append(i)

        # If no merging needed, return as-is
        if all(len(idxs) == 1 for idxs in groups.values()):
            return commits

        merged = []
        for indices in groups.values():
            if len(indices) == 1:
                merged.append(commits[indices[0]])
                continue

            group = [commits[i] for i in indices]
            _log(f"  {_ansi('⋯', _ANSI_DIM)} merging {len(group)} commits with overlapping hunks")

            all_hunks = list(dict.fromkeys(h for c in group for h in c.get("hunks", [])))
            all_issues = []
            seen_issues: set[tuple] = set()
            for c in group:
                for issue in c.get("issues", []):
                    key = (issue.get("path", ""), issue.get("message", ""))
                    if key not in seen_issues:
                        seen_issues.add(key)
                        all_issues.append(issue)

            commits_desc = json.dumps(group, indent=2)
            prompt = MERGE_PROMPT + commits_desc

            merged.append(self.client.call_with_tool(
                prompt, merge_commits,
                all_hunks=all_hunks, all_issues=all_issues,
            ))

        return merged

    def _build_combined_patch(self, hunks: list[str]) -> str:
        """Combine multiple hunks for the same file into a single valid patch.

        The first hunk's file header is kept; subsequent hunks contribute only
        their @@ sections onward, so git apply sees one coherent multi-hunk patch.
        """
        if len(hunks) == 1:
            return hunks[0]

        result: list[str] = []
        # Keep the full first hunk (file header + @@ lines)
        result.append(hunks[0])
        if not result[-1].endswith("\n"):
            result.append("\n")

        for hunk in hunks[1:]:
            # Skip file-header lines; start from the first @@ line
            in_hunk = False
            for line in hunk.splitlines(keepends=True):
                if line.startswith("@@"):
                    in_hunk = True
                if in_hunk:
                    result.append(line)
            if result and not result[-1].endswith("\n"):
                result.append("\n")

        return "".join(result)

    def execute_commits(self, commits: list[dict], hunk_map: dict[str, DiffHunk]) -> None:
        """Stage and commit each group using git apply --cached for individual hunks."""
        for i, commit in enumerate(commits, 1):
            subject = commit["subject"]
            body = commit.get("body", "").strip()
            breaking = commit.get("breaking_change", "").strip()
            parts = [subject]
            if body:
                parts.append(body)
            trailers = []
            if breaking:
                trailers.append(f"BREAKING CHANGE: {breaking}")
            trailers.append(f"Co-Authored-By: fiddlerwoaroof/git-smart-commit ({self.config.model})")
            parts.append("\n".join(trailers))
            message = "\n\n".join(parts)
            hunk_ids: list[str] = commit.get("hunks", [])

            print(f"\n[{i}/{len(commits)}] {subject}")

            # Group hunk IDs by file path (preserving order)
            by_file: dict[str, list[DiffHunk]] = {}
            untracked_paths: list[str] = []
            untracked_seen: set[str] = set()
            binary_paths: list[str] = []
            binary_seen: set[str] = set()
            for hid in hunk_ids:
                dh = hunk_map.get(hid)
                if dh is None and "#" in hid:
                    # Fallback: LLM may have used wrong index for an untracked file
                    base_path = hid.rsplit("#", 1)[0]
                    fallback = f"{base_path}#1"
                    if fallback in hunk_map and hunk_map[fallback].is_untracked:
                        dh = hunk_map[fallback]
                        print(f"  ~ Remapped {hid!r} → {fallback!r} (untracked file)", file=sys.stderr)
                if dh is None:
                    print(f"  ! Unknown hunk {hid!r}, skipping", file=sys.stderr)
                    continue
                if dh.is_untracked:
                    if dh.file_path not in untracked_seen:
                        untracked_seen.add(dh.file_path)
                        untracked_paths.append(dh.file_path)
                elif dh.is_binary:
                    if dh.file_path not in binary_seen:
                        binary_seen.add(dh.file_path)
                        binary_paths.append(dh.file_path)
                else:
                    by_file.setdefault(dh.file_path, []).append(dh)

            # Apply hunks per file via git apply --cached
            for fpath, file_hunks in by_file.items():
                patch_text = self._build_combined_patch([dh.patch for dh in file_hunks])
                with tempfile.NamedTemporaryFile(
                    mode="w", suffix=".patch", delete=False, encoding="utf-8"
                ) as f:
                    f.write(patch_text)
                    patch_file = f.name
                try:
                    self.git(["apply", "--cached", patch_file])
                    n = len(file_hunks)
                    print(f"  + {fpath} ({n} hunk{'s' if n != 1 else ''})")
                except RuntimeError as e:
                    print(f"  ! Could not apply patch for {fpath}: {e}", file=sys.stderr)
                finally:
                    os.unlink(patch_file)

            # Add binary files whole (can't use git apply for binary content)
            for fpath in binary_paths:
                try:
                    self.git(["add", "--", fpath])
                    print(f"  + {fpath} (binary file)")
                except RuntimeError as e:
                    print(f"  ! Could not add {fpath}: {e}", file=sys.stderr)

            # Add untracked (new) files whole
            for fpath in untracked_paths:
                try:
                    self.git(["add", "--", fpath])
                    print(f"  + {fpath} (new file)")
                except RuntimeError as e:
                    print(f"  ! Could not add {fpath}: {e}", file=sys.stderr)

            # Check something is actually staged
            staged = self.git(["diff", "--cached", "--name-only"]).strip()
            if not staged:
                print("  (nothing staged, skipping)")
                continue

            try:
                self.git(["commit", "-m", message])
                print(f"  ✓ Committed")
            except RuntimeError as e:
                print(f"  ! Error committing: {e}", file=sys.stderr)
                raise

    def update_gitignore(self, patterns: list[str], yes: bool = False) -> None:
        """Offer to append suggested patterns to .gitignore, skipping any already present."""
        if not patterns:
            return

        gitignore_path = self.repo / ".gitignore"

        existing: set[str] = set()
        existing_content = ""
        if gitignore_path.exists():
            existing_content = gitignore_path.read_text()
            for line in existing_content.splitlines():
                stripped = line.strip()
                if stripped and not stripped.startswith("#"):
                    existing.add(stripped)

        new_patterns = [p for p in patterns if p not in existing]
        if not new_patterns:
            return

        print("\nSuggested .gitignore additions:")
        for p in new_patterns:
            print(f"  {p}")

        if not yes:
            try:
                answer = input("Add these to .gitignore? [y/N] ").strip().lower()
            except (KeyboardInterrupt, EOFError):
                return
            if answer not in ("y", "yes"):
                return

        with open(gitignore_path, "a") as f:
            if existing_content and not existing_content.endswith("\n"):
                f.write("\n")
            f.write("\n# Added by git-smart-commit\n")
            for p in new_patterns:
                f.write(f"{p}\n")

        print(f"  Updated .gitignore ({len(new_patterns)} new pattern(s)).")


# ── Terminal color helpers ──────────────────────────────────────────────────────

_ANSI_RESET  = "\033[0m"
_ANSI_BOLD   = "\033[1m"
_ANSI_DIM    = "\033[2m"
_ANSI_RED    = "\033[31m"
_ANSI_GREEN  = "\033[32m"
_ANSI_YELLOW = "\033[33m"
_ANSI_CYAN   = "\033[36m"


def _ansi(text: str, *codes: str) -> str:
    """Wrap text in ANSI escape codes when stdout is a TTY (no-op otherwise)."""
    if not sys.stdout.isatty():
        return text
    return "".join(codes) + text + _ANSI_RESET


def _diff_stats(patch: str) -> tuple[int, int]:
    """Count (added_lines, removed_lines) from a unified diff patch string."""
    added = removed = 0
    for line in patch.splitlines():
        if line.startswith("+") and not line.startswith("+++"):
            added += 1
        elif line.startswith("-") and not line.startswith("---"):
            removed += 1
    return added, removed


# ── Progress logging ────────────────────────────────────────────────────────────
#
# Progress/status messages go to stdout so they are visible in normal use
# (many shells redirect stderr to /dev/null). They are suppressed when
# _QUIET is True (set by --quiet or automatically when --json is active).

_QUIET: bool = False


def _log(msg: str) -> None:
    """Print a progress line to stdout unless quiet mode is active."""
    if not _QUIET:
        print(msg)


def _log_reasoning(text: str) -> None:
    """Print reasoning/thinking tokens dimmed and indented, unless quiet."""
    if _QUIET:
        return
    prefix = _ansi("  │ ", _ANSI_DIM)
    for line in text.splitlines():
        print(prefix + _ansi(line, _ANSI_DIM))


# ── Display helpers ────────────────────────────────────────────────────────────

def print_proposed_commits(commits: list[dict], gitignore: list[str],
                           hunk_map: dict[str, "DiffHunk"] | None = None) -> None:
    width = shutil.get_terminal_size((80, 24)).columns
    n = len(commits)
    print()
    for i, commit in enumerate(commits, 1):
        subject  = commit["subject"]
        body     = commit.get("body", "").strip()
        hunk_ids = commit.get("hunks", [])
        breaking = commit.get("breaking_change", "").strip()
        issues   = commit.get("issues") or []

        # ── box top ──────────────────────────────────────────────────────────
        label       = f"[{i}/{n}] "
        header_len  = len("┌─") + len(label) + len(subject) + 1
        fill        = max(1, width - header_len)
        print(
            "┌─"
            + _ansi(label, _ANSI_DIM)
            + _ansi(subject, _ANSI_BOLD)
            + " " + "─" * fill
        )

        # ── body ─────────────────────────────────────────────────────────────
        if body:
            print("│")
            for line in body.splitlines():
                print(f"│    {line}")

        # ── files ─────────────────────────────────────────────────────────────
        if hunk_ids:
            print("│")
            # Aggregate hunks per file (preserving order)
            by_file: dict[str, list] = {}
            for hid in hunk_ids:
                if hunk_map and hid in hunk_map:
                    dh = hunk_map[hid]
                    by_file.setdefault(dh.file_path, []).append(dh)
                else:
                    fpath = hid.rsplit("#", 1)[0] if "#" in hid else hid
                    by_file.setdefault(fpath, [])

            max_path = max((len(fp) for fp in by_file), default=0)
            for fpath, hunks in by_file.items():
                total_added = total_removed = 0
                is_new = any(getattr(dh, "is_untracked", False) for dh in hunks)
                for dh in hunks:
                    if dh is not None and dh.patch:
                        a, r = _diff_stats(dh.patch)
                        total_added   += a
                        total_removed += r

                if not hunks:
                    # No hunk data available — neutral display
                    print(f"│    {_ansi('~', _ANSI_YELLOW)} {fpath}")
                    continue

                if is_new:
                    marker, color = "+", _ANSI_GREEN
                elif total_added == 0 and total_removed > 0:
                    marker, color = "×", _ANSI_RED
                else:
                    marker, color = "~", _ANSI_YELLOW

                padding   = " " * (max_path - len(fpath))
                stats_str = _ansi(f"+{total_added} -{total_removed}", _ANSI_DIM)
                print(f"│    {_ansi(marker, color)} {_ansi(fpath, color)}{padding}  {stats_str}")

        # ── breaking change ───────────────────────────────────────────────────
        if breaking:
            print("│")
            print(f"│  {_ansi('⚠ BREAKING CHANGE:', _ANSI_RED + _ANSI_BOLD)} {breaking}")

        # ── issues ────────────────────────────────────────────────────────────
        if issues:
            print("│")
            print(f"│  {_ansi('Issues:', _ANSI_YELLOW)}")
            for issue in issues:
                print(f"│    - {issue['path']}: {issue['message']}")

        # ── box bottom ────────────────────────────────────────────────────────
        print("│")
        print("└" + "─" * (width - 1))
        print()

    if gitignore:
        print("  Suggested .gitignore patterns:")
        for pattern in gitignore:
            print(f"    {_ansi(pattern, _ANSI_DIM)}")
        print()


# ── Textual TUI ────────────────────────────────────────────────────────────────

@dataclass
class _Reclassify:
    """Signal returned by the TUI / text-mode to trigger a fresh classification."""
    critique: str


def _build_commit_markup(commit: dict, hunk_map: dict | None, index: int, total: int) -> str:
    """Build a Rich markup string for the commit detail panel."""
    from rich.markup import escape as esc

    subject = esc(commit["subject"])
    lines = [
        f"[dim bold][{index}/{total}][/dim bold]  [bold]{subject}[/bold]",
        "",
    ]

    body = commit.get("body", "").strip()
    if body:
        for line in body.splitlines():
            lines.append(f"  {esc(line)}")
        lines.append("")

    hunk_ids = commit.get("hunks", [])
    if hunk_ids:
        by_file: dict[str, list] = {}
        for hid in hunk_ids:
            if hunk_map and hid in hunk_map:
                dh = hunk_map[hid]
                by_file.setdefault(dh.file_path, []).append(dh)
            else:
                fpath = hid.rsplit("#", 1)[0] if "#" in hid else hid
                by_file.setdefault(fpath, [])

        max_path = max((len(fp) for fp in by_file), default=0)
        for fpath, hunks in by_file.items():
            total_added = total_removed = 0
            is_new = any(getattr(dh, "is_untracked", False) for dh in hunks)
            for dh in hunks:
                if dh is not None and dh.patch:
                    a, r = _diff_stats(dh.patch)
                    total_added += a
                    total_removed += r

            padding  = " " * (max_path - len(fpath))
            stats    = f"+{total_added} -{total_removed}"
            fp_esc   = esc(fpath)

            if not hunks:
                lines.append(f"  [yellow]~ {fp_esc}[/yellow]")
            elif is_new:
                lines.append(f"  [green]+ {fp_esc}[/green]{padding}  [dim]{stats}[/dim]")
            elif total_added == 0 and total_removed > 0:
                lines.append(f"  [red]× {fp_esc}[/red]{padding}  [dim]{stats}[/dim]")
            else:
                lines.append(f"  [yellow]~ {fp_esc}[/yellow]{padding}  [dim]{stats}[/dim]")
        lines.append("")

    breaking = commit.get("breaking_change", "").strip()
    if breaking:
        lines.append(f"  [bold red]⚠ BREAKING CHANGE:[/bold red] {esc(breaking)}")
        lines.append("")

    issues = commit.get("issues") or []
    if issues:
        lines.append(f"  [yellow]Issues found:[/yellow]")
        for issue in issues:
            path = esc(issue.get("path", ""))
            msg  = esc(issue.get("message", ""))
            lines.append(f"    [dim]·[/dim] [dim]{path}:[/dim] {msg}")
        lines.append("")

    return "\n".join(lines)


def _get_editor() -> str:
    """Return the preferred editor from environment variables."""
    return (
        os.environ.get("GIT_EDITOR")
        or os.environ.get("VISUAL")
        or os.environ.get("EDITOR")
        or "vi"
    )


def _open_editor_for_message(msg: str) -> str | None:
    """Open $EDITOR with *msg* and return the edited text, or None if unchanged/empty."""
    with tempfile.NamedTemporaryFile(suffix=".txt", mode="w", encoding="utf-8", delete=False) as f:
        f.write(msg + "\n")
        tmpfile = f.name
    try:
        subprocess.run([_get_editor(), tmpfile], check=False)
        with open(tmpfile, encoding="utf-8") as f:
            edited = f.read().strip()
        return edited if edited else None
    finally:
        try:
            os.unlink(tmpfile)
        except OSError:
            pass


def _merge_two_commits(commits: list[dict], a: int, b: int) -> list[dict]:
    """Merge commits at indices *a* and *b* into one (keeping *a*'s subject)."""
    if a == b or not (0 <= a < len(commits)) or not (0 <= b < len(commits)):
        return commits
    if a > b:
        a, b = b, a
    ca, cb = commits[a], commits[b]
    bodies = [c.get("body", "") for c in (ca, cb) if c.get("body", "").strip()]
    merged: dict = {
        "subject": ca["subject"],
        "body": "\n\n".join(bodies),
        "hunks": list(ca.get("hunks", [])) + list(cb.get("hunks", [])),
        "issues": list(ca.get("issues", [])) + list(cb.get("issues", [])),
    }
    if ca.get("breaking_change") or cb.get("breaking_change"):
        merged["breaking_change"] = ca.get("breaking_change") or cb.get("breaking_change")
    result = []
    for i, c in enumerate(commits):
        if i == a:
            result.append(merged)
        elif i == b:
            pass  # dropped — merged into a
        else:
            result.append(c)
    return result


def _apply_editor_result(commits: list[dict], idx: int, new_msg: str) -> list[dict]:
    """Parse *new_msg* (subject + optional body) back into *commits[idx]*."""
    if "\n\n" in new_msg:
        subject, _, body = new_msg.partition("\n\n")
    else:
        subject, body = new_msg, ""
    commits[idx]["subject"] = subject.strip()
    commits[idx]["body"] = body.strip()
    return commits


def _run_text_confirmation(
    commits: list[dict], gitignore: list[str], hunk_map: dict | None
) -> list[dict] | None:
    """Interactive text-mode confirmation loop (used when stdout is not a TTY)."""
    HELP = (
        "  y              — proceed with commits as shown\n"
        "  n / q          — cancel\n"
        "  e <N>          — open $EDITOR for commit N's full message\n"
        "  m <A> <B>      — merge commits A and B (keeps A's subject)\n"
        "  d <N>          — delete commit N (hunks remain unstaged)\n"
        "  r <critique>   — re-classify with critique and re-show plan\n"
        "  ?              — show this help\n"
    )
    while True:
        print_proposed_commits(commits, gitignore, hunk_map)
        print()
        print("Commands: y=proceed  n=cancel  e <N>=edit  m <A> <B>=merge  d <N>=delete  r <critique>=re-classify  ?=help")
        try:
            answer = input("> ").strip()
        except (KeyboardInterrupt, EOFError):
            print()
            return None

        if answer in ("y", "yes"):
            return commits

        if answer in ("n", "no", "q", "quit"):
            print("Cancelled.")
            return None

        if answer in ("?", "h", "help"):
            print(HELP)
            continue

        parts = answer.split()
        if parts and parts[0] == "e" and len(parts) == 2:
            try:
                n = int(parts[1]) - 1
            except ValueError:
                print("Usage: e <N>  (e.g. e 1)")
                continue
            if not (0 <= n < len(commits)):
                print(f"Commit {n + 1} does not exist (1–{len(commits)}).")
                continue
            c = commits[n]
            msg = c["subject"]
            if c.get("body"):
                msg += "\n\n" + c["body"]
            result = _open_editor_for_message(msg)
            if result:
                commits = _apply_editor_result(commits, n, result)
            continue

        if parts and parts[0] == "m" and len(parts) == 3:
            try:
                a, b = int(parts[1]) - 1, int(parts[2]) - 1
            except ValueError:
                print("Usage: m <A> <B>  (e.g. m 1 2)")
                continue
            if a == b:
                print("Cannot merge a commit with itself.")
                continue
            if not (0 <= a < len(commits)) or not (0 <= b < len(commits)):
                print(f"Commit numbers must be between 1 and {len(commits)}.")
                continue
            commits = _merge_two_commits(commits, a, b)
            print(f"Merged commits {a + 1} and {b + 1}.")
            continue

        if parts and parts[0] == "d" and len(parts) == 2:
            try:
                n = int(parts[1]) - 1
            except ValueError:
                print("Usage: d <N>  (e.g. d 2)")
                continue
            if len(commits) <= 1:
                print("Cannot delete the only commit.")
                continue
            if not (0 <= n < len(commits)):
                print(f"Commit {n + 1} does not exist (1–{len(commits)}).")
                continue
            commits.pop(n)
            print(f"Deleted commit {n + 1} (hunks remain unstaged).")
            continue

        if parts and parts[0] == "r":
            critique = " ".join(parts[1:]).strip()
            if not critique:
                print("Usage: r <critique>  (e.g. r put the test changes in a separate commit)")
                continue
            return _Reclassify(critique=critique)

        print("Unknown command — type ? for help.")


def _run_confirm_tui(
    commits: list[dict], gitignore: list[str], hunk_map: dict | None
) -> list[dict] | None:
    """Launch the interactive Textual TUI.

    Returns the (possibly edited) commit list if the user confirmed, or
    None if the user cancelled.
    """
    # Textual imports are lazy: no startup cost when TUI is not used.
    from textual.app import App, ComposeResult
    from textual.binding import Binding
    from textual.containers import Horizontal, Vertical
    from textual.screen import ModalScreen
    from textual.widgets import Footer, Header, Input, Label, ListItem, ListView, Static

    class _EditModal(ModalScreen[str | None]):
        """Modal dialog for editing a commit's subject line."""

        # Use ModalScreen selector so the CSS works regardless of class name
        # mangling that can occur with functions-scoped class definitions.
        DEFAULT_CSS = """
        _EditModal { align: center middle; }
        _EditModal > Vertical {
            width: 70; height: auto;
            border: double $accent; padding: 1 2; background: $surface;
        }
        """

        BINDINGS = [Binding("escape", "dismiss_none", "Cancel", priority=True)]

        def __init__(self, current: str) -> None:
            super().__init__()
            self._current = current

        def compose(self) -> ComposeResult:
            with Vertical():
                yield Label("Edit commit subject:")
                yield Input(value=self._current, id="subj-input")
                yield Label("[dim]Enter to save · Esc to cancel[/dim]", markup=True)

        def on_mount(self) -> None:
            self.query_one(Input).focus()

        def on_input_submitted(self, event: Input.Submitted) -> None:
            self.dismiss(event.value.strip() or None)

        def action_dismiss_none(self) -> None:
            self.dismiss(None)

    class _CritiqueModal(ModalScreen[str | None]):
        """Modal dialog for entering a re-classification critique."""

        DEFAULT_CSS = """
        _CritiqueModal { align: center middle; }
        _CritiqueModal > Vertical {
            width: 76; height: auto;
            border: double $warning; padding: 1 2; background: $surface;
        }
        """

        BINDINGS = [Binding("escape", "dismiss_none", "Cancel", priority=True)]

        def compose(self) -> ComposeResult:
            with Vertical():
                yield Label("Re-classify with critique:")
                yield Input(placeholder="e.g. put the test changes in a separate commit", id="critique-input")
                yield Label("[dim]Enter to submit · Esc to cancel[/dim]", markup=True)

        def on_mount(self) -> None:
            self.query_one(Input).focus()

        def on_input_submitted(self, event: Input.Submitted) -> None:
            self.dismiss(event.value.strip() or None)

        def action_dismiss_none(self) -> None:
            self.dismiss(None)

    class _App(App[list[dict] | _Reclassify | None]):
        """Two-panel TUI: commit list on the left, details on the right."""

        TITLE = "git-smart-commit — review proposed commits"

        CSS = """
        Screen   { layout: vertical; }
        #panels  { layout: horizontal; height: 1fr; }
        #sidebar { width: 34; border-right: solid $primary-darken-3; }
        ListView { height: 1fr; }
        ListView > ListItem { padding: 0 1; }
        #detail  { width: 1fr; padding: 1 2; overflow-y: auto; }
        """

        BINDINGS = [
            Binding("y", "confirm",        "Proceed",      priority=True),
            Binding("n", "cancel",         "Cancel",       priority=True),
            Binding("q", "cancel",         "Quit",         priority=True),
            Binding("e", "edit_subject",   "Edit subject"),
            Binding("E", "edit_message",   "Edit message ($EDITOR)"),
            Binding("m", "merge_next",     "Merge ↓"),
            Binding("d", "delete_commit",  "Delete"),
            Binding("r", "reclassify",     "Re-classify"),
            Binding("k", "cursor_up",      "",             show=False),
            Binding("j", "cursor_down",    "",             show=False),
        ]

        def __init__(
            self,
            commits: list[dict],
            gitignore_patterns: list[str],
            hunk_map: dict | None,
        ) -> None:
            super().__init__()
            self._commits   = [dict(c) for c in commits]
            self._gitignore = gitignore_patterns
            self._hunk_map  = hunk_map
            self._sel       = 0          # plain int — no reactive watcher needed
            self._list_gen  = 0          # bumped on each rebuild to avoid duplicate IDs

        # ── layout ───────────────────────────────────────────────────────────

        def compose(self) -> ComposeResult:
            yield Header(show_clock=False)
            with Horizontal(id="panels"):
                with Vertical(id="sidebar"):
                    with ListView(id="list"):
                        for i in range(len(self._commits)):
                            yield ListItem(Label(self._label(i)), id=f"ci-{self._list_gen}-{i}")
                # Initialise detail with the first commit so no on_mount needed.
                init = (
                    _build_commit_markup(
                        self._commits[0], self._hunk_map, 1, len(self._commits)
                    )
                    if self._commits else ""
                )
                yield Static(init, id="detail", markup=True)
            yield Footer()

        # ── helpers ──────────────────────────────────────────────────────────

        def _label(self, i: int) -> str:
            subj = self._commits[i]["subject"]
            if len(subj) > 27:
                subj = subj[:26] + "…"
            return f"{'▶' if i == self._sel else ' '} {i + 1}. {subj}"

        def _set_sel(self, idx: int) -> None:
            """Change selection: refresh both sidebar labels and the detail pane."""
            old, self._sel = self._sel, idx
            for i in (old, idx):
                try:
                    self.query_one(f"#ci-{self._list_gen}-{i}", ListItem).query_one(Label).update(
                        self._label(i)
                    )
                except Exception:
                    pass
            try:
                self.query_one("#detail", Static).update(
                    _build_commit_markup(
                        self._commits[idx], self._hunk_map, idx + 1, len(self._commits)
                    )
                )
            except Exception:
                pass

        # ── events ───────────────────────────────────────────────────────────

        def on_list_view_highlighted(self, event: ListView.Highlighted) -> None:
            if event.item is None:
                return
            try:
                idx = int(event.item.id.split("-")[-1])
            except Exception:
                return
            if idx != self._sel:
                self._set_sel(idx)

        # ── actions ──────────────────────────────────────────────────────────

        def action_cursor_up(self) -> None:
            try:
                self.query_one("#list", ListView).action_cursor_up()
            except Exception:
                pass

        def action_cursor_down(self) -> None:
            try:
                self.query_one("#list", ListView).action_cursor_down()
            except Exception:
                pass

        def action_confirm(self) -> None:
            self.exit(self._commits)

        def action_cancel(self) -> None:
            self.exit(None)

        def action_edit_subject(self) -> None:
            idx = self._sel

            def _apply(new_subject: str | None) -> None:
                if not new_subject:
                    return
                self._commits[idx]["subject"] = new_subject
                try:
                    self.query_one(f"#ci-{self._list_gen}-{idx}", ListItem).query_one(Label).update(
                        self._label(idx)
                    )
                    self.query_one("#detail", Static).update(
                        _build_commit_markup(
                            self._commits[idx], self._hunk_map,
                            idx + 1, len(self._commits),
                        )
                    )
                except Exception:
                    pass

            self.push_screen(_EditModal(self._commits[idx]["subject"]), _apply)

        def action_edit_message(self) -> None:
            """Open $EDITOR with the full commit message (subject + body)."""
            idx = self._sel
            commit = self._commits[idx]
            msg = commit["subject"]
            if commit.get("body"):
                msg += "\n\n" + commit["body"]
            with self.suspend():
                result = _open_editor_for_message(msg)
            if result:
                self._commits = _apply_editor_result(self._commits, idx, result)
                try:
                    self.query_one(f"#ci-{self._list_gen}-{idx}", ListItem).query_one(Label).update(
                        self._label(idx)
                    )
                    self.query_one("#detail", Static).update(
                        _build_commit_markup(
                            self._commits[idx], self._hunk_map,
                            idx + 1, len(self._commits),
                        )
                    )
                except Exception:
                    pass

        def _rebuild_list(self) -> None:
            """Repopulate the sidebar list after the commit list changes.

            lv.clear() schedules removal asynchronously, so we bump _list_gen
            to ensure newly mounted items always get fresh unique IDs.
            """
            self._list_gen += 1
            lv = self.query_one("#list", ListView)
            lv.clear()
            for i in range(len(self._commits)):
                lv.mount(ListItem(Label(self._label(i)), id=f"ci-{self._list_gen}-{i}"))

        def action_merge_next(self) -> None:
            """Merge the selected commit with the next one."""
            idx = self._sel
            if idx >= len(self._commits) - 1:
                self.notify("No next commit to merge with", severity="warning")
                return
            self._commits = _merge_two_commits(self._commits, idx, idx + 1)
            self._sel = min(idx, len(self._commits) - 1)
            self._rebuild_list()
            if self._commits:
                self.query_one("#detail", Static).update(
                    _build_commit_markup(
                        self._commits[self._sel], self._hunk_map,
                        self._sel + 1, len(self._commits),
                    )
                )

        def action_delete_commit(self) -> None:
            """Drop the selected commit (its hunks remain unstaged)."""
            if len(self._commits) <= 1:
                self.notify("Cannot delete the only commit", severity="warning")
                return
            idx = self._sel
            self._commits.pop(idx)
            self._sel = min(idx, len(self._commits) - 1)
            self._rebuild_list()
            if self._commits:
                self.query_one("#detail", Static).update(
                    _build_commit_markup(
                        self._commits[self._sel], self._hunk_map,
                        self._sel + 1, len(self._commits),
                    )
                )
            self.notify(f"Commit {idx + 1} deleted — hunks remain unstaged")

        def action_reclassify(self) -> None:
            """Prompt for a critique and re-run the LLM classifier."""
            def _submit(critique: str | None) -> None:
                if critique:
                    self.exit(_Reclassify(critique=critique))
            self.push_screen(_CritiqueModal(), _submit)

    return _App(commits, gitignore, hunk_map).run()


# ── Main ───────────────────────────────────────────────────────────────────────

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Group git changes into logical commits using Qwen3-Coder.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--repo", default=".", help="Path to git repository")
    parser.add_argument("--dry-run", action="store_true", help="Show proposed commits without executing")
    parser.add_argument("--model", default=None, help=f"Model name (default: {DEFAULT_MODEL}, or smart-commit.model in git config)")
    parser.add_argument("--yes", "-y", action="store_true", help="Commit without confirmation")
    parser.add_argument("--quiet", "-q", action="store_true", help="Suppress progress output (for scripting/CI)")
    parser.add_argument("--json", dest="json_out", action="store_true", help="Output JSON and exit")
    parser.add_argument(
        "--api-base", default=None,
        help="API base URL (default: http://localhost:11434 for Ollama). "
             "Use https://openrouter.ai/api/v1 for OpenRouter. "
             "Or set LLM_API_BASE env var.",
    )
    parser.add_argument(
        "--api-key", default=None,
        help="API key for authenticated endpoints (or set LLM_API_KEY env var)",
    )
    parser.add_argument(
        "--critique", default="",
        metavar="TEXT",
        help="Agent feedback on a previous proposal (triggers revised classification)",
    )
    parser.add_argument(
        "--plan", metavar="FILE",
        help="Execute a previously saved commit plan (JSON from --json / --save-plan). "
             "Skips classification entirely — the exact plan is replayed.",
    )
    parser.add_argument(
        "--save-plan", metavar="FILE",
        help="When used with --json, also write the plan to FILE for later use with --plan.",
    )
    parser.add_argument(
        "--setup", action="store_true",
        help="Run the interactive configuration wizard and exit.",
    )

    args = parser.parse_args()
    global _QUIET
    _QUIET = args.quiet or args.json_out  # suppress progress when outputting JSON
    repo = Path(args.repo).resolve()

    # Setup wizard: run before git-repo validation so --setup --global works
    # from any directory (not just inside a git repo).
    if args.setup:
        # Try to resolve a repo for "local" scope option; silently ignore if
        # the current directory is not a git repo.
        setup_repo: Path | None = None
        check = subprocess.run(
            ["git", "rev-parse", "--git-dir"],
            cwd=repo,
            capture_output=True,
            text=True,
        )
        if check.returncode == 0:
            setup_repo = repo
        run_setup_wizard(setup_repo)
        return

    # Verify it's a git repo early so we can read git config
    try:
        _git(["rev-parse", "--git-dir"], repo)
    except RuntimeError:
        print(f"Error: {repo} is not a git repository.", file=sys.stderr)
        sys.exit(3)

    # Configure API backend.
    # Priority: CLI arg > env var > git config (smart-commit.*) > built-in default.
    gc = read_git_config(repo)
    issue_patterns = read_issue_patterns(repo, gc)
    api_config = ApiConfig(
        base_url=(
            args.api_base
            or os.environ.get("LLM_API_BASE")
            or gc.get("smart-commit.api-base")
            or OLLAMA_BASE_URL
        ).rstrip("/"),
        model=(
            args.model
            or os.environ.get("LLM_MODEL")
            or gc.get("smart-commit.model")
            or DEFAULT_MODEL
        ),
        api_key=(
            args.api_key
            or os.environ.get("LLM_API_KEY")
            or gc.get("smart-commit.api-key")
        ),
    )
    client = LLMClient(api_config)
    try:
        _main_body(client, repo, args, issue_patterns)
    finally:
        client.close()


def _main_body(client: LLMClient, repo: Path, args, issue_patterns) -> None:
    """Core logic of main(), separated so LLMClient is always closed."""
    analyzer = GitAnalyzer(repo, client)
    analyzer.check_git_state()

    # Collect hunks (unstages staged files first so everything is uniform)
    _log("Scanning for changes…")
    hunk_map = analyzer.collect_hunks()
    if not hunk_map:
        print("No changes found.", file=sys.stderr)
        sys.exit(1)

    # Plan replay mode: load a previously saved plan, skip classification
    if args.plan:
        try:
            with open(args.plan) as f:
                plan = json.load(f)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Error: could not load plan from {args.plan}: {e}", file=sys.stderr)
            sys.exit(1)
        commits = plan["commits"]
        gitignore = plan.get("gitignore", [])
        # Restore saved patches into hunk_map if present (makes plan self-contained)
        for hunk_id, patch in plan.get("hunk_patches", {}).items():
            if hunk_id not in hunk_map:
                # Best-effort: reconstruct a DiffHunk from the saved patch
                fpath = hunk_id.rsplit("#", 1)[0] if "#" in hunk_id else hunk_id
                hunk_map[hunk_id] = DiffHunk(
                    hunk_id=hunk_id, file_path=fpath, patch=patch
                )
        if args.dry_run:
            print_proposed_commits(commits, gitignore, hunk_map)
            return
        if args.yes:
            print_proposed_commits(commits, gitignore, hunk_map)
        elif sys.stdout.isatty():
            result = _run_confirm_tui(commits, gitignore, hunk_map)
            if result is None:
                print("Cancelled.")
                return
            commits = result
        else:
            print_proposed_commits(commits, gitignore, hunk_map)
            try:
                answer = input("Proceed with these commits? [y/N] ").strip().lower()
            except (KeyboardInterrupt, EOFError):
                return
            if answer not in ("y", "yes"):
                print("Cancelled.")
                return
        analyzer.execute_commits(commits, hunk_map)
        analyzer.update_gitignore(gitignore, yes=args.yes)
        print("\nDone.")
        return

    n_files = len({dh.file_path for dh in hunk_map.values()})
    n_hunks = len(hunk_map)
    _log(f"Found {n_hunks} hunk(s) across {n_files} file(s). Analyzing…")

    commits, gitignore = analyzer.classify_changes(
        hunk_map, critique=args.critique, issue_patterns=issue_patterns or None
    )

    usage = analyzer.client.usage
    if usage.total_tokens > 0:
        _log(f"  {_ansi(str(usage), _ANSI_DIM)}")

    # JSON mode: dump plan (including patches for self-contained replay) and exit
    if args.json_out:
        hunk_patches = {hid: dh.patch for hid, dh in hunk_map.items() if not dh.is_untracked}
        plan_json = json.dumps(
            {"commits": commits, "hunk_patches": hunk_patches, "gitignore": gitignore},
            indent=2,
        )
        print(plan_json)
        if args.save_plan:
            with open(args.save_plan, "w") as f:
                f.write(plan_json)
        return

    if args.dry_run:
        print_proposed_commits(commits, gitignore, hunk_map)
        return

    # Interactive TUI when stdout is a terminal; plain-text fallback otherwise.
    if args.yes:
        print_proposed_commits(commits, gitignore, hunk_map)
    else:
        while True:
            if sys.stdout.isatty():
                result = _run_confirm_tui(commits, gitignore, hunk_map)
            else:
                result = _run_text_confirmation(commits, gitignore, hunk_map)
            if result is None:
                print("Cancelled.")
                return
            if isinstance(result, _Reclassify):
                _log(f"Re-classifying with critique: {result.critique!r}…")
                commits, gitignore = analyzer.classify_changes(
                    hunk_map, critique=result.critique, issue_patterns=issue_patterns or None
                )
                usage = analyzer.client.usage
                if usage.total_tokens > 0:
                    _log(f"  {_ansi(str(usage), _ANSI_DIM)}")
                continue
            commits = result
            break

    analyzer.execute_commits(commits, hunk_map)
    analyzer.update_gitignore(gitignore, yes=args.yes)
    print("\nDone.")


if __name__ == "__main__":
    main()
