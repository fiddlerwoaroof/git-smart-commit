#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pydantic",
# ]
# ///
"""
git-smart-commit: Analyze unstaged changes, group into logical commits, execute them.

Uses an LLM for classification. Supports Ollama (default) and any OpenAI-compatible
API (OpenRouter, Anthropic, Together, etc.) via --api-base.

Usage:
    git-smart-commit [--repo PATH] [--dry-run] [--model MODEL] [--yes]
                     [--api-base URL] [--api-key KEY]

Options:
    --repo PATH     Path to git repository (default: current directory)
    --dry-run       Print proposed commits without executing
    --model MODEL   Model name (default: qwen3-coder:30b-a3b-q8_0)
    --yes           Skip confirmation prompt and commit immediately
    --json          Output proposed commits as JSON and exit (implies --dry-run)
    --api-base URL  API base URL (default: http://localhost:11434 for Ollama)
                    Use https://openrouter.ai/api/v1 for OpenRouter, etc.
    --api-key KEY   API key for authenticated endpoints (or set LLM_API_KEY env var)
    --help          Show this message

Exit codes:
    0   Success
    1   No changes found
    2   Model/Ollama error
    3   Git error
    4   User cancelled
"""

from pathlib import Path
from pydantic import BaseModel, Field, ValidationError
from typing import Any, Callable

import argparse
import ast
import httpx
import inspect
import json
import os
import subprocess
import sys
import textwrap
from dataclasses import dataclass

# ── Configuration ──────────────────────────────────────────────────────────────

SYSTEM_PROMPT = textwrap.dedent("""\
    You are a senior software engineer helping organize messy working-tree changes
    into clean, logical git commits.

    Given a list of changed files and their diffs, your job is to group them into
    one or more commits. Each commit should represent a single logical change
    (e.g. "add feature X", "fix bug in Y", "update dependencies",
    "refactor Z").

    You do not need to commit every file. Skip junk files (editor backups,
    build artifacts, OS metadata). Instead, collect suggested .gitignore patterns
    for them in the gitignore argument.

    You also actively look for common coding issues that a linter
    would catch and code smells such as using conditionals where
    polymorphism is more appropriate or violations of the Law of
    Demeter: misused APIs, suspicious code patterns, etc. For example:

    ```java
    // missing if braces
    if (a == 1) # wrong, add an issue "missing braces for if statement"
       b;
       c;
    d;
    ```

    ```c
    // incorrect arguments for well-known functions and Constructors
    printf(1); # wrong, add an issue "printf called with invalid arguments"
    ```

    ```python
    // Wrong keyword for the programming language
    if True:
        throw new Exception("foo") # wrong, add an issue "invalid keywords for python"

    ```

    ```python
    # Using exceptions for control flow instead of sys.exit
    if answer == "no":
        raise Exception("Cancelled.")  # wrong: should be sys.exit(0) or return
    ```

    Any detected issues should be added to the list of issues in a
    particular commit.

    Rules:
    - CRITICAL: You can only commit WHOLE FILES. Partial staging is NOT supported. If a single file contains multiple unrelated changes, you MUST group them together into one single commit. Do not split a file across multiple commits.
    - CRITICAL: Tool calls must use JSON for arguments. Other formats will result in failure. In particular, strings must be surrounded by double-quotes (") and if one occurs literally, escape with backslashes.
    - Keep related changes together (same feature, same module, same concern).
    - Separate unrelated concerns into different commits, provided they do not violate the whole-file rule above.
    - Dependency/lockfile changes belong with the commit that caused them.
    - Test files belong with the code they test.
    - Use strict conventional commit format:
        subject: type(scope): short description  (under 72 chars)
        body: 4-10 lines, plain text, wrapped at 80 chars, no markdown
    - Type Definitions:
        * feat: Adds a net-new capability, flag, or behavior (e.g., adding .venv support, new CLI flags). Use this even for personal tools/dotfiles!
        * fix: Resolves a bug, crash, or incorrect behavior.
        * refactor: Structural changes that DO NOT change external behavior. If it adds a feature, it is a 'feat'.
        * chore: Routine maintenance, dependency bumps, or minor environment tweaks with no new logic.
        * (Other types: docs, test, style, build, ci)
    - feat is the highest priority commit type
    - chore is the lowest priority commit type
    - Do not repeat the commit type in the subject description (e.g., avoid "refactor(tools): refactor the parser").
    - Write body content based only on what you observe in the diff.
      Do not reference issue numbers or details not visible in the changes.
    - In the issues field, call out any bugs, incorrect API usage, or suspicious
      patterns you observe in the diff. Examples: wrong number of arguments,
      misused stdlib functions, unreachable code, obvious logic errors. Be specific:
      include the file, the offending line or pattern, and why it's wrong.
      Leave issues empty only if you find nothing suspicious.
    - Watch specifically for arguments passed to constructors or functions that
      don't accept them (e.g. Exception() does not accept a file= keyword argument).

    Call the propose_commits tool with your answer.
""")


OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "qwen3-coder:30b-a3b-q8_0"
API_TIMEOUT = 300.0
NUM_CTX = 128000


@dataclass
class ApiConfig:
    """API connection configuration."""
    base_url: str = OLLAMA_BASE_URL
    model: str = DEFAULT_MODEL
    api_key: str | None = None

    @property
    def is_ollama(self) -> bool:
        """True when targeting a native Ollama endpoint (not OpenAI-compatible)."""
        # OpenAI-compatible APIs use /v1 in the path; Ollama doesn't.
        return "/v1" not in self.base_url

    @property
    def chat_url(self) -> str:
        if self.is_ollama:
            return f"{self.base_url}/api/chat"
        return f"{self.base_url}/chat/completions"

    @property
    def auth_headers(self) -> dict[str, str]:
        if self.api_key:
            return {"Authorization": f"Bearer {self.api_key}"}
        return {}


# Module-level config, set from main() before any API calls.
_api_config = ApiConfig()

# ── Tool registry ──────────────────────────────────────────────────────────────
#
# Defining a new tool:
#   1. Define argument types as pydantic BaseModel subclasses
#   2. Define a top-level args model (also a BaseModel) for the tool
#   3. Decorate a function with @tool(ArgsModel)
#
# Schema generation, validation, and parsing are all handled automatically.

_TOOL_REGISTRY: dict[str, dict] = {}


def tool(args_model: type[BaseModel]):
    """Decorator factory. @tool(MyArgsModel) registers a function as an LLM tool."""
    def decorator(fn: Callable) -> Callable:
        spec = {
            "type": "function",
            "function": {
                "name": fn.__name__,
                "description": (fn.__doc__ or "").strip(),
                "parameters": args_model.model_json_schema(),
            },
        }
        _TOOL_REGISTRY[fn.__name__] = {"spec": spec, "model": args_model, "fn": fn}
        fn._tool_spec = spec
        fn._tool_model = args_model
        return fn
    return decorator


def _build_payload(prompt: str, tools: list[dict] | None = None,
                   stream: bool = False) -> dict:
    """Build an API request payload, adapting to Ollama or OpenAI format."""
    cfg = _api_config
    if cfg.is_ollama:
        payload: dict[str, Any] = {
            "model": cfg.model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": stream,
            "keep_alive": "1h",
            "options": {
                "temperature": 0.2,
                "num_ctx": NUM_CTX,
            },
        }
        if tools:
            payload["tools"] = tools
    else:
        payload = {
            "model": cfg.model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": stream,
            "temperature": 0.2,
        }
        if tools:
            payload["tools"] = tools
            # Force the model to call the specific tool
            payload["tool_choice"] = {
                "type": "function",
                "function": {"name": tools[0]["function"]["name"]},
            }
    return payload


def _extract_tool_args(data: dict) -> dict | None:
    """Extract tool-call arguments from a response, handling both API formats."""
    cfg = _api_config
    if cfg.is_ollama:
        tool_calls = data.get("message", {}).get("tool_calls", [])
        if tool_calls:
            raw = tool_calls[0]["function"]["arguments"]
            return json.loads(raw) if isinstance(raw, str) else raw
        # Fallback: content as JSON
        content = data.get("message", {}).get("content", "").strip()
    else:
        choices = data.get("choices", [])
        if not choices:
            return None
        msg = choices[0].get("message", {})
        tool_calls = msg.get("tool_calls", [])
        if tool_calls:
            raw = tool_calls[0]["function"]["arguments"]
            return json.loads(raw) if isinstance(raw, str) else raw
        content = msg.get("content", "").strip()

    # Fallback: parse content as JSON (both backends)
    if content:
        content = content.lstrip("```json").lstrip("```").rstrip("```").strip()
        return json.loads(content)
    return None


def call_ollama_with_tool(
    ollama_model: str,
    prompt: str,
    tool_fn: Callable,
    num_ctx: int = NUM_CTX,
    **tool_kwargs,
) -> Any:
    """Call the LLM forcing tool_fn to be called.

    Parses and validates the response into the tool's args model, then calls
    tool_fn(validated_args, **tool_kwargs) and returns its result.
    """
    spec = tool_fn._tool_spec
    args_model: type[BaseModel] = tool_fn._tool_model
    actual_prompt = prompt
    retries = 0
    failures = []
    while True:
        retries += 1
        payload = _build_payload(actual_prompt, tools=[spec], stream=False)
        headers = {**_api_config.auth_headers, "Content-Type": "application/json"}
        with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=5.0)) as client:
            response = client.post(_api_config.chat_url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()

        raw_args = _extract_tool_args(data)
        if raw_args is None:
            raw_args = {}

        # Fix stringified nested values: some models return e.g.
        # {"commits": "[{'subject': ...}]"} instead of a proper nested structure.
        # Detect strings that look like Python lists/dicts and try to parse them.
        for key, val in raw_args.items():
            if isinstance(val, str) and val.strip().startswith(("[", "{")):
                try:
                    raw_args[key] = json.loads(val)
                except json.JSONDecodeError:
                    try:
                        raw_args[key] = ast.literal_eval(val)
                    except (ValueError, SyntaxError):
                        pass  # leave as-is, let pydantic report the error

        try:
            validated = args_model.model_validate(raw_args).post_process()
            return tool_fn(validated, **tool_kwargs)
        except ValidationError as e:
            failures.append(str(e))
            if retries > 5:
                raise

            actual_prompt = textwrap.dedent(f"""
            Your previous ({len(failures)}) attempts failed with a validation error:
            {'\n'.join(set(failures))}

            Try again following this prompt exactly:
            {prompt}
            """)


# ── Git helpers ────────────────────────────────────────────────────────────────


def git(args: list[str], cwd: Path) -> str:
    """Run a git command and return stdout. Raises on non-zero exit."""
    result = subprocess.run(
        ["git"] + args,
        cwd=cwd,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed:\n{result.stderr.strip()}")
    return result.stdout


def get_changed_files(repo: Path) -> list[str]:
    """Return list of files with unstaged or staged changes."""
    # Both staged (cached) and unstaged, deduped
    staged = git(["diff", "--cached", "--name-only"], repo).splitlines()
    unstaged = git(["diff", "--name-only"], repo).splitlines()
    untracked = git(["ls-files", "--others", "--exclude-standard"], repo).splitlines()
    seen = set()
    files = []
    for f in staged + unstaged + untracked:
        if f and f not in seen:
            seen.add(f)
            files.append(f)
    return files


DIFF_CHARS_PER_FILE = 6000    # files larger than this get hunk-summarized
DIFF_TOTAL_CHARS   = 120000   # hard cap on total context sent to classifier
HUNK_SUMMARIZE_CTX = 8192     # smaller context for summarization calls (faster)


def split_hunks(diff_text: str) -> list[str]:
    """Split a unified diff into individual hunks (each starting with @@)."""
    lines = diff_text.splitlines(keepends=True)
    hunks: list[str] = []
    header_lines: list[str] = []
    current: list[str] = []

    for line in lines:
        if line.startswith("@@"):
            if current:
                hunks.append("".join(current))
            current = header_lines + [line]
        elif line.startswith(("diff --git", "index ", "--- ", "+++ ")):
            header_lines.append(line)
            if current:
                # flush any open hunk first
                hunks.append("".join(current))
                current = []
        else:
            if current:
                current.append(line)
            # lines before the first @@ go into header_lines
            elif not line.startswith("@@"):
                header_lines.append(line)

    if current:
        hunks.append("".join(current))

    return hunks or [diff_text]   # fallback: treat whole diff as one hunk


PRIOR_HUNK_WINDOW = 4   # how many previous hunk summaries to include as context


def summarize_file_diff(model: str, fname: str, diff_text: str, max_chars: int) -> str:
    """Ask the model to summarize each hunk, passing the last 3 summaries as context."""
    hunks = split_hunks(diff_text)
    hunk_summaries: list[str] = []

    for i, hunk in enumerate(hunks, 1):
        prior_context = ""
        if hunk_summaries:
            window = hunk_summaries[-PRIOR_HUNK_WINDOW:]
            start_idx = i - len(window)
            prior_context = "Previous hunks in this file:\n" + "\n".join(
                f"  Hunk {start_idx + j}: {s}" for j, s in enumerate(window)
            ) + "\n\n"

        prompt = textwrap.dedent(f"""\
            Summarize hunk {i} of {len(hunks)} from '{fname}' in 1-3 plain English sentences.
            Focus on WHAT changed. Preserve verbatim any code that looks buggy or suspicious
            (wrong arguments, misused APIs, incorrect syntax). Output only the summary.

            {prior_context}Hunk {i}:
            {hunk}
        """)
        first_hunk = i

        summary = call_ollama(model, prompt, num_ctx=HUNK_SUMMARIZE_CTX)

        hunk_summaries.append(summary.strip())

    print()
    return f"[summarized — {len(hunks)} hunk(s)]\n" + "\n".join(
        f"  Hunk {i}: {s}" for i, s in enumerate(hunk_summaries, 1)
    )


def build_diff_summary(repo: Path, model: str) -> str:
    """Return diff context for the classifier.

    Small files: include raw diff.
    Large files: summarize each hunk via a fast model call, then include summaries.
    """
    parts: list[str] = []
    total_chars = 0

    for label, extra_args in [("Staged", ["--cached"]), ("Unstaged", [])]:
        stat = git(["diff"] + extra_args + ["--stat", "--no-color"], repo).strip()
        if not stat:
            continue
        parts.append(f"{label}:\n{stat}\n")

        changed = git(["diff"] + extra_args + ["--name-only", "--no-ext-diff"], repo).splitlines()
        for fname in changed:
            file_diff = git(["diff"] + extra_args + ["--no-color", "--no-ext-diff", "--", fname], repo)

            if len(file_diff) <= DIFF_CHARS_PER_FILE:
                content = file_diff
            else:
                print(f"  Summarizing large diff: {fname} ({len(file_diff):,} chars)…",
                      file=sys.stderr)
                content = summarize_file_diff(model, fname, file_diff, DIFF_CHARS_PER_FILE)

            parts.append(content)
            total_chars += len(content)

            if total_chars >= DIFF_TOTAL_CHARS:
                parts.append("\n... (remaining diffs omitted — total size limit reached)")
                break
        else:
            continue
        break   # hit total limit inside inner loop

    untracked = git(["ls-files", "--others", "--exclude-standard"], repo).splitlines()
    if untracked:
        parts.append("Untracked (new files):\n" + "\n".join(f"  {f}" for f in untracked))

    return "\n".join(parts)


# ── LLM client (streaming) ────────────────────────────────────────────────────

def call_ollama(model: str, prompt: str, num_ctx: int = NUM_CTX) -> str:
    """Call the LLM with streaming and return the assistant message text."""
    cfg = _api_config
    payload = _build_payload(prompt, stream=True)
    headers = {**cfg.auth_headers, "Content-Type": "application/json"}
    with httpx.Client(timeout=httpx.Timeout(connect=30.0, read=60.0, write=30.0, pool=5.0)) as client:
        with client.stream("POST", cfg.chat_url, json=payload, headers=headers) as response:
            response.raise_for_status()
            chunks = []
            for line in response.iter_lines():
                if not line:
                    continue
                # OpenAI SSE format: lines prefixed with "data: "
                if line.startswith("data: "):
                    line = line[6:]
                if line.strip() == "[DONE]":
                    break
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                if cfg.is_ollama:
                    content = data.get("message", {}).get("content", "")
                    done = data.get("done", False)
                else:
                    delta = (data.get("choices", [{}])[0]
                             .get("delta", {}))
                    content = delta.get("content", "")
                    done = data.get("choices", [{}])[0].get("finish_reason") is not None
                if content:
                    chunks.append(content)
                if done:
                    break
            return "".join(chunks)


# ── Tool definitions ───────────────────────────────────────────────────────────

class Issue(BaseModel):
    message: str = Field(description="REQUIRED. A brief description of a code issue, including a line number")
    path: str = Field(description="REQUIRED. The path affected by the issue")

class Commit(BaseModel):
    subject: str = Field(description="REQUIRED. Conventional commit subject line (type(scope): desc), under 72 chars")
    files: list[str] = Field(description="REQUIRED. Repository-relative paths to include in this commit")
    body: str = Field(default="", description="4-10 line plain-text commit body, no markdown")
    issues: list[Issue] = Field(default_factory=list, description="Issues noticed in the code being committed")


    def post_process(self):
        self.body = textwrap.fill(self.body, 80)
        return self

class ProposeCommitsArgs(BaseModel):
    commits: list[Commit] = Field(description="REQUIRED. Logical commit groups")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commits = [commit.post_process() for commit in self.commits]
        return self

sample_issue = Issue(
    message="Unused import 'os' on line 1",
    path="main.py"
)

sample_commit = Commit(
    subject="refactor(main): cleanup imports",
    files=["main.py"],
    body="Removed unnecessary imports to improve load time and code cleanliness.",
    issues=[sample_issue]
)

SAMPLE_OUTPUT = ProposeCommitsArgs(
    commits=[sample_commit],
    gitignore=["*.pyc", "__pycache__/"]
)



@tool(ProposeCommitsArgs)
def propose_commits(result: ProposeCommitsArgs, model: str) -> tuple[list[dict], list[str]]:
    """Group working-tree changes into logical commits and suggest .gitignore patterns for junk files."""
    commits = [c.model_dump() for c in result.commits]
    if not commits:
        raise Exception("Error: Model returned no valid commit groups.")

    commits = merge_overlapping_commits(model, commits)
    return commits, result.gitignore


class MergeCommitsArgs(BaseModel):
    commit: Commit = Field(description="REQUIRED. A single merged commit covering all the provided changes")
    gitignore: list[str] = Field(default_factory=list, description="Suggested .gitignore patterns for junk files")

    def post_process(self):
        self.commit = self.commit.post_process()
        return self


@tool(MergeCommitsArgs)
def merge_commits(result: MergeCommitsArgs, all_files: list[str], all_issues: list[dict]) -> dict:
    """Merge multiple overlapping commits into a single coherent commit."""
    commit = result.commit.model_dump()
    # Ensure all files and issues are preserved regardless of what the model returns
    commit["files"] = all_files
    commit["issues"] = all_issues
    return commit


MERGE_PROMPT = textwrap.dedent("""\
    You are merging multiple proposed commits that all touch overlapping files
    and therefore cannot be committed separately (partial staging is not supported).

    Combine them into a SINGLE commit that:
    - Has one conventional commit subject line (type(scope): desc), under 72 chars
    - Has a body (4-10 lines) summarizing ALL the changes coherently
    - Lists ALL affected files (deduped)
    - Collects ALL issues (deduped)
    - If any commit is a feat type, it must take priority over the others (although the others should be mentioned in the commit body)
    - chore should only be used if no other category applies.

    Do NOT just concatenate the subjects. Write a new, coherent subject and body
    that covers the full set of changes as a single logical unit.

    Call the merge_commits tool with your answer.

    Here are the commits to merge:
""")


def merge_overlapping_commits(model: str, commits: list[dict]) -> list[dict]:
    """Merge commits whose file sets overlap, since partial staging isn't supported.

    Uses union-find to detect groups, then asks the model to write a single
    coherent commit message for each merged group.
    """
    if len(commits) <= 1:
        return commits

    # Build a mapping from each file to the indices of commits that touch it
    file_to_indices: dict[str, list[int]] = {}
    for i, c in enumerate(commits):
        for f in c.get("files", []):
            file_to_indices.setdefault(f, []).append(i)

    # Union-find to group commits sharing files
    parent = list(range(len(commits)))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for indices in file_to_indices.values():
        for idx in indices[1:]:
            union(indices[0], idx)

    # Group commits by their root
    groups: dict[int, list[int]] = {}
    for i in range(len(commits)):
        groups.setdefault(find(i), []).append(i)

    # Check if any merging is actually needed
    if all(len(idxs) == 1 for idxs in groups.values()):
        return commits

    merged = []
    for indices in groups.values():
        if len(indices) == 1:
            merged.append(commits[indices[0]])
            continue

        group = [commits[i] for i in indices]
        print(f"  Merging {len(group)} overlapping commits…", file=sys.stderr)

        # Collect all files and issues for the merged commit
        all_files = list(dict.fromkeys(f for c in group for f in c.get("files", [])))
        all_issues = []
        seen_issues = set()
        for c in group:
            for issue in c.get("issues", []):
                key = (issue.get("path", ""), issue.get("message", ""))
                if key not in seen_issues:
                    seen_issues.add(key)
                    all_issues.append(issue)

        # Format the commits for the merge prompt
        commits_desc = json.dumps(group, indent=2)
        prompt = MERGE_PROMPT + commits_desc

        merged.append(call_ollama_with_tool(
            model, prompt, merge_commits,
            all_files=all_files, all_issues=all_issues,
        ))

    return merged


def classify_changes(model: str, files: list[str], diff_summary: str, critique: str = "") -> tuple[list[dict], list[str]]:
    """Ask the model to group files into logical commits. Returns (commits, gitignore_patterns)."""
    file_list = "\n".join(f"  - {f}" for f in files)

    critique_section = ""
    if critique:
        critique_section = (
            f"\n\nA reviewer rated the previous proposal below 7/10. Their feedback:\n"
            f"{critique}\n"
            f"Revise your commit groupings to address this feedback."
        )

    prompt = (
        f"{SYSTEM_PROMPT}\n\n"
        f"Sample Output:\n{SAMPLE_OUTPUT.model_dump_json()}\n\n"
        f"Changed files:\n{file_list}\n\n"
        f"Diffs:\n{diff_summary}"
        f"{critique_section}"
    )

    return call_ollama_with_tool(model, prompt, propose_commits, model=model)




# ── Commit execution ───────────────────────────────────────────────────────────

def execute_commits(repo: Path, commits: list[dict]) -> None:
    """Stage and commit each group in order."""
    for i, commit in enumerate(commits, 1):
        subject = commit["subject"]
        body = commit.get("body", "").strip()
        message = f"{subject}\n\n{body}" if body else subject
        files = commit["files"]

        print(f"\n[{i}/{len(commits)}] {subject}")

        # Stage the files for this commit
        for f in files:
            try:
                git(["add", "--", f], repo)
                print(f"  + {f}")
            except RuntimeError as e:
                print(f"  ! Could not stage {f}: {e}", file=sys.stderr)

        # Check something is actually staged
        staged = git(["diff", "--cached", "--name-only"], repo).strip()
        if not staged:
            print("  (nothing staged, skipping)")
            continue

        try:
            git(["commit", "-m", message], repo)
            print(f"  ✓ Committed")
        except RuntimeError as e:
            print(f"   Error committing: {e}", file=sys.stderr)
            raise


# ── Display helpers ────────────────────────────────────────────────────────────

def print_proposed_commits(commits: list[dict], gitignore: list[str]) -> None:
    print("\nProposed commits:")
    print("─" * 60)
    for i, commit in enumerate(commits, 1):
        print(f"\n  [{i}] {commit['subject']}")
        body = commit.get("body", "").strip()
        print()
        if body:
            for line in body.splitlines():
                print(f"      {line}")
        for f in commit.get("files", []):
            print(f"       + {f}")
        if (issues:= commit.get("issues")) != []:
            print("  Issues in this commit:")
            for issue in issues:
                print(f"    - {issue["path"]}: {issue["message"]}")
    if gitignore:
        print("\n  Suggested .gitignore patterns:")
        for pattern in gitignore:
            print(f"       {pattern}")
    print()


# ── Main ───────────────────────────────────────────────────────────────────────

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Group git changes into logical commits using Qwen3-Coder.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--repo", default=".", help="Path to git repository")
    parser.add_argument("--dry-run", action="store_true", help="Show proposed commits without executing")
    parser.add_argument("--model", default=DEFAULT_MODEL, help=f"Model name (default: {DEFAULT_MODEL})")
    parser.add_argument("--yes", "-y", action="store_true", help="Commit without confirmation")
    parser.add_argument("--json", dest="json_out", action="store_true", help="Output JSON and exit")
    parser.add_argument(
        "--api-base", default=None,
        help="API base URL (default: http://localhost:11434 for Ollama). "
             "Use https://openrouter.ai/api/v1 for OpenRouter. "
             "Or set LLM_API_BASE env var.",
    )
    parser.add_argument(
        "--api-key", default=None,
        help="API key for authenticated endpoints (or set LLM_API_KEY env var)",
    )
    parser.add_argument(
        "--critique", default="",
        metavar="TEXT",
        help="Agent feedback on a previous proposal (triggers revised classification)",
    )

    args = parser.parse_args()
    repo = Path(args.repo).resolve()

    # Configure API backend
    global _api_config
    _api_config = ApiConfig(
        base_url=(args.api_base
                  or os.environ.get("LLM_API_BASE", OLLAMA_BASE_URL)).rstrip("/"),
        model=args.model,
        api_key=args.api_key or os.environ.get("LLM_API_KEY"),
    )

    # Verify it's a git repo
    try:
        git(["rev-parse", "--git-dir"], repo)
    except RuntimeError:
        print(f"Error: {repo} is not a git repository.", file=sys.stderr)
        sys.exit(3)

    # Collect changed files
    print("Scanning for changes...", file=sys.stderr)
    files = get_changed_files(repo)
    if not files:
        raise Exception("No changes found.")

    print(f"Found {len(files)} changed file(s). Analyzing...", file=sys.stderr)

    # Build stat summary and classify
    diff_summary = build_diff_summary(repo, args.model)
    commits, gitignore = classify_changes(args.model, files, diff_summary, critique=args.critique)

    # JSON mode: just dump and exit
    if args.json_out:
        print(json.dumps({"commits": commits, "gitignore": gitignore}, indent=2))
        return

    # Display proposed commits
    print_proposed_commits(commits, gitignore)

    if args.dry_run:
        return

    # Confirm unless --yes
    if not args.yes:
        success = False
        try:
            answer = input("Proceed with these commits? [y/N] ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            return
        if answer not in ("y", "yes"):
            print("Cancelled.")
            return

    execute_commits(repo, commits)
    print("\nDone.")


if __name__ == "__main__":
    main()
